<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OneDeakin CloudDeakin Template</title>
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css?version=202001061835" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <!-- prod -->
    <link rel="stylesheet" type="text/css" href="/content/enforced/1308183-cd_dlf_apps/onedeakin/onedeakin.css?v=20220329">
    <link rel="stylesheet" type="text/css" href="/content/deakin-embedded-comments/embeddedComments.css?v=20210208">
</head><body><div class="oneDeakin_instructions" style="font-size: 1rem; background: #e9ecef; padding: .4rem 1rem; border-radius: .3rem; margin-bottom: 1rem;">
<p><strong>OneDeakin template features</strong></p>
<blockquote>
<p>The OneDeakin template is full of features that help you advance the narrative and make learning and teaching happen.</p>
</blockquote>
<p>The CloudDeakin DLF Apps site contains information on all the template features, and how and when to use them. Accessing this site requires a simple two step process.</p>
<p><strong>Step 1:</strong> Use this form to be <a title="enrol in the CloudDeakin DLF Apps site" href="https://forms.office.com/r/0p7xpyMThG" target="blank">enrolled in the CloudDeakin DLF Apps site</a>.</p>
<p><strong>Step 2: </strong><span style="text-decoration: underline;">Only after you have enrolled in Step 1 above</span>, you can then access the <a title="access the CloudDeakin DLF Apps site" href="/d2l/home/1308183" target="blank">DLF Apps site</a> or search for 'DLF Apps' in the Sites Grid Menu.</p>
</div>
<div class="oneDeakin_instructions" style="font-size: 1rem; background: #e9ecef; padding: .4rem 1rem; border-radius: .3rem;">
<p><strong>OneDeakin Template Colours</strong></p>
<p>Define the OneDeakin Template colour by using the Heading3 'teal', 'turquoise', 'orange' or 'pink'.</p>
<p>When your page is published, this Heading3 will be removed from view. It it important that this first Heading3 remains in the OneDeakin template.</p>
</div>
<h3>teal</h3>
<h3>banner medium bus/inf/information-systems-21</h3>
<!-- CONTENT STARTS HERE -->
<h2>Biased AI – is it fair&nbsp;</h2>
<p>Bias refers to any form of preference leading to a judgement or decision. Some biases can be automatic and unconscious, such as when decisions are based on heuristics, when the brain has created mental shortcuts, that it is hard to detect by the person making those decisions. Biases can cause problematic situations for humans and equally so for AI/machine learning.</p>
<p>The following example from&nbsp;<a href="https://ebookcentral.proquest.com/lib/deakin/reader.action?docID=6795440&amp;ppg=54" target="_blank" rel="noopener">Blackman (2022, p.42) </a>&nbsp;demonstrates how human bias affects AI or machine learning.</p>
<h3>container callout</h3>
<p>An organisation was using a resume-reading AI system/tool to select candidates for interviews based on the information in their resumes. The AI tool was trained by inputting the data of resumes of successful applicants from the previous 10 years in the organisation. The AI tool was trained to look for patterns that a person would be ‘ideal’ for an interview. The results from the AI tool led to interviews of the following people who applied:&nbsp;</p>
<ul>
<li>30% of all men&nbsp;</li>
<li>20% of all women&nbsp;</li>
<li>10% of all Black man&nbsp;</li>
<li>5% of all Black women&nbsp;</li>
</ul>
<hr>
<p>The results may or may not seem surprising to you; It appears that the AI tool developed by the organisation is biased against women, Black men and Black women in particular. This bias has created an ‘unfair’ result against our efforts to achieve a more egalitarian society.&nbsp;</p>
<h3>Where does the potential bias or discrimination in AI come from?</h3>
<blockquote>
<p>How does an AI tool develop these biases</p>
</blockquote>
<p>The resume AI tool and other real-life examples have shown that AI biases comes from human prejudices as machine learning originates from the input of data. Biases in AI systems or tools can arise from:&nbsp;</p>
<ul>
<li>Cognitive bias – this is an umbrella term used to describe the tendency when people use their personal experiences and/or emotions to affect their judgements and decisions. &nbsp;These biases could either be introduced in the:
<ul>
<li>design process&nbsp;</li>
<li>data set used for training includes biases</li>
</ul>
</li>
<li>Lack of complete data/sample size – if the data set only contains part of the picture it may be biased.&nbsp;</li>
</ul>
<p>The ‘racist soap dispenser’ was an example where bias was introduced into a product that went on the market. A soap dispenser was designed and ‘trained’ to automatically dispense soap when it sensed a hand in close proximity, and it reflected light back to the sensor. However, once it hit the market it was found that it would only dispense soap to light skinned hands. As it turned out the designers were all white and unknowingly trained the ‘dispenser’ not to detect darker complexions which absorb more of this light rather than reflect it back. Apart from creating controversy on social media, this was a costly error for the company to redesign and produce a ‘non-racist’ soap dispenser!&nbsp;</p>
<h3><em class="fa fa-eye" style="font-size: 24px;"></em><strong style="font-size: 1em;">&nbsp;Watch</strong></h3>
<p>Watch this short video below to see the ‘racist soap dispenser’ after it was installed in public bathroom. If you have ever had a problem grasping the importance of diversity in tech and its impact on society, watch this video.</p>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr"></p>
<p lang="en" dir="ltr"><iframe width="500" height="281" src="https://www.youtube.com/embed/YJjv_OeiHmo?feature=oembed&amp;wmode=opaque&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen" title="This 'Racist soap dispenser' at Facebook office does not work for black people"></iframe></p>
</blockquote>
<p>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</p>
<h3>How to mitigate bias in AI and machine learning algorithm?</h3>
<p>Minimising bias in AI systems not only allows people to fully participate in society it is required for people to place their trust in the system. By understanding and identifying that biases may be present in a process/product is the first step to mitigating the bias and deciding on bias-mitigation strategy.&nbsp;</p>
<p>Below are 6 potential ways to help mitigate AI bias recommended by&nbsp; Silberg and Manyika (2019).</p>
<h3>grid</h3>
<h4>column</h4>
<p><img src="AI_green%20with%20bias.jpg" title="" width="80" height="80" style="display: block; margin-left: auto; margin-right: auto;"></p>
<p>1. Be aware of contexts in which Al can help correct for bias and those in which there is high risk for Al to exacerbate bias</p>
<h4>column</h4>
<p><img src="process_blue.png" title="" width="80" height="80" style="display: block; margin-left: auto; margin-right: auto;"></p>
<p>2. Establish processes and practices to test for and mitigate bias in Al Systems</p>
<h4>column</h4>
<p><img src="communication_green.png" title="" width="80" height="80" style="display: block; margin-left: auto; margin-right: auto;"></p>
<p>3. Engage in fact-based conversations about potential biases in human decisions</p>
<hr>
<h3>grid</h3>
<h4>column</h4>
<p><img src="humanoid_blue.png" title="" width="80" height="80" style="display: block; margin-left: auto; margin-right: auto;"></p>
<p>4. Fully explore how humans and machines can best work together&nbsp;</p>
<h4>column</h4>
<p><img src="qualitative-research_green.png" title="" width="80" height="80" style="display: block; margin-left: auto; margin-right: auto;"></p>
<p>5.&nbsp; Invest more in bias research, make more data available for research (while respecting privacy), and adopt a multidisciplinary approach&nbsp;</p>
<h4>column</h4>
<p><img src="diversity_blue.png" title="" width="80" height="80" style="display: block; margin-left: auto; margin-right: auto;"></p>
<p>6. Invest more in diversifying the Al field&nbsp;</p>
<hr>
<p>Although the steps in the above image appears to show a clear path to minimise bias, bias mitigation is not so straightforward. Tech firms use various types of metrics and definitions of fairness to use to compare the outputs of various machine learning models to test for bias or discrimination. Each of these metrics needs to be fair by producing less biased outputs but also compatible for the result you are trying to achieve so that the ‘right’ people get the interview or the house loan.</p>
<p>To make the algorithms less prone to bias you could decide to remove the labels such as ‘sex’ or ‘race’. However, removing these classes or data sets may not give you the "optimal" results. For example when screening people who are at risk for diabetes, both sex and race are useful, as diabetes presents differently in both sex and race.</p>
<p>And there are issues with quantitative metrics as they are not compatible with each other. An example of this is the case of the AI tool COMPAS, developed by a private company to predict whether a criminal would reoffend, by assigning individuals with a risk score which are used to inform judges for the sentencing of these people. &nbsp;</p>
<h3><i class="fa fa-gears" style="font-size: 24px;"></i><strong style="font-size: 1em;"> Explore</strong></h3>
<p>Have a look at the prior offence and the risk score that COMPAS had assigned these defendants following their arrest for drug possession and petty theft. Read the following scenarios with the individuals’ offenses and try and work what level of Risk from Low (1) to High (10) that AI allocated to them.&nbsp;</p>
<h3>container instructions</h3>
<p>Use the arrows to move between cards and the turn button to reveal the answer.</p>
<hr>
<p style="text-align: center;"><iframe src="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=1527351&amp;type=lti&amp;rCode=DeakinUniversity-3823867" title="MIS715 6.3 Mitigating bias" allowfullscreen="allowfullscreen" allow="microphone *; camera *; autoplay *" height="600" width="779"></iframe></p>
<p style="text-align: right;"><span style="font-size: 14px;">Source:&nbsp;<a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a></span></p>
<p>ProRebulica, a non-profit newsroom, claimed that Northpointe, the developers of COMPAS, had incorrectly labelled African-American’s to be labelled as ‘high risk’ almost double to that of white defendants being mislabelled. Blackman (2022) discusses this disparity, suggesting that COMPAS used legitimate quantitative metrics to ‘maximise’ true positives, scenario 1, whereas ProPublica used metrics to minimise false positives, scenario 2.&nbsp;</p>
<p><strong>Scenario1:</strong> The AI tool provides risk rating scores on defendants to recommend to judges. It avoids charges of discrimination by pointing out that it was using a perfectly legitimate quantitative metric for fairness. More specifically, the system aimed at maximising its true positive rate across Black and White defendants. The assumption behind it is relatively simple: it’s really bad to let people who are likely to reoffend go free. The better we are at identifying those, the better it is. In this case, the AI system wants to maximize true positives (therefore it may have increased the likelihood of returning false positives).&nbsp;</p>
<p><strong>Scenario 2:</strong> The assessment of the AI system's performance uses a different quantitative metric for fairness: the rate of false positives across White and Black defendants. The idea behind this is also simple: it’s really bad to put people who are unlikely to reoffend in jail. The better we are at not unnecessary jailing people, the better we do. In this case, ProPublica wants the AI to minimize false positive (but this may decrease true positives).</p>
<p>The above scenarios show that metrics for bias identification and mitigation using technical tools alone cannot ensure ethical concerns are addressed. The tools can tell how various tweaks to AI leads to different scores according to different fairness metrics, they cannot however suggest which metrics will maximise fair outcomes. This can result in a lot of ethical concerns. Therefore, ethical judgement needs to be made by humans who are qualified to deal with ethical issues. Data scientists and AI engineers are often ill-equipped to answer the question: which, if any, of these quantitative metrics of fairness are the ethical or appropriate ones to use?&nbsp;</p>
<h3>container instructions</h3>
<p>we'll discuss these two scenarios in more depth in class</p>
<hr><hr>
<p><!-- CONTENT ENDS HERE --></p>
<div><iframe src="/content/deakin-embedded-comments/embedded-comments-loader.html?v=20201006" style="position: absolute; left: -9999px; top: -9999px; width: 0px; height: 0px;"></iframe> <!--FORUMVARIABLESTART-->
<div style="display: none;" data-forumname="null"></div>
<!--FORUMVARIABLEEND--></div>
<div>
<script src="https://s.brightspace.com/lib/jquery/2.2.4/jquery.min.js?version=202001061835" integrity="sha384-rY/jv8mMhqDabXSo+UCggqKtdmBfd3qC2/KvyTDNQ6PcUJXaxK1tMepoQda4g5vB" crossorigin="anonymous"></script>
<!-- prod -->
<script src="/content/enforced/1308183-cd_dlf_apps/onedeakin/onedeakin.js?v=20220329"></script>
<script src="/content/deakinscripts/moment/moment.2.21.js"></script>
<script src="/content/deakin-embedded-comments/embeddedComments.js?v=20210208"></script>
<script>
if(navigator.onLine)console.log("online"),$(document).ready((function(){var e,t,n;-1<window.location.href.indexOf("https://d2l")?console.log("in CloudDeakin"):(console.log("not in CloudDeakin"),$("body").prepend('<div style="background:#990050;color:#FFFFFF;padding:1rem;text-align:center;"><p>You are viewing downloaded HTML content outside of CloudDeakin.<br>Some content, images, links, interactives etc. may not render or function as they do when they are viewed in CloudDeakin.</p></div>'),$.expr[":"].textEquals=$.expr[":"].textEquals||$.expr.createPseudo((function(e){return function(t){return $(t).text().match("^"+e+"$")}})),$("h3:textEquals('container'), h3:textEquals('Container'),  h3:textEquals('container callout'), h3:textEquals('Container callout'), h3:textEquals('container instructions'), h3:textEquals('Container instructions'), h3:textEquals('container colourbox'), h3:textEquals('Container colourbox'), h3:textEquals('container references'), h3:textEquals('Container references'), h3:textEquals('banner'), h3:textEquals('banner big'), h3:textEquals('banner medium'), h3:textEquals('banner small'), h3:textEquals('fullwidth'), h3:textEquals('floatright'), h3:textEquals('floatleft'), h3:textEquals('flipcard'), h3:textEquals('tabs'), h3:textEquals('grid'), h3:textEquals('accordion'), h3:textEquals('Accordion'), h3:textEquals('popup'), h3:textEquals('fluidvideo'), h3:textEquals('teal'), h3:textEquals('turquoise'), h3:textEquals('pink'), h3:textEquals('orange'), h3:textEquals('nikeri'), h4:textEquals('front'), h4:textEquals('back'), h4:textEquals('column'), h3:contains('banner'), hr, .oneDeakin_instructions").remove(),$("body").addClass("oneDeakin"),$("body").removeAttr("style"),$("body").css({'font-family':'"HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif','font-size':'16px','margin':'10px auto','max-width':'90%'}),n="<link rel='stylesheet' href='https://d2l.deakin.edu.au/content/enforced/1308183-cd_dlf_apps/onedeakin/onedeakin.css' type='text/css' media='screen'>",(t=(e=$("head")).find("link[rel='stylesheet']:last")).length?t.after(n):e.append(n))}));else{console.log("offline");var body=document.getElementsByTagName("body")[0],newDiv=document.createElement("div");newDiv.innerHTML='<p style="background:#990050;color:#FFFFFF;padding:1rem;text-align:center;width:100%;font-size:14px;">You are viewing downloaded HTML content outside of CloudDeakin without any internet connection.<br>Some content, images, links, interactives etc. may not render or function as they do when they are viewed in CloudDeakin.</p>',body.insertBefore(newDiv,body.firstChild);var h3_ele=document.querySelectorAll("h3");if(h3_ele.length)for(var i=0;i<h3_ele.length;i++)"teal"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"turquoise"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"pink"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"nikeri"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"orange"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"container"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Container"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"container callout"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Container callout"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"container instructions"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Container instructions"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"container colourbox"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Container colourbox"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"container references"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Container references"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"banner"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"banner big"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"banner medium"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"banner small"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"fullwidth"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"floatright"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"floatleft"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"flipcard"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"tabs"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"grid"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"accordion"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Accordion"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"popup"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"fluidvideo"==h3_ele[i].textContent&&(h3_ele[i].style.display="none");var h4_ele=document.querySelectorAll("h4");if(h4_ele.length)for(i=0;i<h4_ele.length;i++)"front"==h4_ele[i].textContent&&(h4_ele[i].style.display="none"),"back"==h4_ele[i].textContent&&(h4_ele[i].style.display="none"),"column"==h4_ele[i].textContent&&(h4_ele[i].style.display="none");var instr_ele=document.getElementsByClassName("oneDeakin_instructions");for(i=0;i<instr_ele.length;++i)(item=instr_ele[i]).style.display="none";var hr_ele=document.getElementsByTagName("hr");for(i=0;i<hr_ele.length;++i)(item=hr_ele[i]).style.display="none"}
</script></div>
<div style="position: static !important;"></div>
<div style="position: static !important;"></div>
<div style="position: static !important;"></div>
<p></p></body></html>