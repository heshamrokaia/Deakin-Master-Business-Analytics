<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OneDeakin CloudDeakin Template</title>
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css?version=202001061835" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <!-- prod -->
    <link rel="stylesheet" type="text/css" href="/content/enforced/1308183-cd_dlf_apps/onedeakin/onedeakin.css?v=20220329">
    <link rel="stylesheet" type="text/css" href="/content/deakin-embedded-comments/embeddedComments.css?v=20210208">
</head><body><div class="oneDeakin_instructions" style="font-size: 1rem; background: #e9ecef; padding: .4rem 1rem; border-radius: .3rem; margin-bottom: 1rem;">
<p><strong>OneDeakin template features</strong></p>
<blockquote>
<p>The OneDeakin template is full of features that help you advance the narrative and make learning and teaching happen.</p>
</blockquote>
<p>The CloudDeakin DLF Apps site contains information on all the template features, and how and when to use them. Accessing this site requires a simple two step process.</p>
<p><strong>Step 1:</strong> Use this form to be <a title="enrol in the CloudDeakin DLF Apps site" href="https://forms.office.com/r/0p7xpyMThG" target="blank">enrolled in the CloudDeakin DLF Apps site</a>.</p>
<p><strong>Step 2: </strong><span style="text-decoration: underline;">Only after you have enrolled in Step 1 above</span>, you can then access the <a title="access the CloudDeakin DLF Apps site" href="/d2l/home/1308183" target="blank">DLF Apps site</a> or search for 'DLF Apps' in the Sites Grid Menu.</p>
</div>
<div class="oneDeakin_instructions" style="font-size: 1rem; background: #e9ecef; padding: .4rem 1rem; border-radius: .3rem;">
<p><strong>OneDeakin Template Colours</strong></p>
<p>Define the OneDeakin Template colour by using the Heading3 'teal', 'turquoise', 'orange' or 'pink'.</p>
<p>When your page is published, this Heading3 will be removed from view. It it important that this first Heading3 remains in the OneDeakin template.</p>
</div>
<h3>teal</h3>
<h3>banner medium bus/inf/information-systems-20</h3>
<!-- CONTENT STARTS HERE -->
<h2>Framework to manage AI bias</h2>
<p>So far, the frameworks that you have looked at have focused heavily on how to manage AI risks, this framework focuses on AI biases. As you have learnt so far, a significant proportion of AI is inherently predisposed to bias. However, even if variables that could introduce bias, such as race, age, or gender, are omitted from the training or design of an AI system due to legal or ethical considerations, the system will still exhibit some degree of bias. The AI system will still be able to identify the variable that was excluded. It does this by using indirect features (or a combination of variables) which act as a proxy, or an indirect measure (Hines, 2022). For example, a proxy that could be used is occupation or residential location which could be associated with demographic information but is less explicitly discriminatory.&nbsp;</p>
<p>A study in The Lancet (Gichoya et al., 2022) showed that an AI model was able to predict a patient’s race with a high degree of accuracy from anonymised medical imaging data when clinical experts could not. This study highlights the potential for AI to perpetuate biases even when variables that could lead to bias are omitted.</p>
<p>With this in mind, Townson (2023) suggests that organisational leaders should acknowledge that the inherent bias in AI systems cannot be completely eliminated and to focus on remediating or mitigating bias by following these 3 steps:</p>
<h3>container instructions</h3>
<p>Click on the plus signs to expand and learn more about each of the three steps.</p>
<hr>
<h3>accordion</h3>
<h4><img src="1.jpg" alt="1" title="1" width="25" height="24"> Step 1: Decide on Data and Design</h4>
<p>Currently there is no gold standard to guarantee equitable AI practices that applies for all organisations or all situations. The algorithms can be checked to determine if equal numbers from each protected class, such as race, gender and age, are selected and in equal proportions. This, however, is dependent on what information was initially provided in the input data.</p>
<p>This emphasises the significance of selecting the appropriate approach. To minimise potential problems, an organisation must identify which class or group to protect for their specific application and determine the relevant issues to be mindful of. To address these concerns Townsen (2023 para.8) poses questions such as:&nbsp;</p>
<blockquote>
<p>• Is the issue about the difference in the sizes of groups, or different accuracy rates between the groups?&nbsp;<br>• For group sizes, does fairness demand an equal number from each group type or a proportional percentage?<br>• For differing accuracy rates, is the data accurately labelled, and, if so, which group needs the most predictive equity?</p>
</blockquote>
<p>This results in a decision tree where each choice leads to a possible consequence. This can help ensure that certain groups are protected, and considerations of ethical and fairness issues are being taken into account, avoiding bias and discrimination against certain groups of people. Aligning these considerations with the organisation's policy ensures a standard is met among different areas of the organisation and consistency can be applied across the organisation.</p>
<p></p>
<h4><img src="2.jpg" alt="2" title="2" width="25" height="24"> Step 2: Check Outputs</h4>
<p>To prevent the possibility of the output of an AI system causing more harm than good, it is crucial to check its fairness and impact. Poorly considered approaches may have unintended consequences. Additionally, algorithms tend to be literal and doesn’t deal very well with intersectionality which could result in disparate impact for a group of the data. &nbsp;Intersectionality describes the interconnected nature of social identities and how they overlap. It recognizes that individuals experience discrimination and privilege based on multiple dimensions of their identity, such as race, gender, class, sexual orientation, religion, ability, and more (Steinmetz, 2020). Townsen (2023) gives the example to highlight the issue:</p>
<blockquote>
<p>If we say a credit product needs to be equally available to men and women, disabled or not, an algorithm’s solution could be to select male wheelchair users and only nondisabled women. This means that equal numbers of men, women, disabled people, and nondisabled people are included in the data, but disabled women can never be selected.</p>
</blockquote>
<p>To counteract this issue Townson (2023) suggests using a ‘generative adversarial networks (GANS)’ approach. This framework uses two systems, the original model (or generator) and a second model that acts as an auditor (or discriminator). The outputs of both models are compared to generate a fairer solution.&nbsp;</p>
<h4><img src="3.jpg" alt="3" title="3" width="25" height="24"> Step 3: Monitor for problems&nbsp;</h4>
<p>Monitoring the outputs over time is crucial and by frequently checking the AI model’s output, suspicious patterns or errors could be identified. AI systems are designed to make predictions or decisions based on input data. Accuracy of the output data is dependent on the quality of the input data and the assumptions made in the development stage of the AI system. Once the AI system is released into the real world it may encounter new scenarios or input data that are present in the real world. Additionally, if input data changes due to seasonal or cyclical patterns such as weather patterns, further challenges are present, and the accuracy of predictions or outcomes need to be monitored for accuracy.</p>
<hr>
<p>In topic 6 you learnt that there are many types of biases which can easily go unnoticed that can create unwanted outputs. Likewise, output errors can occur when data sets don’t have adequate data, or there is limited input data for certain groups. An example that Townson (2023) reports is that few clinical trials in the USA have adequate numbers of ethnic minority groups to predict their treatment outcomes compared to white men attending college. &nbsp;</p>
<hr>
<p><!-- CONTENT ENDS HERE --></p>
<div><iframe src="/content/deakin-embedded-comments/embedded-comments-loader.html?v=20201006" style="position: absolute; left: -9999px; top: -9999px; width: 0px; height: 0px;"></iframe> <!--FORUMVARIABLESTART-->
<div style="display: none;" data-forumname="null"></div>
<!--FORUMVARIABLEEND--></div>
<div>
<script src="https://s.brightspace.com/lib/jquery/2.2.4/jquery.min.js?version=202001061835" integrity="sha384-rY/jv8mMhqDabXSo+UCggqKtdmBfd3qC2/KvyTDNQ6PcUJXaxK1tMepoQda4g5vB" crossorigin="anonymous"></script>
<!-- prod -->
<script src="/content/enforced/1308183-cd_dlf_apps/onedeakin/onedeakin.js?v=20220329"></script>
<script src="/content/deakinscripts/moment/moment.2.21.js"></script>
<script src="/content/deakin-embedded-comments/embeddedComments.js?v=20210208"></script>
<script>
if(navigator.onLine)console.log("online"),$(document).ready((function(){var e,t,n;-1<window.location.href.indexOf("https://d2l")?console.log("in CloudDeakin"):(console.log("not in CloudDeakin"),$("body").prepend('<div style="background:#990050;color:#FFFFFF;padding:1rem;text-align:center;"><p>You are viewing downloaded HTML content outside of CloudDeakin.<br>Some content, images, links, interactives etc. may not render or function as they do when they are viewed in CloudDeakin.</p></div>'),$.expr[":"].textEquals=$.expr[":"].textEquals||$.expr.createPseudo((function(e){return function(t){return $(t).text().match("^"+e+"$")}})),$("h3:textEquals('container'), h3:textEquals('Container'),  h3:textEquals('container callout'), h3:textEquals('Container callout'), h3:textEquals('container instructions'), h3:textEquals('Container instructions'), h3:textEquals('container colourbox'), h3:textEquals('Container colourbox'), h3:textEquals('container references'), h3:textEquals('Container references'), h3:textEquals('banner'), h3:textEquals('banner big'), h3:textEquals('banner medium'), h3:textEquals('banner small'), h3:textEquals('fullwidth'), h3:textEquals('floatright'), h3:textEquals('floatleft'), h3:textEquals('flipcard'), h3:textEquals('tabs'), h3:textEquals('grid'), h3:textEquals('accordion'), h3:textEquals('Accordion'), h3:textEquals('popup'), h3:textEquals('fluidvideo'), h3:textEquals('teal'), h3:textEquals('turquoise'), h3:textEquals('pink'), h3:textEquals('orange'), h3:textEquals('nikeri'), h4:textEquals('front'), h4:textEquals('back'), h4:textEquals('column'), h3:contains('banner'), hr, .oneDeakin_instructions").remove(),$("body").addClass("oneDeakin"),$("body").removeAttr("style"),$("body").css({'font-family':'"HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif','font-size':'16px','margin':'10px auto','max-width':'90%'}),n="<link rel='stylesheet' href='https://d2l.deakin.edu.au/content/enforced/1308183-cd_dlf_apps/onedeakin/onedeakin.css' type='text/css' media='screen'>",(t=(e=$("head")).find("link[rel='stylesheet']:last")).length?t.after(n):e.append(n))}));else{console.log("offline");var body=document.getElementsByTagName("body")[0],newDiv=document.createElement("div");newDiv.innerHTML='<p style="background:#990050;color:#FFFFFF;padding:1rem;text-align:center;width:100%;font-size:14px;">You are viewing downloaded HTML content outside of CloudDeakin without any internet connection.<br>Some content, images, links, interactives etc. may not render or function as they do when they are viewed in CloudDeakin.</p>',body.insertBefore(newDiv,body.firstChild);var h3_ele=document.querySelectorAll("h3");if(h3_ele.length)for(var i=0;i<h3_ele.length;i++)"teal"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"turquoise"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"pink"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"nikeri"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"orange"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"container"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Container"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"container callout"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Container callout"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"container instructions"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Container instructions"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"container colourbox"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Container colourbox"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"container references"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Container references"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"banner"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"banner big"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"banner medium"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"banner small"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"fullwidth"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"floatright"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"floatleft"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"flipcard"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"tabs"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"grid"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"accordion"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"Accordion"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"popup"==h3_ele[i].textContent&&(h3_ele[i].style.display="none"),"fluidvideo"==h3_ele[i].textContent&&(h3_ele[i].style.display="none");var h4_ele=document.querySelectorAll("h4");if(h4_ele.length)for(i=0;i<h4_ele.length;i++)"front"==h4_ele[i].textContent&&(h4_ele[i].style.display="none"),"back"==h4_ele[i].textContent&&(h4_ele[i].style.display="none"),"column"==h4_ele[i].textContent&&(h4_ele[i].style.display="none");var instr_ele=document.getElementsByClassName("oneDeakin_instructions");for(i=0;i<instr_ele.length;++i)(item=instr_ele[i]).style.display="none";var hr_ele=document.getElementsByTagName("hr");for(i=0;i<hr_ele.length;++i)(item=hr_ele[i]).style.display="none"}
</script></div>
<div style="position: static !important;"></div>
<div style="position: static !important;"></div>
<div style="position: static !important;"></div></body></html>