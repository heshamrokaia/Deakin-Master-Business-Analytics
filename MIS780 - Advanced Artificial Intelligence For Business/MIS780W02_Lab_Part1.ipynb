{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP2Th8B0LLey"
      },
      "source": [
        "## MIS780 - Advanced Artificial Intelligence for Business\n",
        "\n",
        "## Week 2 - Part 1: Natual Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0vSWcazLLe3"
      },
      "source": [
        "In this session, you will get familar with Python natural language processing took kit (NLTK) and perform various text processing operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzrilAsu8Qoe"
      },
      "source": [
        "## Table of Content\n",
        "   \n",
        "1. [Text Processing](#cell_Processing)\n",
        "    - [Tokenization](#cell_Tokenization)\n",
        "    - [Stemming](#cell_Stemmers)\n",
        "    - [From Lists to String](#cell_Lists)\n",
        "    - [Sentence Segmentation](#cell_Sentence)\n",
        "    \n",
        "    \n",
        "2. [Tagging Words](#cell_Tagging)\n",
        "    - [Using a Tagger](#cell_Tagger)\n",
        "    - [Representing Tagged Tokens](#cell_Representing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0biwbV3S8Qpt"
      },
      "source": [
        "<a id = \"cell_Processing\"></a>\n",
        "### **1. Text Processing**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the NLTK toolbox before we start\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuNris8n891h",
        "outputId": "162c8730-796a-442c-b3c6-ac7a663b5349"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"cell_Tokenization\"></a>\n",
        "###  Tokenization\n",
        "\n",
        "First, we need to define the data we will use in this section:"
      ],
      "metadata": {
        "id": "0_dFJkEO9A9g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzPnO9N48Qpu",
        "outputId": "8380a33b-4d45-4c9e-bba2-1fd85f6ac7df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Online activities such as articles, website text, blog posts, social media posts are generating unstructured textual data. Corporate and business need to analyze textual data to understand customer activities, opinion, and feedback to successfully derive their business.\n"
          ]
        }
      ],
      "source": [
        "raw = '''Online activities such as articles, website text, blog posts, \\\n",
        "social media posts are generating unstructured textual data. \\\n",
        "Corporate and business need to analyze textual data to understand \\\n",
        "customer activities, opinion, and feedback to successfully derive their business.'''\n",
        "print(raw)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the datatype of raw\n",
        "type(raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB0082Dnqns0",
        "outputId": "44b2707f-1caa-4c78-e526-ba86cf7bcbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxBBLi3H8Qpw"
      },
      "source": [
        "Split text into tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKkSGonF8Qpw",
        "outputId": "cb54e7c0-79f6-482d-b2ae-5e05e54aad0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Online', 'activities', 'such', 'as', 'articles', ',', 'website', 'text', ',', 'blog', 'posts', ',', 'social', 'media', 'posts', 'are', 'generating', 'unstructured', 'textual', 'data', '.', 'Corporate', 'and', 'business', 'need', 'to', 'analyze', 'textual', 'data', 'to', 'understand', 'customer', 'activities', ',', 'opinion', ',', 'and', 'feedback', 'to', 'successfully', 'derive', 'their', 'business', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(raw)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRKePkIz8Qpx"
      },
      "source": [
        "<a id = \"cell_Stemmers\"></a>\n",
        "### Stemming\n",
        "\n",
        "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. We use `PorterStemmer()` function to perform stemming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y31u1Tm8Qpx",
        "outputId": "da73d2e0-241b-416a-d1a7-e09528158df5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['onlin',\n",
              " 'activ',\n",
              " 'such',\n",
              " 'as',\n",
              " 'articl',\n",
              " ',',\n",
              " 'websit',\n",
              " 'text',\n",
              " ',',\n",
              " 'blog',\n",
              " 'post',\n",
              " ',',\n",
              " 'social',\n",
              " 'media',\n",
              " 'post',\n",
              " 'are',\n",
              " 'gener',\n",
              " 'unstructur',\n",
              " 'textual',\n",
              " 'data',\n",
              " '.',\n",
              " 'corpor',\n",
              " 'and',\n",
              " 'busi',\n",
              " 'need',\n",
              " 'to',\n",
              " 'analyz',\n",
              " 'textual',\n",
              " 'data',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'custom',\n",
              " 'activ',\n",
              " ',',\n",
              " 'opinion',\n",
              " ',',\n",
              " 'and',\n",
              " 'feedback',\n",
              " 'to',\n",
              " 'success',\n",
              " 'deriv',\n",
              " 'their',\n",
              " 'busi',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from nltk import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "[porter.stem(t) for t in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSyVkkX98Qpy"
      },
      "source": [
        "<a id = \"cell_Sentence\"></a>\n",
        "### Sentence Segmentation\n",
        "\n",
        "Raw text can be splitted into sentences using `sent_tokenize()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M0Tsw8c8Qpz",
        "outputId": "e02a2405-8518-4866-91f4-2b582039f8b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Online activities such as articles, website text, blog posts, social media posts are generating unstructured textual data.',\n",
              " 'Corporate and business need to analyze textual data to understand customer activities, opinion, and feedback to successfully derive their business.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sents =  sent_tokenize(raw)\n",
        "[s for s in sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPe-Ooh78Qp0"
      },
      "source": [
        "<a id = \"cell_Stopwords\"></a>\n",
        "### Stopwords Removal\n",
        "\n",
        "Stopwords considered as noise in the text. Text may contain stop words such as *is*, *am*, *are*, *this*, *a*, *an*, *the*, *etc.*\n",
        "\n",
        "In NLTK for removing stopwords, you need to create a list of stopwords and filter out your list of tokens from these words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Huph2zkh8Qp0",
        "outputId": "8e9c2682-ca09-47b6-ffe3-881ad1922285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'further', 'should', \"wouldn't\", 'is', 'there', 'for', 'we', \"couldn't\", 'isn', 'once', 'again', 'didn', 'yours', 'being', 'he', \"isn't\", 'mightn', 'weren', 'who', \"he'd\", 'own', 'with', 'a', 'into', 'now', 'it', 'can', \"shan't\", 'up', 'yourselves', 'here', 'that', 'y', 'were', 'shan', 'the', 'under', 'hadn', 'why', 'over', 'of', 'some', 'these', 'won', \"we're\", 'not', 'hasn', \"it'd\", 'other', 'them', 'too', 'your', 'i', 'most', \"doesn't\", 'me', \"shouldn't\", \"we've\", \"needn't\", 'wasn', 'no', 'will', 'ours', 'be', 'my', 'ourselves', 'aren', 'each', \"haven't\", 'at', 'our', \"she'll\", 'an', \"you've\", 'don', 'they', 'doing', 'herself', \"i'm\", 'ma', 'mustn', 'where', 'only', 'few', 'haven', 'below', 'same', \"it'll\", \"they've\", \"hadn't\", 'how', 'd', 'this', 'all', 'did', \"you're\", 'but', \"they're\", 'his', 'very', 'its', \"he's\", 'those', 'am', \"mustn't\", 'have', 'nor', 'has', 'before', 'had', 'shouldn', 'been', 'through', 'm', \"she's\", 'or', 'from', 'doesn', 'was', \"you'll\", \"weren't\", 'having', 'theirs', \"mightn't\", \"i'd\", 'while', \"you'd\", \"didn't\", 'whom', 'as', 'needn', \"won't\", 'hers', 'any', 'then', 'you', \"wasn't\", 'do', 's', 'wouldn', 'couldn', 'until', \"they'd\", \"hasn't\", 're', 'in', \"aren't\", 'ain', 'what', 'when', 'above', 'which', 've', \"i've\", 'she', \"we'd\", 'after', 'on', 'themselves', \"they'll\", 'and', 'so', \"he'll\", 'o', 'than', 'more', 'if', 'are', \"i'll\", 'between', \"we'll\", 'such', 'myself', 'll', \"should've\", 'yourself', 'off', \"don't\", 'itself', 'to', 'her', 'himself', 't', 'by', 'just', 'about', 'him', 'both', 'down', \"she'd\", \"that'll\", 'because', \"it's\", 'out', 'during', 'does', 'their', 'against'}\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBivaXAL8Qp1",
        "outputId": "499f4742-fa6f-4589-c5ee-e4d75a6f1c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentence: ['Online', 'activities', 'such', 'as', 'articles', ',', 'website', 'text', ',', 'blog', 'posts', ',', 'social', 'media', 'posts', 'are', 'generating', 'unstructured', 'textual', 'data', '.', 'Corporate', 'and', 'business', 'need', 'to', 'analyze', 'textual', 'data', 'to', 'understand', 'customer', 'activities', ',', 'opinion', ',', 'and', 'feedback', 'to', 'successfully', 'derive', 'their', 'business', '.']\n",
            "Filterd Sentence: ['Online', 'activities', 'articles', ',', 'website', 'text', ',', 'blog', 'posts', ',', 'social', 'media', 'posts', 'generating', 'unstructured', 'textual', 'data', '.', 'Corporate', 'business', 'need', 'analyze', 'textual', 'data', 'understand', 'customer', 'activities', ',', 'opinion', ',', 'feedback', 'successfully', 'derive', 'business', '.']\n"
          ]
        }
      ],
      "source": [
        "filtered_sent=[]\n",
        "for w in tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sent.append(w)\n",
        "print(\"Tokenized Sentence:\",tokens)\n",
        "print(\"Filterd Sentence:\",filtered_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk1Seckt8Qp2"
      },
      "source": [
        "<a id = \"cell_Lists\"></a>\n",
        "### From Lists to String\n",
        "\n",
        "Convert list of tokens back into string. Use `' '.join(tokens)` to take all the items in `tokens` and concatenate them as one bigstring, using `' '` as a spacer between the items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "SdYQXaAV8Qp2",
        "outputId": "925a2975-19f5-4d1a-9c58-c2ed8fa6cd85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Online', 'activities', 'such', 'as', 'articles', ',', 'website', 'text', ',', 'blog', 'posts', ',', 'social', 'media', 'posts', 'are', 'generating', 'unstructured', 'textual', 'data', '.', 'Corporate', 'and', 'business', 'need', 'to', 'analyze', 'textual', 'data', 'to', 'understand', 'customer', 'activities', ',', 'opinion', ',', 'and', 'feedback', 'to', 'successfully', 'derive', 'their', 'business', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Online activities such as articles , website text , blog posts , social media posts are generating unstructured textual data . Corporate and business need to analyze textual data to understand customer activities , opinion , and feedback to successfully derive their business .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "print(tokens)\n",
        "' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp8nOent8Qp4"
      },
      "source": [
        "<a id = \"cell_Tagging\"></a>\n",
        "### **2. Tagging Words**\n",
        "\n",
        "The process of classifying words into their **parts-of-speech** and labeling them accordingly is known as **part-of-speech tagging**, **POS tagging**, or simply **tagging**. Parts-of-speech are also known as **lexical categories**. The collection of tags used for a particular task is known as a **tagset**.\n",
        "\n",
        "<a id = \"cell_Tagger\"></a>\n",
        "### Using a Tagger\n",
        "\n",
        "POS tagger, processes a sequence of words, and attaches a part of speech tag to each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQZ2DGBS8Qp5",
        "outputId": "703ceffe-7d51-4916-f372-d44e70cbcd04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('And', 'CC'),\n",
              " ('now', 'RB'),\n",
              " ('for', 'IN'),\n",
              " ('something', 'NN'),\n",
              " ('completely', 'RB'),\n",
              " ('different', 'JJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "text = nltk.word_tokenize(\"And now for something completely different\")\n",
        "nltk.pos_tag(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWFsipFV8Qp5"
      },
      "source": [
        "Here we see that `and` is **CC**, a coordinating conjunction; `now` and `completely` are **RB**, or adverbs; `for` is **IN**, a preposition; `something` is **NN**, a noun; and `different` is **JJ**, an adjective.\n",
        "\n",
        "Let’s look at another example, this time including some homonyms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHvMxf798Qp6",
        "outputId": "0d7b316b-a509-4efe-8f66-356f4f6b4b0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('They', 'PRP'),\n",
              " ('refuse', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('permit', 'VB'),\n",
              " ('us', 'PRP'),\n",
              " ('to', 'TO'),\n",
              " ('obtain', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('refuse', 'NN'),\n",
              " ('permit', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
        "nltk.pos_tag(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grdf273L8Qp7"
      },
      "source": [
        "<a id = \"cell_Representing\"></a>\n",
        "### Representing Tagged Tokens\n",
        "\n",
        "By convention in NLTK, a tagged token is represented using a tuple consisting of the token and the tag. We can create one of these special tuples from the standard string representation of a tagged token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9W2o5Ox8Qp7",
        "outputId": "bdce11f0-4bca-4890-bdca-045d01e89a07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fly', 'NN')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "tagged_token = nltk.tag.str2tuple('fly/NN')\n",
        "tagged_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WBeOGbU_8Qp8",
        "outputId": "9b333a0e-738e-44ea-e998-7ae15a809441"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "tagged_token[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MJEMHLyR8Qp9",
        "outputId": "0ff42b4f-a6e7-4953-98ba-01b50c874c42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tagged_token[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKqsG9Y8Qp-"
      },
      "source": [
        "We can construct a list of tagged tokens directly from a string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHtK4MPY8Qp-",
        "outputId": "bdef0427-c485-47bf-ee00-f117381ca74f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The/AT',\n",
              " 'grand/JJ',\n",
              " 'jury/NN',\n",
              " 'commented/VBD',\n",
              " 'on/IN',\n",
              " 'a/AT',\n",
              " 'number/NN',\n",
              " 'of/IN.',\n",
              " 'other/AP',\n",
              " 'topics/NNS',\n",
              " ',/,',\n",
              " 'AMONG/IN',\n",
              " 'them/PPO',\n",
              " 'the/AT',\n",
              " 'Atlanta/NP',\n",
              " 'and/CC',\n",
              " 'Fulton/NP-tl',\n",
              " 'County/NN-tl',\n",
              " 'purchasing/VBG',\n",
              " 'departments/NNS',\n",
              " 'which/WDT',\n",
              " 'it/PPS',\n",
              " 'said/VBD',\n",
              " '``/``',\n",
              " 'ARE/BER',\n",
              " 'well/QL',\n",
              " 'operated/VBN',\n",
              " 'and/CC',\n",
              " 'follow/VB',\n",
              " 'generally/RB',\n",
              " 'accepted/VBN',\n",
              " 'practices/NNS',\n",
              " 'which/WDT',\n",
              " 'inure/VB',\n",
              " 'to/IN',\n",
              " 'the/AT',\n",
              " 'best/JJT',\n",
              " 'interest/NN',\n",
              " 'of/IN',\n",
              " 'both/ABX',\n",
              " 'governments/NNS',\n",
              " \"''/''\",\n",
              " './.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "sent = '''The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN. \\\n",
        "other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC \\\n",
        "Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS \\\n",
        "said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB \\\n",
        "accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT \\\n",
        "interest/NN of/IN both/ABX governments/NNS ''/'' ./.'''\n",
        "sent.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUu4aIku8Qp_",
        "outputId": "3956ec89-a754-4500-95cc-5af05146ceb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'AT'),\n",
              " ('grand', 'JJ'),\n",
              " ('jury', 'NN'),\n",
              " ('commented', 'VBD'),\n",
              " ('on', 'IN'),\n",
              " ('a', 'AT'),\n",
              " ('number', 'NN'),\n",
              " ('of', 'IN.'),\n",
              " ('other', 'AP'),\n",
              " ('topics', 'NNS'),\n",
              " (',', ','),\n",
              " ('AMONG', 'IN'),\n",
              " ('them', 'PPO'),\n",
              " ('the', 'AT'),\n",
              " ('Atlanta', 'NP'),\n",
              " ('and', 'CC'),\n",
              " ('Fulton', 'NP-TL'),\n",
              " ('County', 'NN-TL'),\n",
              " ('purchasing', 'VBG'),\n",
              " ('departments', 'NNS'),\n",
              " ('which', 'WDT'),\n",
              " ('it', 'PPS'),\n",
              " ('said', 'VBD'),\n",
              " ('``', '``'),\n",
              " ('ARE', 'BER'),\n",
              " ('well', 'QL'),\n",
              " ('operated', 'VBN'),\n",
              " ('and', 'CC'),\n",
              " ('follow', 'VB'),\n",
              " ('generally', 'RB'),\n",
              " ('accepted', 'VBN'),\n",
              " ('practices', 'NNS'),\n",
              " ('which', 'WDT'),\n",
              " ('inure', 'VB'),\n",
              " ('to', 'IN'),\n",
              " ('the', 'AT'),\n",
              " ('best', 'JJT'),\n",
              " ('interest', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('both', 'ABX'),\n",
              " ('governments', 'NNS'),\n",
              " (\"''\", \"''\"),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "[nltk.tag.str2tuple(t) for t in sent.split()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGiv-Nx38QqA"
      },
      "source": [
        "### References:\n",
        "\n",
        "- Bird, S., Klein, E., & Loper, E. (2009). Natual Language Processing with Python. O’Reilly Media, Sebastopol, CA 95472. https://www.oreilly.com/library/view/natural-language-processing/9780596803346/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}