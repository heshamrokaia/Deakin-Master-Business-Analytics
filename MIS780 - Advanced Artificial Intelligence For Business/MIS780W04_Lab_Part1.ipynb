{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChzP7bUpwHUi"
      },
      "source": [
        "## MIS780 - Advanced Artificial Intelligence for Business\n",
        "\n",
        "## Week 4 - Part 1: Multi-layer Perceptron for Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiXx2H2CwHUj"
      },
      "source": [
        "In this notebook, we will perform Ames house price prediction using Deep Learning models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFYEI2kewHUj"
      },
      "source": [
        "## Table of Content\n",
        "   \n",
        "   \n",
        "1. [Preparation](#cell_Preparation)    \n",
        "    \n",
        "    \n",
        "2. [Ames real-estate data](#cell_Ames)\n",
        "\n",
        "\n",
        "3. [Deep Learning with Sequential Model](#cell_deep)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "echY0d4FwHUk"
      },
      "source": [
        "<a id = \"cell_Preparation\"></a>\n",
        "## 1. Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxmFLi1mwHUk"
      },
      "source": [
        "Load some standard Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJX4EQQlwHUl"
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import math\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI8atDxAwHUl"
      },
      "source": [
        "Next, load `Sklearn` and its wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQtV9mb8wHUm"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyH3eaXkwHUm"
      },
      "source": [
        "Some options to control Pandas display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Md-7pQwHUm"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zutLPbntwHUm"
      },
      "source": [
        "<a id = \"cell_Ames\"></a>\n",
        "## 2. Ames real-estate data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwaJyn-HwHUn"
      },
      "source": [
        "Upload the provided data set `ames_house_data.csv` to Google Colab and run the below code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dCgZMhJwHUn"
      },
      "source": [
        "ames_data_org = pd.read_csv(\"ames_house_data.csv\")\n",
        "ames_data_org.set_index('PID', inplace=True)\n",
        "ames_data_org.head(10)\n",
        "print('Number of records read: ', ames_data_org.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSCGGxxvwHUo"
      },
      "source": [
        "Find the column types and the number of missing values in each column<br>\n",
        "Note that we can also use: `ames_data_org.info()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXoMTqgUwHUo",
        "scrolled": true
      },
      "source": [
        "# Finding column types\n",
        "ames_data_org.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYQH_TIQwHUo",
        "scrolled": true
      },
      "source": [
        "# Identification of missing values\n",
        "missing = ames_data_org.isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "missing.sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f_3QgCawHUp"
      },
      "source": [
        "Drop columns with lots of missing values then show statistics about each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNWrPH8AwHUp"
      },
      "source": [
        "ames_data_org.drop(['Pool_QC', 'Misc_Feature', 'Alley', 'Fence', 'Fireplace_Qu'], axis=1, inplace=True)\n",
        "ames_data_org.describe(include='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm-8e1c-wHUp"
      },
      "source": [
        "Select numeric columns and a few \"promising\" one-hot-encoded categorical variables.<br>\n",
        "Note to avoid those columns with huge class unbalance, i.e. those where `freq` is approximately `equal` to count!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYnAkvcdwHUp"
      },
      "source": [
        "ames_data_num = ames_data_org.select_dtypes(include='number')\n",
        "ames_data_hstyle= pd.get_dummies(ames_data_org['House_Style'], prefix='HStyle')\n",
        "ames_data_area= pd.get_dummies(ames_data_org['Neighborhood'], prefix='Area')\n",
        "ames_data = pd.concat([ames_data_num, ames_data_hstyle, ames_data_area], axis=1, join='inner')\n",
        "label_col = 'SalePrice'\n",
        "ames_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vorjond-wHUq"
      },
      "source": [
        "Split data for training and validation. Split index ranges into three parts, however, ignore the third."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69-2j8WfwHUq"
      },
      "source": [
        "train_size, valid_size, test_size = (0.7, 0.3, 0.0)\n",
        "ames_train, ames_valid = train_test_split(ames_data,\n",
        "                                      test_size=valid_size,\n",
        "                                      random_state=2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4TNw3BjwHUq"
      },
      "source": [
        "Extract data for training and validation into x and y vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m21SpxTwHUq"
      },
      "source": [
        "ames_y_train = ames_train[[label_col]]\n",
        "ames_x_train = ames_train.drop(label_col, axis=1)\n",
        "ames_y_valid = ames_valid[[label_col]]\n",
        "ames_x_valid = ames_valid.drop(label_col, axis=1)\n",
        "\n",
        "print('Size of training set: ', len(ames_x_train))\n",
        "print('Size of validation set: ', len(ames_x_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the data can be applied to a deep learning model. Missing values needs to be dealed with, and the data needs to be scaled to `[-1,1]` range.\n",
        "\n",
        "Create an imputation model using training set and use it to impute both training and validation data."
      ],
      "metadata": {
        "id": "gKP3cY6RmzfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Missing training values before imputation = ', ames_x_train.isnull().sum().sum())\n",
        "print('Missing validation values before imputation = ', ames_x_valid.isnull().sum().sum())\n",
        "\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean').fit(ames_x_train)\n",
        "ames_x_train = pd.DataFrame(imputer.transform(ames_x_train),\n",
        "                            columns = ames_x_train.columns, index = ames_x_train.index)\n",
        "ames_x_valid = pd.DataFrame(imputer.transform(ames_x_valid),\n",
        "                            columns = ames_x_valid.columns, index = ames_x_valid.index)\n",
        "\n",
        "print('Missing training values after imputation = ', ames_x_train.isnull().sum().sum())\n",
        "print('Missing validation values after imputation = ', ames_x_valid.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "EC5yYXNwm2Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create a scaling model using training set and use it to scale both training and validation data."
      ],
      "metadata": {
        "id": "YEiwV1fxmuNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1), copy=True).fit(ames_x_train)\n",
        "ames_x_train = pd.DataFrame(scaler.transform(ames_x_train),\n",
        "                            columns = ames_x_train.columns, index = ames_x_train.index)\n",
        "ames_x_valid = pd.DataFrame(scaler.transform(ames_x_valid),\n",
        "                            columns = ames_x_valid.columns, index = ames_x_valid.index)\n",
        "\n",
        "print('X train min =', round(ames_x_train.min().min(),4), '; max =', round(ames_x_train.max().max(), 4))\n",
        "print('X valid min =', round(ames_x_valid.min().min(),4), '; max =', round(ames_x_valid.max().max(), 4))"
      ],
      "metadata": {
        "id": "t8EimqBLmqb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ames_x_valid.head(10)"
      ],
      "metadata": {
        "id": "vs-3ujk9m5rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJyQOYWAacBt"
      },
      "source": [
        "<a id = \"cell_deep\"></a>\n",
        "## 3. Deep Learning with Sequential Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4ETqTZueo-2"
      },
      "source": [
        "Load required libraries for Deep Learning with Sequential model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFyxtwOnepHM"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Nadam, RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSRuaCHqacBu"
      },
      "source": [
        "Convert pandas data frames to `np` arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo8HhGy7acBu"
      },
      "source": [
        "arr_x_train = np.array(ames_x_train)\n",
        "arr_y_train = np.array(ames_y_train)\n",
        "arr_x_valid = np.array(ames_x_valid)\n",
        "arr_y_valid = np.array(ames_y_valid)\n",
        "\n",
        "print('Training shape:', arr_x_train.shape)\n",
        "print('Training samples: ', arr_x_train.shape[0])\n",
        "print('Validation samples: ', arr_x_valid.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w4bb1BfacBu"
      },
      "source": [
        "Create several **Keras models** for experiment purpose.\n",
        "\n",
        "The first is very simple, consisting of two layers and `Adam` optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l25gkO1LacBu"
      },
      "source": [
        "def basic_model_1(x_size, y_size):\n",
        "    t_model = Sequential()\n",
        "    t_model.add(Dense(100, activation=\"relu\", input_shape=(x_size,)))\n",
        "    t_model.add(Dense(y_size))\n",
        "    t_model.compile(\n",
        "        loss='mean_squared_error',\n",
        "        optimizer=RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-07, weight_decay=0.0),\n",
        "        metrics=[metrics.mae])\n",
        "    return(t_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Fg6R9HacBv"
      },
      "source": [
        "The second with `RMSProp` optimizer consists of 4 layers and the first uses 20% dropouts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y-gY550acBv"
      },
      "source": [
        "def basic_model_2(x_size, y_size):\n",
        "    t_model = Sequential()\n",
        "    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n",
        "    t_model.add(Dropout(0.2))\n",
        "    t_model.add(Dense(180, activation=\"relu\"))\n",
        "    t_model.add(Dense(20, activation=\"relu\"))\n",
        "    t_model.add(Dense(y_size))\n",
        "    t_model.compile(\n",
        "        loss='mean_squared_error',\n",
        "        optimizer=RMSprop(learning_rate=0.005, rho=0.9, momentum=0.0, epsilon=1e-07, weight_decay=0.0,),\n",
        "        metrics=[metrics.mae])\n",
        "    return(t_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl8zuxxRacBv"
      },
      "source": [
        "Now we create the executable model using one of the above functions. Run below code until the end to obtain the result, then change `basic_model_1` to `basic_model_2` and run the code again. Compare the results generated by the two models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMNkYYzmacBv"
      },
      "source": [
        "model = basic_model_1(arr_x_train.shape[1], arr_y_train.shape[1])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9QMM7Gemk5u"
      },
      "source": [
        "Specify Keras callbacks which allow additional functionality while the model is being fitted. ***EarlyStopping*** watches one of the model measurements and stops fitting when no improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr-F4DI9acBv"
      },
      "source": [
        "Fit the model and record the history of training and validation.\n",
        "As we specified `EarlyStopping` with `patience=20`, with luck the training will stop in less than 200 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX0JGImQacBw"
      },
      "source": [
        "history = model.fit(arr_x_train, arr_y_train,\n",
        "    batch_size=64,\n",
        "    epochs=500,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(arr_x_valid, arr_y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrdM2IqAacBw"
      },
      "source": [
        "Evaluate and report performance of the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD-TiME2acBw"
      },
      "source": [
        "train_score = model.evaluate(arr_x_train, arr_y_train, verbose=0)\n",
        "valid_score = model.evaluate(arr_x_valid, arr_y_valid, verbose=0)\n",
        "\n",
        "print('Train MAE: ', round(train_score[1], 2), ', Train Loss: ', round(train_score[0], 2))\n",
        "print('Val MAE: ', round(valid_score[1], 2), ', Val Loss: ', round(valid_score[0], 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now plot the true vs. predicted values."
      ],
      "metadata": {
        "id": "fO8Al1izejo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_predict = model.predict(arr_x_valid)\n",
        "# plot\n",
        "plt.scatter(arr_y_valid, y_valid_predict)\n",
        "plt.ylabel('arr_y_valid')\n",
        "plt.xlabel('y_valid_predict')\n",
        "plt.show()\n",
        "\n",
        "corr_result = np.corrcoef(arr_y_valid.reshape(1,879)[0], y_valid_predict.reshape(1,879)[0])\n",
        "print('The Correlation between true and predicted values is: ',round(corr_result[0,1],3))\n",
        "\n"
      ],
      "metadata": {
        "id": "7f0_PTOKejFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpZwEKa4acBw"
      },
      "source": [
        "Now plot the training history, i.e. the *Mean Absolute Error* and *Loss (Mean Squared Error)*, which were both defined at the time of model compilation.\n",
        "\n",
        "Note that the plot shows validation error as less than training error, which is quite deceptive. The reason for this is that training error is calculated for the entire epoch (and at its begining it was much worse than at the end), whereas the validation error is taken from the last batch (after the model improved). See the above evaluation statistics to confirm that the evaluation puts these errors in the correct order at the very end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXSrN9L4gehg"
      },
      "source": [
        "def plot_hist(h, xsize=6, ysize=5):\n",
        "    # Prepare plotting\n",
        "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "    plt.rcParams[\"figure.figsize\"] = [xsize, ysize]\n",
        "\n",
        "    # Get training and validation keys\n",
        "    ks = list(h.keys())\n",
        "    n2 = math.floor(len(ks)/2)\n",
        "    train_keys = ks[0:n2]\n",
        "    valid_keys = ks[n2:2*n2]\n",
        "\n",
        "    # summarize history for different metrics\n",
        "    for i in range(n2):\n",
        "        plt.plot(h[train_keys[i]])\n",
        "        plt.plot(h[valid_keys[i]])\n",
        "        plt.title('Training vs Validation '+train_keys[i])\n",
        "        plt.ylabel(train_keys[i])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.draw()\n",
        "        plt.show()\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MJXDRcwacBw"
      },
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "\n",
        "# Plot history\n",
        "plot_hist(hist, xsize=6, ysize=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdoJn_niwHUy"
      },
      "source": [
        "### References:\n",
        "\n",
        "- Pathak, M. (2019). Using XGBoost in Python. https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
        "- XGBoost GPU Support https://xgboost.readthedocs.io/en/latest/gpu/\n",
        "- Agarwal, R. (2020). Lightning Fast XGBoost on Multiple GPUs. https://towardsdatascience.com/lightning-fast-xgboost-on-multiple-gpus-32710815c7c3"
      ]
    }
  ]
}