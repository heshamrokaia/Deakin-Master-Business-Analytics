{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChzP7bUpwHUi"
      },
      "source": [
        "## MIS780 - Artificial Intelligence for Business\n",
        "\n",
        "## Week 4 - Part 2: Multi-layer Perceptron for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiXx2H2CwHUj"
      },
      "source": [
        "In this notebook, we will perform prediction of default of credit card clients using Deep Learning models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFYEI2kewHUj"
      },
      "source": [
        "## Table of Content\n",
        "   \n",
        "   \n",
        "1. [Preparation](#cell_Preparation)    \n",
        "    \n",
        "    \n",
        "2. [Credit Card Client Data](#cell_Ames)\n",
        "\n",
        "\n",
        "3. [Deep Learning with Sequential Model](#cell_deep)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "echY0d4FwHUk"
      },
      "source": [
        "<a id = \"cell_Preparation\"></a>\n",
        "## 1. Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxmFLi1mwHUk"
      },
      "source": [
        "Load some standard Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJX4EQQlwHUl"
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import math\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI8atDxAwHUl"
      },
      "source": [
        "Next, load `Sklearn` and its wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQtV9mb8wHUm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyH3eaXkwHUm"
      },
      "source": [
        "Some options to control Pandas display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Md-7pQwHUm"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zutLPbntwHUm"
      },
      "source": [
        "<a id = \"cell_Ames\"></a>\n",
        "## 2. Credit Card Client data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwaJyn-HwHUn"
      },
      "source": [
        "The data set for this exercise is `default_of_credit_card_clients.csv`, which can be accessed from Cloud Deakin. Description about this data set can be accessed from [Kaggle website](https://www.kaggle.com/datasets/mariosfish/default-of-credit-card-clients). The aim of this exercise is to predict the class value of `dpnm` column (`1` for default, and `0` for not-default)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dCgZMhJwHUn"
      },
      "source": [
        "credit_data_org = pd.read_csv(\"default_of_credit_card_clients.csv\")\n",
        "print('Number of records read: ', credit_data_org.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_data_org.head(10)"
      ],
      "metadata": {
        "id": "xjSs_2tV0QFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSCGGxxvwHUo"
      },
      "source": [
        "Find the column types and the number of missing values in each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXoMTqgUwHUo",
        "scrolled": true
      },
      "source": [
        "# Finding column types\n",
        "credit_data_org.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYQH_TIQwHUo",
        "scrolled": true
      },
      "source": [
        "# Check for missing values\n",
        "missing = credit_data_org.isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "missing.sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove the ID Column\n",
        "credit_data_org = credit_data_org.drop('ID', axis=1)\n",
        "credit_data_org.head()\n"
      ],
      "metadata": {
        "id": "o_QtGGDqTGed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vorjond-wHUq"
      },
      "source": [
        "Split data for training and validation. Split index ranges into three parts, however, ignore the third."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69-2j8WfwHUq"
      },
      "source": [
        "train_size, valid_size, test_size = (0.7, 0.3, 0.0)\n",
        "credit_train, credit_valid = train_test_split(credit_data_org,\n",
        "                                      test_size=valid_size,\n",
        "                                      random_state=2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4TNw3BjwHUq"
      },
      "source": [
        "Extract data for training and validation into x and y vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m21SpxTwHUq"
      },
      "source": [
        "label_col = 'dpnm'\n",
        "\n",
        "credit_y_train = credit_train[[label_col]]\n",
        "credit_x_train = credit_train.drop(label_col, axis=1)\n",
        "credit_y_valid = credit_valid[[label_col]]\n",
        "credit_x_valid = credit_valid.drop(label_col, axis=1)\n",
        "\n",
        "print('Size of training set: ', len(credit_x_train))\n",
        "print('Size of validation set: ', len(credit_x_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a scaling model using training set and use it to scale both training and validation data."
      ],
      "metadata": {
        "id": "YEiwV1fxmuNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1), copy=True).fit(credit_x_train)\n",
        "credit_x_train = pd.DataFrame(scaler.transform(credit_x_train),\n",
        "                            columns = credit_x_train.columns, index = credit_x_train.index)\n",
        "credit_x_valid = pd.DataFrame(scaler.transform(credit_x_valid),\n",
        "                            columns = credit_x_valid.columns, index = credit_x_valid.index)\n",
        "\n",
        "print('X train min =', round(credit_x_train.min().min(),4), '; max =', round(credit_x_train.max().max(), 4))\n",
        "print('X valid min =', round(credit_x_valid.min().min(),4), '; max =', round(credit_x_valid.max().max(), 4))"
      ],
      "metadata": {
        "id": "t8EimqBLmqb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credit_x_valid.head(10)"
      ],
      "metadata": {
        "id": "vs-3ujk9m5rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJyQOYWAacBt"
      },
      "source": [
        "<a id = \"cell_deep\"></a>\n",
        "## 3. Deep Learning with Sequential Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4ETqTZueo-2"
      },
      "source": [
        "Load required libraries for Deep Learning with Sequential model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFyxtwOnepHM"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Nadam, RMSprop\n",
        "from tensorflow.keras.losses import categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSRuaCHqacBu"
      },
      "source": [
        "Convert pandas data frames to `np` arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo8HhGy7acBu"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "arr_x_train = np.array(credit_x_train)\n",
        "arr_y_train = np.array(credit_y_train)\n",
        "arr_x_valid = np.array(credit_x_valid)\n",
        "arr_y_valid = np.array(credit_y_valid)\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "arr_y_train = to_categorical(arr_y_train, 2)\n",
        "arr_y_valid = to_categorical(arr_y_valid, 2)\n",
        "\n",
        "print('Train shape: x=', arr_x_train.shape, ', y=', arr_y_train.shape)\n",
        "print('Test shape: x=', arr_x_valid.shape, ', y=', arr_y_valid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w4bb1BfacBu"
      },
      "source": [
        "Create  **Keras model** for experiment purpose.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l25gkO1LacBu"
      },
      "source": [
        "def basic_model_1():\n",
        "    t_model = Sequential()\n",
        "    t_model.add(Dense(100, activation=\"relu\", input_shape=(23,)))\n",
        "    t_model.add(Dense(2, activation='softmax'))\n",
        "    t_model.summary()\n",
        "    return(t_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl8zuxxRacBv"
      },
      "source": [
        "Now we create the executable model using one of the above functions. Run below code until the end to obtain the result, then change `basic_model_1` to `basic_model_2` and run the code again. Compare the results generated by the two models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMNkYYzmacBv"
      },
      "source": [
        "model = basic_model_1()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr-F4DI9acBv"
      },
      "source": [
        "Fit the model and record the history of training and validation.\n",
        "As we specified `EarlyStopping` with `patience=20`, with luck the training will stop in less than 200 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX0JGImQacBw"
      },
      "source": [
        "model.compile(optimizer=Nadam(learning_rate=0.005),\n",
        "              loss=categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(arr_x_train, arr_y_train,\n",
        "    batch_size=64,\n",
        "    epochs=100,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(arr_x_valid, arr_y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrdM2IqAacBw"
      },
      "source": [
        "Evaluate and report performance of the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD-TiME2acBw"
      },
      "source": [
        "train_score = model.evaluate(arr_x_train, arr_y_train, verbose=0)\n",
        "valid_score = model.evaluate(arr_x_valid, arr_y_valid, verbose=0)\n",
        "\n",
        "print('Train Accuracy: ', round(train_score[1], 2), ', Train Loss: ', round(train_score[0], 2))\n",
        "print('Val Accuracy: ', round(valid_score[1], 2), ', Val Loss: ', round(valid_score[0], 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(arr_x_valid)\n",
        "\n",
        "# Convert the predicted labels to continuous-multioutput format\n",
        "y_pred_continuous = np.round(y_pred)\n",
        "\n",
        "# Convert the predicted labels to multiclass format\n",
        "y_pred_multiclass = np.argmax(y_pred, axis=1)\n",
        "arr_y_valid = np.argmax(arr_y_valid, axis=1)\n",
        "\n",
        "# Calculate the kappa score\n",
        "kappa = cohen_kappa_score(arr_y_valid, y_pred_multiclass)\n",
        "print(\"The result of Kappa is :\", round(kappa, 3))\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(arr_y_valid, y_pred_multiclass)\n",
        "\n",
        "# Print the report\n",
        "print(\"The result of the classification report is: \\n \",report)"
      ],
      "metadata": {
        "id": "G6Vzqcbb3pMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "cm = confusion_matrix(\n",
        "    arr_y_valid,\n",
        "    y_pred_multiclass)\n",
        "\n",
        "# Create a ConfusionMatrixDisplay object\n",
        "display = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm)\n",
        "\n",
        "# Create a figure with a fixed size\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "\n",
        "# Create a subplot within the figure\n",
        "ax = fig.subplots()\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "display.plot(ax=ax)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cMZjx9ap3vSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Try to improve the prediction peformance of the model (e.g., create more complex models, oversampling the samples in minority class)"
      ],
      "metadata": {
        "id": "exv7mux989PS"
      }
    }
  ]
}