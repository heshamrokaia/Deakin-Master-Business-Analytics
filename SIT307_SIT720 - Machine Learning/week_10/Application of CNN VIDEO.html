<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<script>function lti_launch( vars, target ) {
						var query = '';
						var new_tab = false;

						for(var key in vars) {
							if(query.length == 0) {
								query += '?' + key + '=' + encodeURIComponent(vars[key]);
							}
							else {
								query += '&' + key + '=' + encodeURIComponent(vars[key]);
							}
						}

						var url = '/d2l/customization/pearsonlti/6605/Launch' + query;(target == '_blank') ? window.open( url, '_blank' ) : location.replace( url );}</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/embeds.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					window.D2L.EmbedRenderer.renderEmbeds(document.body);
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/mathjax.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					if (document.querySelector('math') || /\$\$|\\\(|\\\[|\\begin{|\\ref{|\\eqref{/.test(document.body.innerHTML)) {
						document.querySelectorAll('mspace[linebreak="newline"]').forEach(elm => {
							elm.setAttribute('style', 'display: block; height: 0.5rem;');
						});

						window.D2L.MathJax.loadMathJax({
							outputScale: 1.5,
							renderLatex: true,
							enableMML3Support: false
						});
					}
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/prism.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					document.querySelectorAll('.d2l-code').forEach(code => {
						window.D2L.Prism.formatCodeElement(code);
					});
				});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.documentElement.hasAttribute('lang')) return;
						document.documentElement.setAttribute('lang', 'en-GB'); 						
					});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.head.querySelector('title')) return;
						var handleAppendTitle = function(evt) {
							if (!evt || !evt.data) return;

							try {
								var data = JSON.parse(evt.data);
								if (data.handler !== 'd2l.iframe.requestPageTitle' || !data.pageTitle) return;

								window.removeEventListener('message', handleAppendTitle, false);

								var titleElm = document.createElement('title');
								titleElm.textContent = data.pageTitle;
								document.head.appendChild(titleElm);
							} catch (e) {}	
						};

						window.addEventListener('message', handleAppendTitle, false);
						window.parent.postMessage(JSON.stringify({ handler: 'd2l.iframe.requestPageTitle' }), '*');
					});</script><script>window.addEventListener('message', function(event) { 
					if( !event.data ) {
						return;
					}

					var params;
					try {
						params = JSON.parse( event.data );
					}
					catch {
						return;
					}
					if( !params.subject || params.subject !== 'lti.frameResize' ) {
						return;
					}

					const MAX_FRAME_HEIGHT = 10000
					if( !params.height || params.height < 1 || params.height > MAX_FRAME_HEIGHT ) {
						console.warn( 'Invalid height value received, aborting' );
						return;
					}
					var el = document.getElementsByTagName( 'iframe' );
					for ( var i=0; i < el.length; i++ ) {
						if( el[i].contentWindow === event.source ) {
							el[i].style.height = params.height + 'px';
							el[i].style.width = '100%';
							console.info( 'Setting iFrame height to ' + params.height );
							console.info( 'Setting iFrame width to 100%' );
						}
					}
				});</script></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;" data-new-gr-c-s-check-loaded="8.912.0" data-gr-ext-installed=""><h1>Application of CNN</h1>
<div></div>
<div>
<p>Let’s see some real examples. One of the things CNN has been applied to is the CIFAR 10 dataset that features 50,000 training images with 10,000 test images.</p>
<p class="centerImage"><img src="../images/Application%20of%20CNN%20image%201.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. CIFAR 10 dataset: 50,000 training images and 10,000 test images. Source (<a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html" target="_blank" rel="noopener noreferrer">Karpathy, A n.d.</a>)</h5>
<p>In a CNN every network layer acts as a&nbsp;<em>detection filter</em>&nbsp;for the presence of specific features or patterns present in the original data. The first layers in a CNN detect large features that can be recognized and interpreted relatively easily. As you can see the patch which has been shown in red (figure below) are found for interpreting the right side batch of images. Using other categories of images with this filter may not give proper results. In other words, each image filter patch is for finding a specific part of a pattern or texture.</p>
<p class="centerImage"><img src="../images/Application%20of%20CNN%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Layers 1 and 2, source (Olah n.d.)</h5>
<p>You can see the same concept rules in layers 3, 4 and 5. Remember you need a large set of data for feeding these deep networks, otherwise the results can be useless.</p>
<p class="centerImage"><img src="../images/Application%20of%20CNN%20image%203.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Layer 3, source (Olah n.d.)</h5>
<p class="centerImage"><img src="../images/Application%20of%20CNN%20image%204.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Layers 4 and 5, source (Olah n.d.)</h5>
<p>So what has helped deep learning to boost the field:</p>
<ul>
<li>The ability of modelling&nbsp;<strong>Larger models</strong>&nbsp;with new training techniques:
<ul class="subList">
<li>Dropout, Maxout, Maxnorm, ReLU,…</li>
</ul>
</li>
<li>Large ImageNet dataset
<ul class="subList">
<li>1.2 million training samples</li>
<li>1000 categories</li>
</ul>
</li>
<li>Using fast graphical processing units (GPU)
<ul class="subList">
<li>Capable of 1 trillion operations per second!</li>
</ul>
</li>
</ul>
<p>These 3 factors are the most important solutions to the many challenges deep learning was facing at first. They have allowed deep learning to be practically useful.</p>
<p align="center"><iframe src="https://deakin.au.panopto.com/Panopto/Pages/Embed.aspx?id=6a955489-c49f-463e-8604-afe70122a03a&amp;autoplay=false&amp;offerviewer=true&amp;showtitle=true&amp;showbrand=true&amp;captions=true&amp;interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen="" allow="autoplay" aria-label="Panopto Embedded Video Player"></iframe></p>
<table class="tableClear" style="width: 880px;">
<tbody>
<tr>
<td style="width: 59px;"></td>
<td style="width: 466.433px;"><a id="viewTranscript">View transcript</a></td>
<td style="width: 271.567px;"></td>
</tr>
</tbody>
</table>
<article class="js-transcript transcript" id="transcript-en">
<p class="transcript__para">SPEAKER 1: In this tutorial, we're going to show you some basic applications of CNN. So what kind of data are we usually dealing with in deep learning, or in this case, CNN? So one of the most famous ones is CIFAR 10, which is a dataset made out of 50,000 training images and 10,000 test images. The categories or classes in this dataset is made out of truck, sheep, horse, many other things. Also there is another dataset called ImageNet which has the millions of images. So as we have said before, our data is pretty important and in deep learning applications. Now let's jump into the CNN layers and see how they're working. Consider this patch.</p>
<p class="transcript__para">We call these philtres, or CNN weights, which are particularly for this type of data. For example, this one is particularly made for this type of data. So if this type of data comes in, the maximum response will be returned. So these patches are designed to pass this type of patterns. If anything else comes in, the return signal would be low. So we call this layer the low level features. As you can see in the next layer, more details-- such as tyres, birds, and people-- are available. So it means this layer has more meaningful features. We call this level mid-level layers, with more meaningful features.</p>
<p class="transcript__para">So finally, these last layers are the high-level layers of features, which you can see the pattern and the patches inside them. For example, consider this. That looks to be a bicycle. So as you can see, you can detect some sort of pattern for this-- for the wheels of the bicycle. So this was an illustration of different layers of CNN, but let us remind you again, you should have a good amount of data while working with many layers of structures, such as CNN, unless you're in danger of overfitting based on your data.</p>
</article>
<h2 id="your-task">Activity</h2>
<p>This video tutorial will provide some visualization of the CNN layers. Please discuss your understanding in the <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=1734011&amp;type=discuss&amp;rcode=DeakinUniversity-2003941" target="_top" class="kalmod-7432521-3">discussion forum</a>.</p>
<p></p>
<div>
<p class="centerVideo"><iframe src="https://www.youtube.com/embed/cNBBNAxC8l4?wmode=opaque" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen" width="648" height="364" frameborder="0"></iframe></p>
<h5>This is an additional video, hosted on YouTube.</h5>
</div>
<hr style="width: 100%; height: auto; color: #ffffff; border: 1px inset #cccccc;">
<h4 id="references">References</h4>
<p>Karpathy, A n.d.,&nbsp; <a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html" target="_blank" rel="noopener noreferrer">ConvNetJS CIFAR-10 demo</a>, viewed 30 August 2018.</p>
<p>Olah, C n.d.,<a href="http://colah.github.io/" target="_blank" rel="noopener noreferrer">Colah’s blog</a>, viewed 10 September 2018.</p>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>