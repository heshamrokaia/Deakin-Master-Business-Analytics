<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<script>function lti_launch( vars, target ) {
						var query = '';
						var new_tab = false;

						for(var key in vars) {
							if(query.length == 0) {
								query += '?' + key + '=' + encodeURIComponent(vars[key]);
							}
							else {
								query += '&' + key + '=' + encodeURIComponent(vars[key]);
							}
						}

						var url = '/d2l/customization/pearsonlti/6605/Launch' + query;(target == '_blank') ? window.open( url, '_blank' ) : location.replace( url );}</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/embeds.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					window.D2L.EmbedRenderer.renderEmbeds(document.body);
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/mathjax.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					if (document.querySelector('math') || /\$\$|\\\(|\\\[|\\begin{|\\ref{|\\eqref{/.test(document.body.innerHTML)) {
						document.querySelectorAll('mspace[linebreak="newline"]').forEach(elm => {
							elm.setAttribute('style', 'display: block; height: 0.5rem;');
						});

						window.D2L.MathJax.loadMathJax({
							outputScale: 1.5,
							renderLatex: true,
							enableMML3Support: false
						});
					}
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/prism.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					document.querySelectorAll('.d2l-code').forEach(code => {
						window.D2L.Prism.formatCodeElement(code);
					});
				});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.documentElement.hasAttribute('lang')) return;
						document.documentElement.setAttribute('lang', 'en-GB'); 						
					});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.head.querySelector('title')) return;
						var handleAppendTitle = function(evt) {
							if (!evt || !evt.data) return;

							try {
								var data = JSON.parse(evt.data);
								if (data.handler !== 'd2l.iframe.requestPageTitle' || !data.pageTitle) return;

								window.removeEventListener('message', handleAppendTitle, false);

								var titleElm = document.createElement('title');
								titleElm.textContent = data.pageTitle;
								document.head.appendChild(titleElm);
							} catch (e) {}	
						};

						window.addEventListener('message', handleAppendTitle, false);
						window.parent.postMessage(JSON.stringify({ handler: 'd2l.iframe.requestPageTitle' }), '*');
					});</script><script>window.addEventListener('message', function(event) { 
					if( !event.data ) {
						return;
					}

					var params;
					try {
						params = JSON.parse( event.data );
					}
					catch {
						return;
					}
					if( !params.subject || params.subject !== 'lti.frameResize' ) {
						return;
					}

					const MAX_FRAME_HEIGHT = 10000
					if( !params.height || params.height < 1 || params.height > MAX_FRAME_HEIGHT ) {
						console.warn( 'Invalid height value received, aborting' );
						return;
					}
					var el = document.getElementsByTagName( 'iframe' );
					for ( var i=0; i < el.length; i++ ) {
						if( el[i].contentWindow === event.source ) {
							el[i].style.height = params.height + 'px';
							el[i].style.width = '100%';
							console.info( 'Setting iFrame height to ' + params.height );
							console.info( 'Setting iFrame width to 100%' );
						}
					}
				});</script></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;" data-new-gr-c-s-check-loaded="8.912.0" data-gr-ext-installed=""><div>
<h1>Backpropagation Algorithm</h1>
</div>
<div>
<p>In this video, You will explore the backpropagation algorithm, which is used for training MLPs.</p>
<p>As you learnt in the previous lesson, the basic concept of training MLPs is a stochastic gradient-descent rule. Based on the gradient-descent rule we have already shown, you can write the updating formula for&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mi>k</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_{jk}\)"}</annotation></semantics></math>.</p>
<p>In the video you will also go through the&nbsp;<em>algorithm</em>. The input is the training data. You will initialise the weights at the start and update them until the stopping criteria (which could be accuracy or number of iterations or the amount of change) is met.</p>
<p align="center"><iframe src="https://deakin.au.panopto.com/Panopto/Pages/Embed.aspx?id=56804fd0-f632-4fc7-9e16-afe70122a040&amp;autoplay=false&amp;offerviewer=true&amp;showtitle=true&amp;showbrand=true&amp;captions=true&amp;interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen="" allow="autoplay" aria-label="Panopto Embedded Video Player"></iframe></p>
<table class="tableClear" style="width: 880px;">
<tbody>
<tr>
<td style="width: 59px;"></td>
<td style="width: 466.433px;"><a id="viewTranscript">View transcript</a></td>
<td style="width: 271.567px;"></td>
</tr>
</tbody>
</table>
<article class="js-transcript transcript" id="transcript-en">SPEAKER 1: In this tutorial, we're going to explain to you the backpropagation algorithm, which is used for training MLPs. Consider this neural network. These are the inputs. This is the hidden layer and this is the output layer. If you remember the gradient descent from the previous section, the basic concept of training MLP is a stochastic gradient descent rule. If you remember, our aim is to minimise instantaneous approximation for current training sample, xd and xy. So this is our error of function. y hat k is our approximated output for input data. And yk is the true value of the output for that data. Let us first show you how to update the weights of the hidden layer to output layer, wjko.
<p class="transcript__para">Based on the gradient descent rule we have already shown you, we can write the updating formula for wjk. It is wjko can be updated as wjk minus eta partial derivative of Etw, with respect to wjko. This is the basic formulation of gradient descent for finding the minimum value of this error function based on or with respect to these weights, which are wjk. We also define another value called y bar k, which is the unsigmoided argument value at output node k. If you remember in MLP, before returning the final value in the output layer, we will use a sigmoid function on the outputs. So consider this yk as the value before using the sigmoid on that.</p>
<p class="transcript__para">Now, the only question remains is how to find this value. Let us expand this value. So the partial derivative of Et with respect to wjk is equal to the partial derivative of Et with respect to y bar k multiplied by partial derivative of y bar k with respect to wjk. We call this the chain rule. So as you can see, we can simply cancel these two values and we still have the partial derivative of Et with respect to wjk.</p>
<p class="transcript__para">OK. So as we have said before, the y bar k is wjkzj. If you look at here, if you have these z values in here, and if you multiply them by this weight function, this is the output values before using the sigmoid. So if you know y bar k is the summation of wjkzj, now if you take the derivative with respect to wjk, the output would be zj, which as you can see, this is the value of this particular derivative. The next one, again, you're using the chain rule for the first one.</p>
<p class="transcript__para">You're writing the partial derivative of Et with respect to y bar k is equal to the partial derivative of Et with respect to y hat k multiplied by the partial derivative of y hat k with respect to y bar k. Now, this one is just a sigmoid function. If you remember, the derivative of a sigmoid function is one minus the sigmoid function multiplied by sigmoid function. So this is only 1 minus y hat k multiplied in y hat k. Also, the value of this one is pretty simple to calculate because we have this error function here. And if you take the derivative with respect to y hat k, this two will come here.</p>
<p class="transcript__para">Add it to this 1 over 2 and the minus will be added in here too. So it's minus yk minus y hat k. Now, we have to sum all these variables together to find the final update formula. We can call this 2 delta ko, which is 1 minus y hat k multiplied by y hat k. And this one is minus yk minus y hat k. So these two are making this delta k. And also, we have 1zj in here. So the updating rule for the weights from the hidden layer to the output layer is wjko, it should be updated as wjko plus eta, the delta which we found in here, and the zj.</p>
<p class="transcript__para">So this is a simple formula for updating the weights from hidden layer to the output layer. So we learned how can we update the wjk weights, the weights between the hidden layer to the output layer? But we need to update wij's too, which are the weights from input layer to the hidden layer. Let's see how can we do this. So for updating wijh, again, we're writing the error function. Also, we are defining zj as the sigmoid function over z bar j. And z bar j is exactly the same concept as y bar k. So it's the unsigmoided value of zj, which is simply the multiplication of xi and to the wijh. And there is a summation from i1 to im.</p>
<p class="transcript__para">So based on the gradient descent rule, the updating formula should be something like this. wij should be updated as wij minus eta, partial derivative of etw with respect to wijh. OK. Again, we need to find this value. So we're using the chain rule again. The chain rule says, if you can write this value as partial derivative of Et with respect to z bar j multiplied by the partial derivative of z bar j with respect to wij. So this one is simple xi. And the reason is z bar j is this. So if you take the derivative with respect to wij, what remains is only xi.</p>
<p class="transcript__para">Again, we can expand this one way by chain rule, and we can write the partial derivatives of Et with respect to z bar j equals the partial derivative of eta with respect to zj times the partial derivative of zj with respect to z bar j. So as we have said before, this value is simply the derivative of a sigmoid function, which is 1 minus the function multiplied in the function. Which is zj 1 minus zj. But what about this one? For finding the partial derivatives of Et with respect to zj, we're using the chain rule again. But there is a difference in here.</p>
<p class="transcript__para">As you can see, you can say the partial derivatives of Et with respect to y bar k multiplied by the partial derivative of y bar k with respect to zj. But in here, we didn't have the k in this formula, so we're adding the k. So we're obliged to consider all the k's. K equals 1 to k. So now, we have a summation over this value. So knowing here, for finding this quantity, you're saying the partial derivative of y bar k with respect to zj, if you consider the y bar k, it was just this one and the y bar k was this value. And the partial derivative of this with respect to zj is only wjk.</p>
<p class="transcript__para">So this quantity is wjk. And this one is also, we calculated before, and it's minus delta ko. If you remember, we calculated this for updating the weights from the hidden layer to the output layer. So now, we can write the partial derivatives of Et with respect to z bar j is minus delta jh, which is equal to minus zj, 1 minus zj, which is this value. And also, this summation, which is the summation from k equals 1 to k, wjk, delta ko. So finally, we can write the updating formula of wij2. And it's like wij should be updated as wij plus eta, delta jh xi. And the xi was from here.</p>
<p class="transcript__para">So this is the way we're using back propagation in the training MLP. Now that we have learned how to update the weights in detail, it's a good time to go through the algorithm. As we said, the input is the training data. And we are initializing the weights at the start. And until the stopping criteria is met, which could be accuracy or number of iterations or the amount of change, we are having a loop, a for loop. And we're saying for each training sample, compute the values of the hidden nodes, z and y hat, which is the hidden layer and the output layer. Then, we compute the output error like this, delta ko, as we have calculated in the previous slides.</p>
<p class="transcript__para">Also, we are computing the hidden error term for the weights from the input layer to the hidden layer, which is something like this. Now, we can update the weights based on this one. For the hidden layer to the output layer weights, we are updating like this. wjk should be updated as wjk plus eta delta k hj. And for the weights from input layer to the hidden layer. You're saying wij should be updated as wij plus eta, delta jh xi. And as the final value, we are returning the weights. Then when a new point comes in, you're just multiplying in the weights and finding the z and then you're finding the y.</p>
<p class="transcript__para">And then, we can return the final label or the value of the new training sample. So this was an explanation of how backpropagation algorithm works in MLP training.</p>
</article>
<h2 id="your-task">Activity</h2>
<p>Check this tutorial <a href="https://stevenmiller888.github.io/mind-how-to-build-a-neural-network/" target="_blank" rel="noopener noreferrer">Mind: How to Build a Neural Network</a>&nbsp;by Steven Miller about a simple multi-layer perceptron.</p>
<p>It provides several examples that will help you understand the algorithms we have discussed in this lesson.</p>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>