<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<script>function lti_launch( vars, target ) {
						var query = '';
						var new_tab = false;

						for(var key in vars) {
							if(query.length == 0) {
								query += '?' + key + '=' + encodeURIComponent(vars[key]);
							}
							else {
								query += '&' + key + '=' + encodeURIComponent(vars[key]);
							}
						}

						var url = '/d2l/customization/pearsonlti/6605/Launch' + query;(target == '_blank') ? window.open( url, '_blank' ) : location.replace( url );}</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/embeds.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					window.D2L.EmbedRenderer.renderEmbeds(document.body);
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/mathjax.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					if (document.querySelector('math') || /\$\$|\\\(|\\\[|\\begin{|\\ref{|\\eqref{/.test(document.body.innerHTML)) {
						document.querySelectorAll('mspace[linebreak="newline"]').forEach(elm => {
							elm.setAttribute('style', 'display: block; height: 0.5rem;');
						});

						window.D2L.MathJax.loadMathJax({
							outputScale: 1.5,
							renderLatex: true,
							enableMML3Support: false
						});
					}
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/prism.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					document.querySelectorAll('.d2l-code').forEach(code => {
						window.D2L.Prism.formatCodeElement(code);
					});
				});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.documentElement.hasAttribute('lang')) return;
						document.documentElement.setAttribute('lang', 'en-GB'); 						
					});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.head.querySelector('title')) return;
						var handleAppendTitle = function(evt) {
							if (!evt || !evt.data) return;

							try {
								var data = JSON.parse(evt.data);
								if (data.handler !== 'd2l.iframe.requestPageTitle' || !data.pageTitle) return;

								window.removeEventListener('message', handleAppendTitle, false);

								var titleElm = document.createElement('title');
								titleElm.textContent = data.pageTitle;
								document.head.appendChild(titleElm);
							} catch (e) {}	
						};

						window.addEventListener('message', handleAppendTitle, false);
						window.parent.postMessage(JSON.stringify({ handler: 'd2l.iframe.requestPageTitle' }), '*');
					});</script><script>window.addEventListener('message', function(event) { 
					if( !event.data ) {
						return;
					}

					var params;
					try {
						params = JSON.parse( event.data );
					}
					catch {
						return;
					}
					if( !params.subject || params.subject !== 'lti.frameResize' ) {
						return;
					}

					const MAX_FRAME_HEIGHT = 10000
					if( !params.height || params.height < 1 || params.height > MAX_FRAME_HEIGHT ) {
						console.warn( 'Invalid height value received, aborting' );
						return;
					}
					var el = document.getElementsByTagName( 'iframe' );
					for ( var i=0; i < el.length; i++ ) {
						if( el[i].contentWindow === event.source ) {
							el[i].style.height = params.height + 'px';
							el[i].style.width = '100%';
							console.info( 'Setting iFrame height to ' + params.height );
							console.info( 'Setting iFrame width to 100%' );
						}
					}
				});</script></head><body class="cloudFirst" data-new-gr-c-s-check-loaded="8.912.0" data-gr-ext-installed=""><h1>Evaluation of Clustering</h1>
<div>
<p>Now that we’ve learned about clustering methods and Kmeans, it is time to work on evaluation methods of clustering results. Is this a good way of clustering this data?</p>
<p>In this video we review how the clustering evaluation method&nbsp;<em>purity</em>&nbsp;can be used to evaluate the outcomes of a clustering experiment that has produced 3 centroids. It is a very simple example which highlights the fundamental principles of how the algorithm works.</p>
<p>As has been mentioned before, all machine learning algorithms are required to be&nbsp;<em>evaluated</em>. Are the clusters useful? Evaluation of clustering methods is not easy. But, generally there are two main categories of evaluation methods for clustering:</p>
<ul>
<li><strong>External assessment:</strong><br>compare clustering performance against a known clustering (often called&nbsp;<em>Ground truth</em>&nbsp;or Gold standard).</li>
<li><strong>Internal assessment:</strong><br>determine if clustering follows certain intrinsic assumptions (e.g. cluster-to-cluster distance or cluster size etc.).
<ul class="subList">
<li>Examples:<br>Silhouette coefficient, Dunn index etc.</li>
</ul>
</li>
</ul>
<p>The following figure illustrates a sample of ground truth (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>)&nbsp;and the clustering partition found by a clustering algorithm&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>.</p>
<p class="centerImage"><img src="../images/Evaluation%20of%20Clustering%20image%201.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5 id="figure-ground-truth-vs-clustering-partition-found-by-algorithms">Figure. Ground truth VS Clustering partition found by algorithms</h5>
<h3 id="rand-index">Rand Index</h3>
<p>The Rand index, is a measure of the similarity between two data clusters. We have the assignments of data instances to different clusters suggested by a clustering algorithm (say&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>).&nbsp;In external assessment, we have knowledge of the&nbsp;<em>ground truth</em>&nbsp;cluster assignments. (say&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>)&nbsp;The Rand index is a function that measures the similarity of the two assignments&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>,&nbsp;<em>ignoring their permutations</em>.</p>
<p>The Rand index is computed as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>R</mi><mo>=</mo><mfrac><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mo>+</mo><mi>c</mi><mo>+</mo><mi>d</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow><mrow><mo>(</mo><mtable rowspacing="4pt" columnspacing="1em"><mtr><mtd><mi>n</mi></mtd></mtr><mtr><mtd><mn>2</mn></mtd></mtr></mtable><mo>)</mo></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(R = \frac{a+b}{a+b+c+d} = \frac{a+b}{\left(\begin{array}{c} n \\ 2 \end{array}\right)}\)"}</annotation></semantics></math>,&nbsp;where</p>
<ul>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>a</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(a\)"}</annotation></semantics></math>=&nbsp;the number of pairs of data instances that are in the same cluster in both&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>,&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>.</li>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>b</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(b\)"}</annotation></semantics></math>&nbsp;= the number of pairs of data instances that are in the different clusters in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;and in different clusters in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>.</li>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>c</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(c\)"}</annotation></semantics></math>&nbsp;= the number of pairs of data instances that are in the same cluster in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;but in different clusters in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>.</li>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\)"}</annotation></semantics></math>&nbsp;= the number of pairs of data instances that are in the different clusters in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;but in the same clusters in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>.</li>
</ul>
<p>The&nbsp;<em>adjusted rand index</em>&nbsp;is the corrected-for-chance version of the Rand index. In other words the index takes chance into account and corrects any bias introduced by chance.</p>
<h3 id="purity">Purity</h3>
<p>In evaluation methods of clustering, it is common practice to use more than one approach for evaluation because neither of the evaluation methods are comprehensive enough.&nbsp;<em>Purity</em>&nbsp;is a way of quality measurement in clustering methods. As the name suggests, we would like to measure the purity for all clusters in terms of class labels of the data in each cluster. Consider the following figure as an example.</p>
<p class="centerImage"><img src="../images/Evaluation%20of%20Clustering%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5 id="figure-three-obtained-clusters-for-three-types-of-data-cross-circle-and-plus">Figure. Three obtained clusters for three types of data: Cross, Circle, and Plus.</h5>
<p>Each cluster is assigned to the class label which has the majority in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned instances and dividing by the number of total instances.</p>
<p>Based on the figure, the first cluster has 5 crosses and 1 circle, so the majority of the labels are cross. For the next cluster, we have 4 circles and 1 cross and 1 plus, so circle has the majority. And as for the last one we can see 3 pluses and 2 crosses which result in majority of pluses. Now we can calculate the Purity measurement of these obtained clusters as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>P</mi><mi>u</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>=</mo><mfrac><mn>1</mn><mn>17</mn></mfrac><mo>×<!-- × --></mo><mo stretchy="false">(</mo><mn>5</mn><mo>+</mo><mn>4</mn><mo>+</mo><mn>3</mn><mo stretchy="false">)</mo><mo>≈<!-- ≈ --></mo><mn>0.71</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"Purity = \frac{1}{17} \times (5+4+3) \approx 0.71"}</annotation></semantics></math></p>
<p>Which results in approximately&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>71</mn><mi mathvariant="normal">%<!-- % --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(71 \%\)"}</annotation></semantics></math>&nbsp;purity.</p>
<p>Now let us explain one of the disadvantages of this evaluation method. Consider the example we just solved, what if a particular outcome groups the points into&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>17</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(17\)"}</annotation></semantics></math>&nbsp;clusters? One cluster for each point. It may not sound like a clustering approach but in this case the purity measurement would result in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>100</mn><mi mathvariant="normal">%<!-- % --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(100 \%\)"}</annotation></semantics></math>&nbsp;</p>
<p>purity since there is only one single point in each cluster. But it does not make sense. Why?</p>
<blockquote>
<p>This is why we have to make sure that we are selecting a&nbsp;<em>fair number of clusters</em>&nbsp;when we perform clustering on a set of data points.</p>
</blockquote>
<h3 id="mutual-information">Mutual Information</h3>
<p>Mutual information is one of the most popular approaches in analysis of clustering. It measure the agreement between two clustering assignments such as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>,&nbsp;so the aim is almost same as the Rand Index. In mutual information the main question is how informative is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;about&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>&nbsp;or&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>&nbsp;about&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>.&nbsp;How similar are they and are they similar in a useful way? What can you tall about&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;if you look at&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>?</p>
<p>Mutual information is a function that measures the agreement of the two clustering assignments&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>&nbsp;in terms of how informative one is about the other, ignoring permutations. To put it simple, how informative is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;about&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>.&nbsp;</p>
<p>Let’s assume that clustering partition&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;has&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>K</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(K\)"}</annotation></semantics></math>&nbsp;clusters and the partition&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>&nbsp;has&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>K</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(K^\prime\)"}</annotation></semantics></math>&nbsp;clusters, in this case the Mutual Information of these two clustering assignments are computed as:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>M</mi><mi>I</mi><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>K</mi></mrow></munderover><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><msup><mi>K</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mrow></munderover><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><msup><mi>P</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"MI(C,C^\prime) = \sum_{i=1}^{K}\sum_{j=1}^{K^\prime} P(i,j) log \frac{P(i,j)}{P(i)P^\prime(j)}"}</annotation></semantics></math></p>
<p>Where&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(i)\)"}</annotation></semantics></math>&nbsp;denoted the probability of randomly selected instance to belong to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>i</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(i-\)"}</annotation></semantics></math>th cluster of the partition&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>.&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>P</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P^\prime(j)\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(i,j)\)"}</annotation></semantics></math>&nbsp;are also similarly defined. As you can see another advantage of mutual information is the number of clusters which is found by&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>&nbsp;are not required to be exactly the same.</p>
<p>Assume that we have the ground truth clusters and the corresponding assignments (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>) with&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>K</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(K\)"}</annotation></semantics></math>&nbsp;clusters. In this case, we also performed a clustering method such as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>&nbsp;and we obtained<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>K</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(K^\prime\)"}</annotation></semantics></math>&nbsp;clusters. Then we can calculate the mutual information of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>M</mi><mi>I</mi><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(MI(C,C^\prime)\)"}</annotation></semantics></math>.&nbsp;So if our&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>C</mi><mi class="MJX-variant" mathvariant="normal">′<!-- ′ --></mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C^\prime\)"}</annotation></semantics></math>&nbsp;clustering is highly informative based on&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;we can conclude that the&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;clustering method is doing well.</p>
<h3 id="silhouette-coefficient">Silhouette Coefficient</h3>
<p>The silhouette value is a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference). This method has the advantage that it does not require the ground truth cluster assignments. The silhouette coefficient contrasts the average distance between the instances of the same cluster with the average distance between the instances of different clusters:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>s</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>b</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>−<!-- − --></mo><mi>a</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo fence="false" stretchy="false">{</mo><mi>a</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>,</mo><mi>b</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo fence="false" stretchy="false">}</mo></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"s(i) = \frac{b(i)-a(i)}{max\{a(i),b(i)\}}"}</annotation></semantics></math></p>
<p>In the above formula,&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>a</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(a(i)\)"}</annotation></semantics></math>&nbsp;is the average distance of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>i</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(i-\)"}</annotation></semantics></math>th instance with all other instances of the same cluster, and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>b</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(b(i)\)"}</annotation></semantics></math>&nbsp;is the lowest average dissimilarity of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>i</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(i-\)"}</annotation></semantics></math>th instance with all other clusters. The final value of Silhouette calculation ranges from&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>−<!-- − --></mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(-1\)"}</annotation></semantics></math>&nbsp;to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>+</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(+1\)"}</annotation></semantics></math>.</p>
<p>A high value of Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters. If most objects have a high value, then the clustering configuration is appropriate. On the other hand, if many points have a low or negative value, then the clustering configuration may have too many or too few clusters.</p>
<p align="center"><iframe src="https://deakin.au.panopto.com/Panopto/Pages/Embed.aspx?id=6965cc17-c175-487a-99a7-afe70122a0b9&amp;autoplay=false&amp;offerviewer=true&amp;showtitle=true&amp;showbrand=true&amp;captions=true&amp;interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen="" allow="autoplay" aria-label="Panopto Embedded Video Player"></iframe></p>
<table class="tableClear" style="width: 880px;">
<tbody>
<tr>
<td style="width: 59px;"></td>
<td style="width: 466.433px;"><a id="viewTranscript">View transcript</a></td>
<td style="width: 271.567px;"></td>
</tr>
</tbody>
</table>
<article class="js-transcript transcript" id="transcript-en">SPEAKER 1: In this tutorial, you're going to see an example of evaluating a clustering algorithm on a simple problem. As we have said before, a good evaluation of your trained method is obligatory in machine learning. Let's say you ran a clustering algorithm on some data, and this is your output. So you clustered that data into three clusters, which the first one is this one, the second one is this, and this is the last one. Now how can we evaluate the performance of your clustering method? In this example, we're going to talk about purity or homogeneity, which is one of the evaluation methods of clustering. Purity says each cluster is assigned to the class which has the majority in the cluster.
<p class="transcript__para">So in this cluster, the majority is in blue colours. In this cluster, the majority is in red colours. And finally, in the last class, the majority is in the yellow colours. Then the accuracy of this assignment is measured by counting the number of correctly assigned instances and dividing by the number of total instances. As you can see in here, the majority of this cluster is blue circles. So we are saying five out of 17 data samples we have. As for the next cluster, we say the majority is the red one. So we're going to say four, again, out of the 17 data points.</p>
<p class="transcript__para">And as for the last one, we say three, which is the number of the majority of data points in here which are yellow. So the final value for purity is 0.71. So this is one of the examples of how can we evaluate our clustering algorithm. There are certain other methods you can use to evaluate your clustering algorithms. Of course, you're going to learn about them in the rest of your course.</p>
</article>
<p></p>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>