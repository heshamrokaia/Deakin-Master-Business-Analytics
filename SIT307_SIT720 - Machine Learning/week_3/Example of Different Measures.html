<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<script>function lti_launch( vars, target ) {
						var query = '';
						var new_tab = false;

						for(var key in vars) {
							if(query.length == 0) {
								query += '?' + key + '=' + encodeURIComponent(vars[key]);
							}
							else {
								query += '&' + key + '=' + encodeURIComponent(vars[key]);
							}
						}

						var url = '/d2l/customization/pearsonlti/6605/Launch' + query;(target == '_blank') ? window.open( url, '_blank' ) : location.replace( url );}</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/embeds.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					window.D2L.EmbedRenderer.renderEmbeds(document.body);
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/mathjax.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					if (document.querySelector('math') || /\$\$|\\\(|\\\[|\\begin{|\\ref{|\\eqref{/.test(document.body.innerHTML)) {
						document.querySelectorAll('mspace[linebreak="newline"]').forEach(elm => {
							elm.setAttribute('style', 'display: block; height: 0.5rem;');
						});

						window.D2L.MathJax.loadMathJax({
							outputScale: 1.5,
							renderLatex: true,
							enableMML3Support: false
						});
					}
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/prism.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					document.querySelectorAll('.d2l-code').forEach(code => {
						window.D2L.Prism.formatCodeElement(code);
					});
				});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.documentElement.hasAttribute('lang')) return;
						document.documentElement.setAttribute('lang', 'en-GB'); 						
					});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.head.querySelector('title')) return;
						var handleAppendTitle = function(evt) {
							if (!evt || !evt.data) return;

							try {
								var data = JSON.parse(evt.data);
								if (data.handler !== 'd2l.iframe.requestPageTitle' || !data.pageTitle) return;

								window.removeEventListener('message', handleAppendTitle, false);

								var titleElm = document.createElement('title');
								titleElm.textContent = data.pageTitle;
								document.head.appendChild(titleElm);
							} catch (e) {}	
						};

						window.addEventListener('message', handleAppendTitle, false);
						window.parent.postMessage(JSON.stringify({ handler: 'd2l.iframe.requestPageTitle' }), '*');
					});</script><script>window.addEventListener('message', function(event) { 
					if( !event.data ) {
						return;
					}

					var params;
					try {
						params = JSON.parse( event.data );
					}
					catch {
						return;
					}
					if( !params.subject || params.subject !== 'lti.frameResize' ) {
						return;
					}

					const MAX_FRAME_HEIGHT = 10000
					if( !params.height || params.height < 1 || params.height > MAX_FRAME_HEIGHT ) {
						console.warn( 'Invalid height value received, aborting' );
						return;
					}
					var el = document.getElementsByTagName( 'iframe' );
					for ( var i=0; i < el.length; i++ ) {
						if( el[i].contentWindow === event.source ) {
							el[i].style.height = params.height + 'px';
							el[i].style.width = '100%';
							console.info( 'Setting iFrame height to ' + params.height );
							console.info( 'Setting iFrame width to 100%' );
						}
					}
				});</script></head><body class="cloudFirst" data-new-gr-c-s-check-loaded="8.912.0" data-gr-ext-installed=""><h1>Different distance metrics</h1>
<div>
<p>In this video we practise how to calculate some of the&nbsp;<em>distance measurements</em>&nbsp;that were introduced previously.</p>
<p>This is a great way to revise and consolidate your learning of these concepts. It explains how the calculations is implemented as well as&nbsp;<em>where</em>&nbsp;each measurement potentially can be used.</p>
<p>You will see the following distance measures being demonstrated:</p>
<ul>
<li>Euclidean distance</li>
<li>Cosine distance</li>
<li>Mahalanobis distance</li>
<li>Cityblock/Manhattan distance</li>
<li>Minkowski distance</li>
<li>Jaccard distance</li>
</ul>
<p align="center"><iframe src="https://deakin.au.panopto.com/Panopto/Pages/Embed.aspx?id=1dbdbd3a-a827-4de0-a0b4-afe70122a0bf&amp;autoplay=false&amp;offerviewer=true&amp;showtitle=true&amp;showbrand=true&amp;captions=true&amp;interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen="" allow="autoplay" aria-label="Panopto Embedded Video Player"></iframe></p>
<table class="tableClear" style="width: 880px;">
<tbody>
<tr>
<td style="width: 59px;"></td>
<td style="width: 466.433px;"><a id="viewTranscript">View transcript</a></td>
<td style="width: 271.567px;"></td>
</tr>
</tbody>
</table>
<article class="transcript transcript" id="transcript-en">SPEAKER 1: In this tutorial, we're going to show you the different measurements of distance. The Euclidean distance is the first one. For any 2 data instances represented by d-dimensional feature vectors xi and xj, the Euclidean distance is computed as this formula. So you just need to subtract the values in each dimension and take them to power 2. Consider this example. x1 is 1, 1, 2, 1, 0, and x2 is 0, 2, 2, 0, 2. So first, we subtract 1, 0, power 2. Again, 1 to power 2. 2 to the power of 2, 1, 0 power 2, and 0 minus 2, power of 2. So the root square of this value is the Euclidean distance.
<p class="transcript__para">The Euclidean distance is appropriate when we have continuous numerical variables and we want to reflect absolute distances. The next distance is cosine distance. For any 2 data instances represented by d-dimensional feature vectors xi and xj, the cosine distance is computed as xi transposed, xj, and the norm 2 of xi and the norm 2 of xj. Consider this example. As you can see in here, the xi transpose xj would be the multiplication of these values. 1 times 0, 1 plus 1 times 2, 2 times 2, and so on. Also the norm 2 of this vector is just 1 times 1, plus 1 times 1, plus 2 times 2, 1 times 1, and 0 times 0.</p>
<p class="transcript__para">And also, the square root of this 1 multiplied by the norm 2 of these vector. So then as you can see, the final value of cosine distance for these vectors is 0.3453. The cosine similarity is generally used as a metric for measuring distances when the magnitude of the vector does not matter. It's basically in here we are seeking for the angle between 2 vectors. The next distance we are going to see is Mahalanobis distance. Consider any 2 data points represented by d-dimensional feature vectors xi and xj. The Mahalanobis distance is computed as this, which is a pretty simple formula, xi minus xj, the inverse of the covariance matrix, and xi minus xj transposed. Consider this example.</p>
<p class="transcript__para">First, look at these data points. This is the independent variable 1, and this is the independent variable 2. The mean of this data is 500, 500, and it should be a point around here. So we would like to find the distance of x, which is 410 and 400 in 2d, from the mean of this data. So first we need to construct x minus m, which is 410 minus 500, 400 minus 500, which is -90 and -100. Also we find the covariance matrix between these 2 vectors, which is something like this.</p>
<p class="transcript__para">So we can find the Mahalanobis distance by multiplying x minus m, which is -90, -100, to covariance matrix, and then again multiplied by the transpose of this vector, which as you can see, finally, the value is 1.825. The Mahalanobis distance is appropriate when we have continuous numerical variables, and we want to reflect absolute distance, but we want to remove redundancies. If we have repeated variables, there repetitious effect will disappear.</p>
<p class="transcript__para">The next distance is Cityblock, or Manhattan distance. For any 2 data instances represented by d-dimensional feature, vector xi and vector xj, the their Cityblock distance is computed as this, which is pretty simple and is the absolute difference of xi in dimension 1 minus xj in dimension 1, and so on until dimension d. Consider this simple example. So as you can see, the Manhattan distance is just 1 minus 0, absolute value, plus 1 minus 2, and so on. So as you can see, the final value of Manhattan distance is 5.</p>
<p class="transcript__para">The Minkowski Distance is a generalisation of these 2 distances we have just seen, the Euclidean distance and Cityblock distance. So as you remember, the Euclidean distance was in norm 2, and the Cityblock distances is in norm 1. So the Minkowski distance is a generalisation of these distances defined for norm p. So it's basically the same concept. The last distance is Jaccard distance. The Jaccard distance is a distance used to measure diversity of any 2 sets. So it's very important that Jaccard distance is used to measure the diversity of sets. So now we are talking about sets. Consider any 2 instances, such as xi and xj, as the binary vectors.</p>
<p class="transcript__para">Now the Jaccard distance between xi and xj is defined as this. The logical AND of xi and xj, and the logical OR xi and xj, but let's see an example. Consider xi as 1, 0, 1, and the xj as 1, 1, 0. If we want to find the Jaccard distance between these vectors, first we need to have a logical AND between these 2 vectors. So as we can see, 1 and 1 for the first value would result in 1. 0 and 1 for the second one would result in 0. 1 and 0 for the third one result in 0. So if we sum it up, we have only 1, which comes here.</p>
<p class="transcript__para">But as for the next part, is xi logical, OR xj? So for the first value of 1 or 1 is 1, 0 or 1 is again 1, and 1 or 0 is again 1. So if we sum these values, we have three. So the Jaccard distance for these 2 sets is 1 divided by 3. Jaccard similarity, also known as Jaccard index, is a measure to find similarities between sets. As you may have noticed, we have lots of different measurements for distances, so it's upon us to find the best measurement based on our own problem.</p>
</article>
<p></p>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>