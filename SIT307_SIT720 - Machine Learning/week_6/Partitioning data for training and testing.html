<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<link rel="stylesheet" href="https://s.brightspace.com/lib/fonts/0.5.0/fonts.css"></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Logistic%20regression%20formulation%20image%201.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Logistic regression formulation</h1>
</div>
<div>
<p>Logistic regression is named after the function used at the core of the method, the&nbsp;<em>logistic function</em>.</p>
<p>The logistic function is also called the <a href="http://mathworld.wolfram.com/SigmoidFunction.html" target="_blank" rel="noopener noreferrer">sigmoid function</a>. It’s an S-shaped curve (see the above figure) and it can take any real-valued number and map it into a value between&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>&nbsp;but never exactly at those limits. The value approaches but never reaches&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;or&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>.</p>
<p>Let&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}\)"}</annotation></semantics></math>&nbsp;be a data instance, and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y\)"}</annotation></semantics></math>&nbsp;be its class label in <math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo fence="false" stretchy="false">{</mo><mo>−<!-- − --></mo><mn>1</mn><mo>,</mo><mn>1</mn><mo fence="false" stretchy="false">}</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\{-1,1\}\)"}</annotation></semantics></math>.&nbsp;&nbsp;Logistic regression does not directly model&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y\) "}</annotation></semantics></math>&nbsp;in terms&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}\)"}</annotation></semantics></math>.&nbsp;Instead, it models something called <a href="https://www.theanalysisfactor.com/what-is-logit-function/" target="_blank" rel="noopener noreferrer">logit value</a>&nbsp;or&nbsp;<em>log of odds</em>&nbsp;against&nbsp;&nbsp;via linear regression. So generally we are modelling log of odds based on&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}\)"}</annotation></semantics></math>.</p>
<h3 id="but-what-are-the-odds">But what are the&nbsp;<em>odds</em>?</h3>
<p>The odds of class&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>−<!-- − --></mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(-1\)"}</annotation></semantics></math>&nbsp;is defined as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>O</mi><mi>d</mi><mi>d</mi><mi>s</mi><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>−<!-- − --></mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ Odds = \frac{P(y=1\vert\textbf{x})}{1-P(y=1\vert\textbf{x})} \\"}</annotation></semantics></math></p>
<h4 id="example">Example:</h4>
<p>The odds that a randomly chosen day of the week is a weekend are:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mspace linebreak="newline"></mspace><mfrac><mfrac><mn>2</mn><mn>7</mn></mfrac><mrow><mn>1</mn><mo>−<!-- − --></mo><mo stretchy="false">(</mo><mfrac><mn>2</mn><mn>7</mn></mfrac><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mn>2</mn><mn>5</mn></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\\ \frac{\frac{2}{7}}{1-(\frac{2}{7})} = \frac{2}{5} \\\)"}</annotation></semantics></math>&nbsp;</p>
<h3 id="logit">Logit</h3>
<p>This Log of odds is called&nbsp;<em>logit</em>. Logistic regression uses the following linear model:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>−<!-- − --></mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mn>0</mn></msub><mo>+</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mn>1</mn></msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\\ log\frac{P(y=1\vert\textbf{x})}{1-P(y=1\vert\textbf{x})} = \textbf{w}_0 + \textbf{w}_1 \textbf{x} \\\)"}</annotation></semantics></math></p>
<p>So you can easily see the similarity to linear regression! Here we are faced with&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>−<!-- − --></mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(log\frac{P(y=1\vert\textbf{x})}{1-P(y=1\vert\textbf{x})}\)"}</annotation></semantics></math>&nbsp;in linear regression. So we are modelling the&nbsp;<em>logit (log of odds)</em>&nbsp;of a data point such as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y\) "}</annotation></semantics></math>&nbsp;rather than&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}\)"}</annotation></semantics></math>&nbsp;to be labelled as class&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>.</p>
<p>The above expression means that as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}\)"}</annotation></semantics></math>&nbsp;increases by&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\) "}</annotation></semantics></math>&nbsp;the log of the odds increases by&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}_1\)"}</annotation></semantics></math>. in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d-\)"}</annotation></semantics></math>dim case, we also have:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>−<!-- − --></mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>w</mi><mi>d</mi></msub><msub><mi>x</mi><mi>d</mi></msub><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\\ log\frac{P(y=1\vert\textbf{x})}{1-P(y=1\vert\textbf{x})} = w_0 + w_1x_1+...+w_dx_d \\\)"}</annotation></semantics></math></p>
<p>Where&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo>=</mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>d</mi></msub><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x} = [1,x_1,x_2,...,x_d]\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo>=</mo><mo stretchy="false">[</mo><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>w</mi><mi>d</mi></msub><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w} = [w_0,...,w_d]\)"}</annotation></semantics></math>.</p>
<p>We now we have all the requirements for the logistic regression except we need to know how to calculate&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(y=1\vert\textbf{x})\)"}</annotation></semantics></math>?</p>
<p>As you know in linear regression we knew the value of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y\)"}</annotation></semantics></math>,&nbsp;but here we do not know the probability of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(y=1\vert\textbf{x})\)"}</annotation></semantics></math>.&nbsp;We can estimate this probability using the following equation:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ P(y=1\vert\textbf{x}) = \frac{1}{1+exp(-\textbf{x}^T\textbf{w})} \\"}</annotation></semantics></math></p>
<p>The model leads to the following classification rules:</p>
<ul>
<li>when&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo>&gt;</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} &gt; 0\)"}</annotation></semantics></math>,&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>&gt;</mo><mn>0.5</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) &gt; 0.5\)"}</annotation></semantics></math>,&nbsp;we decide in favour of class&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>.</li>
<li>when&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo>&lt;</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} &lt; 0 %\)"}</annotation></semantics></math>,&nbsp;&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>&lt;</mo><mn>0.5</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) &lt; 0.5 %\)"}</annotation></semantics></math>,&nbsp;we decide in favour of class&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>−<!-- − --></mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(-1\)"}</annotation></semantics></math>.</li>
<li><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">when&nbsp;</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo>=</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} = 0\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">,&nbsp;&nbsp;</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mn>0.5</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) = 0.5\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">,&nbsp; both classes are equally possible.</span></li>
</ul>
<h3 id="testing-the-model">Testing the model</h3>
<p>So assume you have trained a logistic regression model and you have come up with proper values of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>.&nbsp;Now by having a test point such as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}\)"}</annotation></semantics></math>&nbsp;you calculate the value of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}^T\textbf{w}\)"}</annotation></semantics></math>.&nbsp;</p>
<ul>
<li>If this value is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo>&gt;</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} &gt; 0\)"}</annotation></semantics></math>&nbsp;then it means&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>&gt;</mo><mn>0.5</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) &gt; 0.5\)"}</annotation></semantics></math>,&nbsp;the point is allocated to class&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>.</li>
<li>On the other hand, if the value of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo>&lt;</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} &lt; 0 %\)"}</annotation></semantics></math>&nbsp;then it means&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>&lt;</mo><mn>0.5</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) &lt; 0.5 %\)"}</annotation></semantics></math>,&nbsp;the point is allocated to class&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>.</li>
<li><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">In case of&nbsp;</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo>=</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} = 0\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">, your model is confused and it returns the same value for both of them.</span></li>
</ul>
<h3 id="summary">Summary</h3>
<p>To conclude, we saw that logistic regression is like a regression problem and the only difference is in modelling the output. In linear regression we are modelling the&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y\)"}</annotation></semantics></math>&nbsp;directly but in here we are modelling the&nbsp;<em>logit(log of odds)</em>. In the next lesson, we are going to see how to train a logistic regression problem.</p>
<p></p>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>