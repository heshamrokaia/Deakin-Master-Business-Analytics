<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

</head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Generalisation%20and%20complexity%20image%201.jpg" alt="Close-up of addition button on calculator" title="Close-up of addition button on calculator" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Generalisation and complexity</h1>
</div>
<div>
<p>Linear regression has a closed form solution. Python implementation uses&nbsp;<em>Singular Value Decomposition</em>&nbsp;(SVD) to compute the Moore-Penrose inverse of matrix&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">X</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{X}\)"}</annotation></semantics></math>. If matrix&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">X</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{X}\)"}</annotation></semantics></math>&nbsp;is of size&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi><mo>×<!-- × --></mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\times d\)"}</annotation></semantics></math>,&nbsp;we incur the computations of the order&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(O(nd^2)\)"}</annotation></semantics></math>&nbsp;assuming&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi><mo>≥<!-- ≥ --></mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n \geq d\)"}</annotation></semantics></math>.</p>
<p>As you can see here&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi><mo>≥<!-- ≥ --></mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n \geq d\)"}</annotation></semantics></math>,&nbsp;the dimension that is larger will be in a linear form in the final complexity (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\)"}</annotation></semantics></math>)&nbsp;and the smaller value will result in a squared form in complexity&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>d</mi><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d^2\)"}</annotation></semantics></math>.&nbsp;So all in all,&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(O(nd^2)\)"}</annotation></semantics></math>.&nbsp;</p>
<p>If&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>≥<!-- ≥ --></mo><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d \geq n\)"}</annotation></semantics></math>,&nbsp;the complexity would be&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>O</mi><mo stretchy="false">(</mo><mi>d</mi><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(O(dn^2)\)"}</annotation></semantics></math>.</p>
<p>We considered the linear regression problem as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">y</mtext></mrow><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">X</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{y} = \textbf{X}\textbf{w}\)"}</annotation></semantics></math>&nbsp;where we used the hypothesis function&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h(\textbf{x}_i) = \textbf{x}_i^T \textbf{w}\)"}</annotation></semantics></math>.&nbsp;&nbsp;We can use any derived features of the original features such as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>ϕ<!-- ϕ --></mi><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\phi(\textbf{x}_i)\)"}</annotation></semantics></math>&nbsp;instead of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}_i\)"}</annotation></semantics></math>.&nbsp;&nbsp;</p>
<p>In other words, we can create our own features. They could be even non-linear!</p>
<p>For example, we can have&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>ϕ<!-- ϕ --></mi><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">[</mo><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><msubsup><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\phi(\textbf{x}_{ij}) = [x_{ij},x_{ij}^2]\)"}</annotation></semantics></math>.&nbsp;As you can see we have added&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msubsup><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_{ij}^2\)"}</annotation></semantics></math>&nbsp;to the feature vector as a new feature. As long as we are using a linear formulation, the problem&nbsp;<em>remains a linear regression</em>.</p>
<p>For example, we can have&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msup><mi>ϕ<!-- ϕ --></mi><mi>T</mi></msup><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h(\textbf{x}_i) = \phi^T(\textbf{x}_i)\textbf{w}\)"}</annotation></semantics></math>&nbsp;and solve it using linear regression. This is called the generalised form of linear regression in which we are adopting our own list of features and try to fit a line based on the new features. Given expressive features, linear regression can be&nbsp;<em>powerful</em>!</p>
<h4 id="generalisation-prediction-on-unseen-data">Generalisation (Prediction on unseen data)</h4>
<p>After training a linear regression model, we can start to predict the output&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y}\)"}</annotation></semantics></math>&nbsp;for a new instance&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}\)"}</annotation></semantics></math>.&nbsp;The predicted output is computed as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mo>=</mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y} = \textbf{x}^T\textbf{w}\)"}</annotation></semantics></math>.&nbsp;Given an unseen set of instances as a test set, we can measure the error in prediction as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mspace linebreak="newline"></mspace><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><msub><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub></mfrac><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub></mrow></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−<!-- − --></mo><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\\ E_{test} = \frac{1}{n_{test}} \sum_{i=1}^{n_{test}} (y_i - \textbf{x}_i^T \textbf{w})^2 \\\)"}</annotation></semantics></math></p>
<p>The above error is called the mean square error (MSE). This is an example of a performance measure. We can also compute many other measures such as&nbsp;<em>Mean Absolute Error</em>&nbsp;(MAE) or explained variance&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo stretchy="false">(</mo><msup><mi>R</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\((R^2)\)"}</annotation></semantics></math>.&nbsp;</p>
<h4 id="model-complexity-of-linear-regression">Model complexity of Linear Regression</h4>
<p>Model complexity of linear models&nbsp;<em>increases</em>&nbsp;with the&nbsp;<em>number of features</em>. We should be aware of model complexity especially if we have a&nbsp;<em>limited set of training data</em>. The reason is the risk of&nbsp;<em>over-fitting</em>&nbsp;on this limited set of training data. Using a limited number of features may also be problematic as it could cause&nbsp;<em>under-fitting</em>.</p>
<h2 id="your-task">Activity</h2>
<p>The following video explores linear regression of non-linear features.</p>
<p></p>
<div>
<p class="centerVideo"><iframe width="648" height="365" src="https://www.youtube.com/embed/py8QrZPT48s?wmode=opaque" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></p>
<h5>This is an additional video, hosted on YouTube.</h5>
</div>
<p></p>
<p></p>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>