<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<link rel="stylesheet" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/open-source-css-and-js/prism/prism.overrides.css" type="text/css"></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Regularised%20linear%20models%20image%201.jpg" alt="Pop Art Halftone Pattern of Wavy Lines Line art illustration of a Pop Art Halftone pattern of concave and convex sine waves. Isolated on white." title="Pop Art Halftone Pattern of Wavy Lines Line art illustration of a Pop Art Halftone pattern of concave and convex sine waves. Isolated on white." style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Regularised linear models</h1>
</div>
<div>
<p>You have learned that even in linear models, using all data dimensions as features may fit the model to true patterns (signal) but also to background noise.</p>
<p>A&nbsp;<em>regulariser</em>&nbsp;is an additional term in the loss function to avoid&nbsp;<em>overfitting</em>. It is called a regulariser since it tries to keep the parameters more normal or regular. In other words, it does not allow regression coefficients (or&nbsp;<em>weights</em>) to take excessively large values. What will happen if one or more weights are excessively large? It implies your model is&nbsp;<em>highly dependent&nbsp;</em>on that one feature.</p>
<p>What if this feature is noise or highly affected by noisy observations? We do not want to rely too much on any one thing when we are designing a model in machine learning! So, this procedure is a way to guide the training process to prefer certain types of weights over others.</p>
<p>Thus our loss function now has another term which is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>λ<!-- λ --></mi><mtext>&nbsp;</mtext><mi>R</mi><mi>e</mi><mi>g</mi><mi>u</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>s</mi><mi>e</mi><mi>r</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda \ Regulariser(\textbf{w})\)"}</annotation></semantics></math>.</p>
<p>You can consider this term as complexity of the model.</p>
<p class="centerImage"><img src="../images/Regularised%20linear%20models%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;Loss function including a regulariser.</h5>
<h3 id="how-do-regularisers-works-in-linear-models">How do regularisers works in linear models?</h3>
<p>Remember the linear model is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi><mo>=</mo><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi></mrow></munderover><mtext>&nbsp;</mtext><msub><mi>w</mi><mi>j</mi></msub><msub><mi>x</mi><mi>j</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y = w_0 + \sum_{j=1}^{d} \ w_jx_j\)"}</annotation></semantics></math>&nbsp;</p>
<p>Should we allow all possible weights? Or impose preferences? What makes a simpler linear model?</p>
<h4 id="our-preference">Our preference</h4>
<p>We do not want huge weights (i.e. do not want to over-rely on any one feature). If weights are huge,&nbsp;<em>a small change</em>&nbsp;in a feature would result in a&nbsp;<em>large change</em>&nbsp;in the prediction! In fact, since we may even have&nbsp;<em>irrelevant features</em>, we want some of the weights to be zero so we can&nbsp;<em>discard some features</em>.</p>
<p>In the following formulation there is a regulariser:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mspace linebreak="newline"></mspace><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow></munder><mtext>&nbsp;</mtext><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"></mrow></munderover><mtext>&nbsp;</mtext><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>+</mo><mi>λ<!-- λ --></mi><mtext>&nbsp;</mtext><mi>R</mi><mi>e</mi><mi>g</mi><mi>u</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>s</mi><mi>e</mi><mi>r</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\\ \min_{w}\ \frac{1}{n} \sum_{i}^{} \ L(y_i,\textbf{x}_i^T\textbf{w}) + \lambda\ Regulariser (\textbf{w}) \\\)"}</annotation></semantics></math></p>
<p>How do we create a regulariser that penalises large weights or encourages small/zero weights ? What should our&nbsp;<em>regulariser function</em>&nbsp;be?</p>
<p>There are two popular regulariser functions:</p>
<h4 id="option-1">Option 1:&nbsp;</h4>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>R</mi><mi>e</mi><mi>g</mi><mi>u</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>s</mi><mi>e</mi><mi>r</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></munder><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mi>w</mi><mi>j</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>1</mn></msub><mo>,</mo><mo stretchy="false">(</mo><msub><mi>l</mi><mn>1</mn></msub><mo>−<!-- − --></mo><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"Regulariser(\textbf{w}) = \sum_{j} |w_j| = ||\textbf{w}||_1, (l_1-norm)"}</annotation></semantics></math></p>
<p>This encourages&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;weights (sparsity). This function implies the closed form function of a square.</p>
<h4 id="option-2">Option 2:</h4>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>R</mi><mi>e</mi><mi>g</mi><mi>u</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>s</mi><mi>e</mi><mi>r</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></munder><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mi>w</mi><mi>j</mi></msub><msup><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>2</mn></msup><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>2</mn></msub><mo>,</mo><mo stretchy="false">(</mo><msub><mi>l</mi><mn>2</mn></msub><mo>−<!-- − --></mo><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"Regulariser(\textbf{w}) = \sum_{j} |w_j|^2 = ||\textbf{w}||_2, (l_2-norm)"}</annotation></semantics></math>&nbsp;</p>
<p>This penalises large weights. This function implies the closed form function of a circle.</p>
<h3 id="regularisation-impact">Regularisation impact</h3>
<p>Consider the figure below. The left image is an illustration of regularisation impact.</p>
<p class="centerImage"><img src="../images/Regularised%20linear%20models%20image%203.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;Illustration of regularisation impact</h5>
<p>The&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>l</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l_1\)"}</annotation></semantics></math>-norm forms a square shape, assuming the loss function is in the form of ellipses in the plot. Since we are minimising the loss function which actually has&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>l</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l_1\)"}</annotation></semantics></math>&nbsp;</p>
<p>-norm regularisation inside it, we need to find a sweet spot which is the intersection of these two regions. If you keep drawing the ellipses, you can find the intersection.</p>
<p>The image on the right indicates the same concept with a different regulariser<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>l</mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l_2\)"}</annotation></semantics></math>-norm which is circular. As you can see in this figure, there is an increased chance of intersection. Also there is less chance of having&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;weights for<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>β<!-- β --></mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\beta_1\)"}</annotation></semantics></math>&nbsp;or&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_1\)"}</annotation></semantics></math>&nbsp;and<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>β<!-- β --></mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\beta_2\)"}</annotation></semantics></math>&nbsp;or<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_2\)"}</annotation></semantics></math>because it looks we can have more options for selection of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_1\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_2\)"}</annotation></semantics></math>.&nbsp;</p>
<p>Now, consider the visualisation of the&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>l</mi><mi>p</mi></msub><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l_p-\)"}</annotation></semantics></math>&nbsp;norm regulariser below.</p>
<p>As you can see, by using a&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>l</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l_1\)"}</annotation></semantics></math>&nbsp;norm regulariser you are using a square shaped regulariser (the pink square) and by using the<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>l</mi><mn>2</mn></msub><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l_2-\)"}</annotation></semantics></math>norm you using a circle shaped regulariser (light blue). The&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi mathvariant="normal">∞<!-- ∞ --></mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\infty-\)"}</annotation></semantics></math>norm regulariser is a square too (in red). Remember that all of these&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>l</mi><mi>p</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l_p\)"}</annotation></semantics></math>&nbsp;norm regularisers penalise larger weights.</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>p</mi><mo>≤<!-- ≤ --></mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(p \leq 1\)"}</annotation></semantics></math>&nbsp;tends to create&nbsp;<em>sparse weights</em>&nbsp;(i.e. lots of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;weights). Higher values of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>p</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(p\)"}</annotation></semantics></math>&nbsp;tends to like&nbsp;<em>similar and small weights</em>. Next, we are going to review two regularisation methods, LASSO and Ridge.</p>
<p class="centerImage"><img src="../images/Regularised%20linear%20models%20image%204.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;Visualisation of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>l</mi><mi>p</mi></msub><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l_p-\)"}</annotation></semantics></math>&nbsp;norm regulariser.</h5>
<p></p>
<h3 id="l1-regularisation-lasso"><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>L</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(L_1\)"}</annotation></semantics></math>&nbsp;Regularisation (LASSO)</h3>
<p>Lasso (<em>Least Absolute Shrinkage and Selection Operator</em>) (also seen in uppercase: LASSO) is a regression analysis method that performs both variable selection and regularisation in order to enhance the prediction accuracy and interpretability of the statistical model it produces.</p>
<p>It is therefore quite common to use the following formulation:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow></munder><mtext>&nbsp;</mtext><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"></mrow></munderover><mtext>&nbsp;</mtext><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>+</mo><msub><mi>λ<!-- λ --></mi><mn>1</mn></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>1</mn></msub><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \min_{w}\ \frac{1}{n} \sum_{i}^{} \ L(y_i,\textbf{x}_i^T\textbf{w}) + \lambda_1 ||\textbf{w}||_1 \\"}</annotation></semantics></math></p>
<p>Now you can see our normal risk function&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>+</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(+\)"}</annotation></semantics></math>&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>l</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l_1\)"}</annotation></semantics></math>-norm regulariser. Another method of regularisation is called Ridge.</p>
<h3 id="l2-regularisation-ridge"><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>L</mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(L_2\)"}</annotation></semantics></math>&nbsp;Regularisation (Ridge)</h3>
<p>The common formulation of Ridge is:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow></munder><mtext>&nbsp;</mtext><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"></mrow></munderover><mtext>&nbsp;</mtext><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>+</mo><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msubsup><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \min_{w}\ \frac{1}{n} \sum_{i}^{} \ L(y_i,\textbf{x}_i^T\textbf{w}) + \lambda_2 ||\textbf{w}||^2_2 \\"}</annotation></semantics></math></p>
<p>This is known as&nbsp;<em>Elastic Net</em>. LASSO and Ridge regularisation are then&nbsp;<em>special cases</em>&nbsp;if Elastic Net for&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub><mo>=</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_2 = 0\) "}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mn>1</mn></msub><mo>=</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_1 = 0\)"}</annotation></semantics></math>.</p>
<h4 id="elastic-net-overcomes-a-limitation-of-lasso-what-is-this-limitation">Elastic Net overcomes a limitation of LASSO. What is this limitation?</h4>
<p>When presented with few samples in high dimension spaces (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>&gt;</mo><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d &gt; n\)"}</annotation></semantics></math>&nbsp;cases, such as bioinformatics datasets), LASSO&nbsp;<em>selects at most&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\)"}</annotation></semantics></math>&nbsp;</em><em>variables&nbsp;</em>before it saturates. But Elastic Net can overcome this problem since it can select a greater number of variables despite the number of data points.</p>
<p>For&nbsp;<em>Linear regression</em>&nbsp;we use&nbsp;<em>square loss</em>&nbsp;functions, i.e.:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−<!-- − --></mo><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ L(y_i,\textbf{x}_{i}^{T}\textbf{w}) = (y_i - \textbf{x}_{i}^{T}\textbf{w})^2 \\"}</annotation></semantics></math></p>
<p>For&nbsp;<em>Logistic regression</em>&nbsp;we use&nbsp;<em>logistic loss</em>&nbsp;function, i.e.:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ L(y_i,\textbf{x}_{i}^{T}\textbf{w}) = log(1+exp(-y_i\textbf{x}_{i}^{T} \textbf{w})) \\"}</annotation></semantics></math></p>
<p>So all in all, we solve the following optimisation:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow></munder><mtext>&nbsp;</mtext><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"></mrow></munderover><mtext>&nbsp;</mtext><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>+</mo><msub><mi>λ<!-- λ --></mi><mn>1</mn></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>1</mn></msub><mo>+</mo><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msubsup><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \min_{w}\ \frac{1}{n} \sum_{i}^{} \ L(y_i,\textbf{x}_i^T\textbf{w}) + \lambda_1 ||\textbf{w}||_1+\lambda_2 ||\textbf{w}||^2_2 \\"}</annotation></semantics></math></p>
<p>For Ridge regularisation (when&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mn>1</mn></msub><mo>=</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_1 = 0\)"}</annotation></semantics></math>,&nbsp;the solution is&nbsp;<em>closed form</em>&nbsp;and is given as:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo>=</mo><mo stretchy="false">(</mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">X</mtext></mrow><mi>T</mi></msup><mi>X</mi><mo>+</mo><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">I</mtext></mrow><msup><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mo>−<!-- − --></mo><mn>1</mn></mrow></msup><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">X</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">y</mtext></mrow><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \textbf{w} = (\textbf{X}^TX + \lambda_2 \textbf{I})^{-1} \textbf{X}^T \textbf{y} \\"}</annotation></semantics></math></p>
<p>For LASSO and Elastic Net, we have to perform&nbsp;<em>iterative optimisation</em>. Since&nbsp;&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>L</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(L_1\)"}</annotation></semantics></math>&nbsp;norm is non-differentiable and non-smooth, a proximal gradient should be used to optimise this loss function.</p>
<p>Also while working with logistic regression, we solve the following optimisation:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mrow></munder><munder><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></munder><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><msub><mi>λ<!-- λ --></mi><mn>1</mn></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>1</mn></msub><mo>+</mo><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msubsup><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \min_{\textbf{w}} \sum_{i} log(1+exp(-y_i\textbf{x}_{i}^{T} \textbf{w}))+ \lambda_1 ||\textbf{w}||_1+\lambda_2 ||\textbf{w}||^2_2 \\"}</annotation></semantics></math></p>
<p>As you can see the only difference is in the fist term&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>log</mi><mo>⁡<!-- ⁡ --></mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext mathcolor="red">\)</mtext></mstyle><annotation encoding="latex">{"version":"1.1","math":"\log(1+exp(-y_i\textbf{x}_{i}^{T} \textbf{w}))\)"}</annotation></semantics></math>&nbsp;which is the&nbsp;<em>log of the likelihood function</em>. For regularised logistic regression, we always have to perform&nbsp;<em>iterative optimisation</em>.</p>
<h4 id="what-are-the-effects-of-regularisation-on-bias-and-variance">What are the effects of Regularisation on bias and variance?</h4>
<p>We know that regularisation&nbsp;<em>increases bias</em>&nbsp;in our model. We are only&nbsp;<em>partially</em>&nbsp;listening to our training data!</p>
<h4 id="why-regularisation-might-make-sense">Why regularisation might make sense?</h4>
<h4 id="because-it-greatly-reduces-the-variance">Because it greatly&nbsp;<em>reduces the variance</em>.</h4>
<p>To conclude, it is useful when the net effect (i.e.&nbsp;<em>bias<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi></mi><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(^2\)"}</annotation></semantics></math>&nbsp;</em>+<em>variance</em>) reduces. The following figure illustrates the effects of bias and variance on model complexity. As you can see, there is a trade-off or optimum model complexity which we always look for.</p>
<p class="centerImage"><img src="../images/Regularised%20linear%20models%20image%205.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;Model complexity vs. bias and variance.</h5>
<p>Another important element is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>λ<!-- λ --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda\)"}</annotation></semantics></math>'s.&nbsp;&nbsp;Obviously by raising the value of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>λ<!-- λ --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda\)"}</annotation></semantics></math>, i.e.,<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_2\)"}</annotation></semantics></math>&nbsp;in Ridge regularisation,&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow></munder><mtext>&nbsp;</mtext><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"></mrow></munderover><mtext>&nbsp;</mtext><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>+</mo><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub><mo fence="false" stretchy="false">|</mo><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo fence="false" stretchy="false">|</mo><msubsup><mo fence="false" stretchy="false">|</mo><mn>2</mn><mn>2</mn></msubsup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\min_{w}\ \frac{1}{n} \sum_{i}^{} \ L(y_i,\textbf{x}_i^T\textbf{w}) + \lambda_2 \vert \vert \textbf{w}\vert \vert ^2_2\)"}</annotation></semantics></math>,&nbsp;we are forcing more regularisation on the loss function. So we should expect smaller values of weights (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>).&nbsp;As if we reduce the value of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_2\)"}</annotation></semantics></math>, we can see larger values for the obtained weights.</p>
<p>The following figure illustrates this point by showing the results of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_2\)"}</annotation></semantics></math>&nbsp;increment on the calculated weights. As you can see with larger values of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_2\)"}</annotation></semantics></math>,&nbsp;we obtain smaller values of weights. On the other hand, by enforcing less regularisation on the loss function (smaller&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_2\)"}</annotation></semantics></math>),&nbsp;</p>
<p>you will get bigger values for weights.</p>
<p class="centerImage"><img src="../images/Regularised%20linear%20models%20image%206.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;Effects of Ridge regularisation on weights.</h5>
<p>Similarly, in case of Lasso and Elastic Net (see the figure below), by imposing more regularisation, you will get smaller values for weights and vice versa. Note that the figure shows&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>−<!-- − --></mo><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&nbsp;</mtext><msub><mi>λ<!-- λ --></mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(-log \ \lambda_1\)"}</annotation></semantics></math>&nbsp;rather than&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_1\)"}</annotation></semantics></math>.</p>
<p class="centerImage"><img src="../images/Regularised%20linear%20models%20image%207.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;Effects of Lasso and Elastic-Net regularisation on weights.</h5>
<p>There is one more interesting fact about LASSO. Because&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>L</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(L_1\)"}</annotation></semantics></math>&nbsp;regularisation shrinks the weights of noisy dimensions to&nbsp;<em>zero</em>, these dimensions&nbsp;<em>do not participate</em>&nbsp;in the prediction model. Only those dimensions that have&nbsp;<em>non-zero weights participate</em>&nbsp;in the prediction. Therefore, LASSO is also used to&nbsp;<em>select predictive features</em>&nbsp;among all dimensions. This is the&nbsp;<em>feature selection</em>&nbsp;property of LASSO.</p>
<h2 id="your-task">Activity</h2>
<p>Check out the this additional video on regularisation:</p>
<p></p>
<div>
<p class="centerVideo"><iframe width="648" height="365" src="https://www.youtube.com/embed/sO4ZirJh9ds?wmode=opaque" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></p>
<h5>This is an additional video, hosted on YouTube.</h5>
</div>
<p>What additional insights did the video provide? <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=1734011&amp;type=discuss&amp;rcode=DeakinUniversity-2001546" target="_top">Share</a>&nbsp;your thoughts.</p>
<p></p>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>