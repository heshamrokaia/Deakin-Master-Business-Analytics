<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<link rel="stylesheet" href="https://s.brightspace.com/lib/fonts/0.5.0/fonts.css"><script>function lti_launch( vars, target ) {
						var query = '';
						var new_tab = false;

						for(var key in vars) {
							if(query.length == 0) {
								query += '?' + key + '=' + encodeURIComponent(vars[key]);
							}
							else {
								query += '&' + key + '=' + encodeURIComponent(vars[key]);
							}
						}

						var url = '/d2l/customization/pearsonlti/6605/Launch' + query;(target == '_blank') ? window.open( url, '_blank' ) : location.replace( url );}</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/embeds.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					window.D2L.EmbedRenderer.renderEmbeds(document.body);
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/mathjax.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					if (document.querySelector('math') || /\$\$|\\\(|\\\[|\\begin{|\\ref{|\\eqref{/.test(document.body.innerHTML)) {
						document.querySelectorAll('mspace[linebreak="newline"]').forEach(elm => {
							elm.setAttribute('style', 'display: block; height: 0.5rem;');
						});

						window.D2L.MathJax.loadMathJax({
							outputScale: 1.5,
							renderLatex: true,
							enableMML3Support: false
						});
					}
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/prism.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					document.querySelectorAll('.d2l-code').forEach(code => {
						window.D2L.Prism.formatCodeElement(code);
					});
				});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.documentElement.hasAttribute('lang')) return;
						document.documentElement.setAttribute('lang', 'en-GB'); 						
					});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.head.querySelector('title')) return;
						var handleAppendTitle = function(evt) {
							if (!evt || !evt.data) return;

							try {
								var data = JSON.parse(evt.data);
								if (data.handler !== 'd2l.iframe.requestPageTitle' || !data.pageTitle) return;

								window.removeEventListener('message', handleAppendTitle, false);

								var titleElm = document.createElement('title');
								titleElm.textContent = data.pageTitle;
								document.head.appendChild(titleElm);
							} catch (e) {}	
						};

						window.addEventListener('message', handleAppendTitle, false);
						window.parent.postMessage(JSON.stringify({ handler: 'd2l.iframe.requestPageTitle' }), '*');
					});</script><script>window.addEventListener('message', function(event) { 
					if( !event.data ) {
						return;
					}

					var params;
					try {
						params = JSON.parse( event.data );
					}
					catch {
						return;
					}
					if( !params.subject || params.subject !== 'lti.frameResize' ) {
						return;
					}

					const MAX_FRAME_HEIGHT = 10000
					if( !params.height || params.height < 1 || params.height > MAX_FRAME_HEIGHT ) {
						console.warn( 'Invalid height value received, aborting' );
						return;
					}
					var el = document.getElementsByTagName( 'iframe' );
					for ( var i=0; i < el.length; i++ ) {
						if( el[i].contentWindow === event.source ) {
							el[i].style.height = params.height + 'px';
							el[i].style.width = '100%';
							console.info( 'Setting iFrame height to ' + params.height );
							console.info( 'Setting iFrame width to 100%' );
						}
					}
				});</script></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><h1>Statistical learning theory of SVM</h1>
<p align="center"><iframe src="https://deakin.au.panopto.com/Panopto/Pages/Embed.aspx?id=d62dca53-2765-46a4-95e2-afe70122a06d&amp;autoplay=false&amp;offerviewer=true&amp;showtitle=true&amp;showbrand=true&amp;captions=true&amp;interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen="" allow="autoplay" aria-label="Panopto Embedded Video Player"></iframe></p>
<table class="tableClear" style="width: 880px;">
<tbody>
<tr>
<td style="width: 59px;"></td>
<td style="width: 466.433px;"><a id="viewTranscript">View transcript</a></td>
<td style="width: 271.567px;"></td>
</tr>
</tbody>
</table>
<article class="transcript transcript" id="transcript-en">
<p class="transcript__para">We have seen several examples of model performances in complexity. We have also talked many times about performance in machine learning models. But now we would like to explain VC dimension, a quantity of complexity which captures much of what we need in machine learning models. First, suppose we pick n instances and assign labels of positive and negative to them randomly. If our hypothesis class is rich enough to learn any association of labels to the data, it's sufficiently complex.</p>
<p class="transcript__para">Consider this figure. There are only two cases in here, whether this positive point is in the right side or in the left side. In both cases, this separating line is rich enough to learn any association of labels to this data. So we call that it's sufficiently complex. We characterise the complexity of hypothesis class by looking at how many instances it can shatter. So this can fit perfectly for all possible label assignments. The number of instances the hypothesis class can shatter is called the VC dimension. In other words, VC dimension of a function said f is the cardinality of the largest dataset that can be shattered by f.</p>
<p class="transcript__para">Let's see an example. Let us assume that we are using a line or a hyperplane as our hypothesis class. If you look at this figure, you can see we can find at least one set of three points into 2D dimension, which all of whose eight possible labelling can be separated by some hyperplane. If you consider the first one, it has been separated by one line. Also, this case. Also, the case which we only have one label, which are these two cases. And also, in other cases, in all eight possible cases, a line could separate this data. But is it possible for this line to shatter labelling of four points? As we can see, it's not, because in this case or whether in this case, we can see one single line is not enough for the model to shatter the data points. And sometimes we need more than one line. So that's why we said if we see dimension of a line in 2D dimension is 3.It means that we can handle all the possible labelings of these three points into 2D dimension. That was a simple definition of VC dimension.</p>
</article>
<p></p>
<div>
<p>Theoretically, does&nbsp;<em>maximum margin</em>&nbsp;make sense?</p>
<p>As you will recall,&nbsp;<em>structural risk minimisation</em>&nbsp;seeks to prevent over-fitting by incorporating a&nbsp;<em>penalty</em>&nbsp;on the model complexity. This means, it prefers&nbsp;<em>simpler</em>&nbsp;functions over more&nbsp;<em>complex</em>&nbsp;functions. The general idea is to minimise the&nbsp;<em>structural risk</em>&nbsp;as where&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h(f)\)"}</annotation></semantics></math>&nbsp;is the complexity of hypothesis function&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>f</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(f\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>λ<!-- λ --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda\)"}</annotation></semantics></math>&nbsp;is a penalty parameter:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>s</mi><mi>t</mi><mi>r</mi></mrow></msub><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>e</mi><mi>m</mi><mi>p</mi></mrow></msub><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>+</mo><mi>λ<!-- λ --></mi><mi>h</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ R_{str} (f) = R_{emp}(f) + \lambda h(f) \\"}</annotation></semantics></math></p>
<p>So we would like choose a less complex model with a small error.</p>
<p>Suppose we pick&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\)"}</annotation></semantics></math>&nbsp;instances and assign labels of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>+</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(+\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(-\)"}</annotation></semantics></math>&nbsp;to them randomly. If our hypothesis class is rich enough to learn any&nbsp;<em>association</em>&nbsp;of labels to the data, it is sufficiently&nbsp;<em>complex</em>.</p>
<p>How about we characterise the complexity of the hypothesis class by looking at how many instances it can&nbsp;<em>shatter</em>(i.e. can fit perfectly for all possible label assignments). The number of instances a hypothesis class can shatter is called its&nbsp;<em>Vapnik-Chervonenkis</em>&nbsp;(<a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory" target="_blank" rel="noopener noreferrer">VC</a>) Dimension.</p>
<h3 id="an-illustration-of-vc-dimension">An Illustration of VC Dimension</h3>
<p>Let us assume that we are using lines (or hyperplanes) as our hypothesis class. In&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>2</mn><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(2-\)"}</annotation></semantics></math>dimension, we can find a line to shatter any labelling of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>3</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(3\)"}</annotation></semantics></math>&nbsp;points. But a line may not be able to shatter some labelling of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>4</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(4\)"}</annotation></semantics></math>.&nbsp;Therefore, VC dimension of a line in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>2</mn><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(2-\)"}</annotation></semantics></math>dimension is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>3</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(3\)"}</annotation></semantics></math>. (In<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d-\)"}</annotation></semantics></math>dimension:&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>+</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d+1\)"}</annotation></semantics></math>.</p>
<p>Consider the following figure. As you can see in the top image, these 3 points with&nbsp;<em>any</em>&nbsp;combination of labels can be separated by a line. It doesn’t matter if you change the labels of the data points.</p>
<p class="centerImage"><img src="../images/Statistical%20learning%20theory%20of%20SVM%20image%201.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;illustration of shattering 3 points by a line vs 4 points where this not possible.</h5>
<p>As you can see from some of the example in the figure, we can successfully separate these points with a line. But in the bottom images you can see that we can come up with situations in which we can not use a single line to separate these data points.</p>
<p>Because in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>2</mn><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(2-\)"}</annotation></semantics></math>dimension, we can always find a line to shatter&nbsp;<em>any</em>&nbsp;labelling of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>3</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(3\)"}</annotation></semantics></math>&nbsp;points. It might not be possible to find a line to shatter&nbsp;<em>any</em>&nbsp;labelling of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>4</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(4\)"}</annotation></semantics></math>&nbsp;points.</p>
<p>The theoretical justification for maximum margin is shown by Vapnik in the following results. The class of optimal linear separators has VC dimension&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>&nbsp;bounded from above as:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>h</mi><mo>≤<!-- ≤ --></mo><mi>m</mi><mi>i</mi><mi>n</mi><mo fence="false" stretchy="false">{</mo><mi>d</mi><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">[</mo></mrow><mfrac><msup><mi>D</mi><mn>2</mn></msup><msup><mi>ρ<!-- ρ --></mi><mn>2</mn></msup></mfrac><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">]</mo></mrow><mo fence="false" stretchy="false">}</mo><mo>+</mo><mn>1</mn><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ h \leq min\{d,\Big[\frac{D^2}{\rho^2}\Big]\} + 1 \\"}</annotation></semantics></math></p>
<p>Where&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>p</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(p\)"}</annotation></semantics></math>&nbsp;is the margin,&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>D</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(D\)"}</annotation></semantics></math>&nbsp;&nbsp;is the diameter of the smallest sphere that can enclose all of the training examples, and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\)"}</annotation></semantics></math>&nbsp;is the dimensionality.</p>
<p>Intuitively, this implies that regardless of dimensionality&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\)"}</annotation></semantics></math>,&nbsp;we can minimise the model complexity (VC dimension) by maximising the margin&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>p</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(p\)"}</annotation></semantics></math>. If&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>p</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(p\)"}</annotation></semantics></math>&nbsp;is maximised or in other words, if we look for a classifier with&nbsp;<em>high margins</em>&nbsp;it means that we have a smaller value for&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">[</mo></mrow><mfrac><msup><mi>D</mi><mn>2</mn></msup><msup><mi>ρ<!-- ρ --></mi><mn>2</mn></msup></mfrac><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">]</mo></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\Big[\frac{D^2}{\rho^2}\Big]\)"}</annotation></semantics></math>,&nbsp;therefore we are going to have a smaller upper bound for the complexity of our model&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>.</p>
<p>To conclude, maximising margins will result in having a less complex model (a smaller upper bound for&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>)&nbsp;</p>
<p>This almost proves why we aim for maximising margins in SVM.</p>
<h4 id="but-what-is-the-the-probabilistic-guarantee">But what is the The Probabilistic Guarantee?</h4>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></msub><mo>≤<!-- ≤ --></mo><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>+</mo><mrow class="MJX-TeXAtom-ORD"><mo maxsize="2.470em" minsize="2.470em">(</mo></mrow><mfrac><mrow><mi>h</mi><mo>+</mo><mi>h</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mrow><mn>2</mn><mi>N</mi></mrow><mi>h</mi></mfrac><mo stretchy="false">)</mo><mo>−<!-- − --></mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mi>p</mi><mn>4</mn></mfrac><mo stretchy="false">)</mo></mrow><mi>N</mi></mfrac><msup><mrow class="MJX-TeXAtom-ORD"><mo maxsize="2.470em" minsize="2.470em">)</mo></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ E_{text} \leq E_{train} + \Bigg( \frac{h+h\ log(\frac{2N}{h}) - log(\frac{p}{4})}{N}\Bigg)^\frac{1}{2} \\"}</annotation></semantics></math></p>
<p>In which:</p>
<ul>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>=</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E_{train} =\)"}</annotation></semantics></math>&nbsp;Error on training set&nbsp;</li>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub><mo>=</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E_{test} =\)"}</annotation></semantics></math>&nbsp;Error on test set (generalisation error)</li>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>N</mi><mo>=</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(N =\)"}</annotation></semantics></math>&nbsp;Size of training set</li>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo>=</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h =\)"}</annotation></semantics></math>&nbsp;VC dimension of the hypothesis class</li>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>p</mi><mo>=</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(p =\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;upper bound on probability that this&nbsp;</span><em style="font-family: Lato, sans-serif; font-size: 0.95rem;">bound</em><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;fails</span></li>
</ul>
<p>Do not get too confused with the formulas. This equation states that the test error (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E_{test}\)"}</annotation></semantics></math>)&nbsp;is upper bounded by the training error (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E_{train}\)"}</annotation></semantics></math>)&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>+</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(+\)"}</annotation></semantics></math>&nbsp;<em>some value</em>. Definitely we would want to minimise this value to be as accurate as possible like our training model. But how can we minimise it?</p>
<p>Look at the equation:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mrow class="MJX-TeXAtom-ORD"><mo maxsize="2.470em" minsize="2.470em">(</mo></mrow><mfrac><mrow><mi>h</mi><mo>+</mo><mi>h</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mrow><mn>2</mn><mi>N</mi></mrow><mi>h</mi></mfrac><mo stretchy="false">)</mo><mo>−<!-- − --></mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mi>p</mi><mn>4</mn></mfrac><mo stretchy="false">)</mo></mrow><mi>N</mi></mfrac><msup><mrow class="MJX-TeXAtom-ORD"><mo maxsize="2.470em" minsize="2.470em">)</mo></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \Bigg( \frac{h+h\ log(\frac{2N}{h}) - log(\frac{p}{4})}{N}\Bigg)^\frac{1}{2} \\"}</annotation></semantics></math></p>
<p>You have only two ways to minimise this.</p>
<p>First increase&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>N</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(N\)"}</annotation></semantics></math>&nbsp;which is the number of training samples, basically, increase the number of samples for training, which is obvious.</p>
<p>The second way is to minimise&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>&nbsp;which is the complexity of the model. This is really useful.</p>
<p>This equation states, by reducing the complexity of the model, you have a higher chance for smaller test values (smaller upper bound). So this is another way to show the importance of maximising margins and handling the complexity of the models.</p>
<h2 id="your-task">Activity</h2>
<p>How do the following statements influence model choice?</p>
<ol>
<li>The upper bound on the generalisation error&nbsp;<em>increases</em>&nbsp;with higher complexity (higher&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>)</li>
<li>The upper bound on the generalisation error&nbsp;<em>reduces</em>&nbsp;with larger training sets (higher&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>N</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(N\)"}</annotation></semantics></math>).</li>
</ol>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>