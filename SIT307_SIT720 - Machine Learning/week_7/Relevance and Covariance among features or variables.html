<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

</head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/SVM%20formulation%20and%20solution%20for%20linearly%20non-separable%20data%20image%201.jpg" alt="3D Rendering Of Abstract Low Poly Shape With Colour Mesh Background" title="3D Rendering Of Abstract Low Poly Shape With Colour Mesh Background" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>SVM formulation and solution for linearly non-separable data</h1>
<div id="comments-link-heading"></div>
</div>
<div>
<p>In SVM, we have so far assumed that data is linearly separable. What approach should we take when data is not linearly separable?</p>
<p>Sometimes, data can be&nbsp;<em>linearly separable</em>&nbsp;but with a narrow margin. At other times, due to noise, some of the instances may not be linearly separable (see the figure for noisy data).</p>
<p class="centerImage"><img src="../images/SVM%20formulation%20and%20solution%20for%20linearly%20non-separable%20data%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;Noise in data.</h5>
<p>It is generally preferred not to interfere with the boundary even with small&nbsp;<em>noisy</em>&nbsp;data points or outliers. It is acceptable to have large margins even though some of the constraints are violated. In practice, we need a trade-off between the&nbsp;<em>margin</em>&nbsp;and the&nbsp;<em>number of errors</em>&nbsp;in classifying the training instances.</p>
<p>This trade-off brings us to the&nbsp;<em>soft margin</em>&nbsp;concept. Consider the following figure; the soft margin concept is defined when the training instances are&nbsp;<em>not linearly</em>&nbsp;separable. Slack variables&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>ζ<!-- ζ --></mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\zeta_i\)"}</annotation></semantics></math>&nbsp;</p>
<p>are added to allow misclassification of outliers, noisy or difficult to classify instances. So basically we are allowing some of the data points to cross the borders and to be in the wrong side of the boundary or to be misclassified.</p>
<p class="centerImage"><img src="../images/SVM%20formulation%20and%20solution%20for%20linearly%20non-separable%20data%20image%203.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;Soft margin concept.</h5>
<p>Although we allow some of the training instances to be misclassified, we still want to&nbsp;<em>minimise</em>&nbsp;the sum of slack variables. So for those data points which their&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>ζ<!-- ζ --></mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\zeta_i\)"}</annotation></semantics></math>&nbsp;value is non-zero, we can infer that they are misclassified, and the amount of misclassification is also presented by&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>ζ<!-- ζ --></mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\zeta_i\)"}</annotation></semantics></math>.</p>
<p>SVM with soft margin uses the following formulation:</p>
<p class="centerImage"><img src="../images/SVM%20formulation%20and%20solution%20for%20linearly%20non-separable%20data%20image%204.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;__Soft margins</h5>
<p>The parameter&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;can be used as a way to achieve the&nbsp;<em>trade-off</em>&nbsp;between large margins and fitting training data. For the high values of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>,&nbsp;&nbsp;we highly penalise the misclassification but for the small values of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>,&nbsp;we allow more misclassifications. That is how SVM handles this trade-off around misclassification.</p>
<h4 id="soft-margin-dual-problem">Soft margin dual problem</h4>
<p>The&nbsp;<em>soft margin dual problem</em>&nbsp;is defined when we change the&nbsp;<em>primal problem</em>&nbsp;with soft margins to dual. It remains the same except that there is an&nbsp;<em>upper bound</em>&nbsp;on the Lagrange multipliers.</p>
<p>The dual problem is given as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>α<!-- α --></mi></mrow></munder><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>α<!-- α --></mi><mi>i</mi></msub><mo>−<!-- − --></mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>α<!-- α --></mi><mi>i</mi></msub><msub><mi>α<!-- α --></mi><mi>j</mi></msub><msub><mi>y</mi><mi>i</mi></msub><msub><mi>y</mi><mi>j</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>j</mi></msub><mspace linebreak="newline"></mspace><mspace linebreak="newline"></mspace><mi>S</mi><mi>u</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>o</mi><mo>:</mo><mtext>&nbsp;</mtext><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>α<!-- α --></mi><mi>i</mi></msub><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi>a</mi><mi>n</mi><mi>d</mi><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mn>0</mn><mo>≤<!-- ≤ --></mo><msub><mi>α<!-- α --></mi><mi>i</mi></msub><mo>≤<!-- ≤ --></mo><mi>C</mi><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi mathvariant="normal">∀<!-- ∀ --></mi><mi>i</mi><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \min_{\alpha} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n}\sum_{i=1}^{n} \alpha_i\alpha_j y_i y_j \textbf{x}_i^T \textbf{x}_j \\ \\ Subject\ to:\ \sum_{i=1}^{n} \alpha_i y_i = 0\ \ and \ \ 0 \leq \alpha_i \leq C \ \ \forall i \\"}</annotation></semantics></math></p>
<p>As you can see the difference is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;Given a solution to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>α<!-- α --></mi><mo>=</mo><mo stretchy="false">[</mo><msub><mi>α<!-- α --></mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>α<!-- α --></mi><mi>n</mi></msub><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\alpha = [\alpha_1,...,\alpha_n]\)"}</annotation></semantics></math>&nbsp;the hyperplane&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>&nbsp;is given as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo>=</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>α<!-- α --></mi><mi>i</mi></msub><msub><mi>y</mi><mi>i</mi></msub><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub><mspace linebreak="newline"></mspace><mspace linebreak="newline"></mspace><mi>b</mi><mo>=</mo><msub><mi>y</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>−<!-- − --></mo><msub><mi>ζ<!-- ζ --></mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>−<!-- − --></mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>α<!-- α --></mi><mi>i</mi></msub><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>k</mi></msub><mtext>&nbsp;</mtext><mi>u</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>g</mi><mtext>&nbsp;</mtext><mi>a</mi><mi>n</mi><mi>y</mi><mtext>&nbsp;</mtext><mi>k</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><mi>c</mi><mi>h</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>h</mi><mi>a</mi><mi>t</mi><mtext>&nbsp;</mtext><msub><mi>α<!-- α --></mi><mi>k</mi></msub><mo>&gt;</mo><mn>0</mn><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \textbf{w} = \sum_{i=1}^{n} \alpha_i y_i \textbf{x}_i \\ \\ b =y_k(1-\zeta_k) - \sum_{i=1}^{n} \alpha_i y_i \textbf{x}_i^T \textbf{x}_k\ using\ any\ k\ such\ that\ \alpha_k &gt; 0 \\"}</annotation></semantics></math></p>
<p>Once again there is one&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>α<!-- α --></mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\alpha_i\)"}</annotation></semantics></math>&nbsp;corresponding to each&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_i\)"}</annotation></semantics></math>. The&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_i\)"}</annotation></semantics></math>&nbsp;corresponding to each non-zero&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>α<!-- α --></mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\alpha_i\)"}</annotation></semantics></math>&nbsp;is called a support vector. Given&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>b</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(b\)"}</annotation></semantics></math>,&nbsp;we can write the classification function as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>f</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo>+</mo><mi>b</mi><mo>=</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>α<!-- α --></mi><mi>i</mi></msub><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo>+</mo><mi>b</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"f(\textbf{x}) = \textbf{w}^T\textbf{x} + b = \sum_{i=1}^{n} \alpha_i y_i \textbf{x}_i^T \textbf{x} + b"}</annotation></semantics></math></p>
<p>Once again solving for&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>α<!-- α --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\alpha\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>b</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(b\)"}</annotation></semantics></math>,&nbsp;we need to use training data only in the form of dot products&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}_{i}^{T}\textbf{x}\)"}</annotation></semantics></math>&nbsp;Further, the classification function uses only a dot product between&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}\)"}</annotation></semantics></math>&nbsp;and support vectors<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}_i\)"}</annotation></semantics></math>.</p>
<h3 id="summary">Summary</h3>
<p>In the previous step we investigated how an SVM handles perfectly separable data points. In this lesson we looked at how it handles&nbsp;<em>almost separable</em>&nbsp;data points. In the next lesson we are going to review non-linear SVMs.</p>
<h2 id="your-task">Activity</h2>
<p>Reinforce your understanding with this video.</p>
<p></p>
<div>
<p class="centerVideo"><iframe width="648" height="364" src="https://www.youtube.com/embed/5oVQBF_p6kY?wmode=opaque" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></p>
<h5>This is an additional video, hosted on YouTube.</h5>
</div>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>