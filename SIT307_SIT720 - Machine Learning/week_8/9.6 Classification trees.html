<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

</head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Classification%20trees%20image%201.jpg" alt="Low Angle View Of Plants Photo Taken In Novi Sad, Serbia" title="Low Angle View Of Plants Photo Taken In Novi Sad, Serbia" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Classification trees</h1>
</div>
<div>
<p><em>Classification and Regression Trees</em>&nbsp;(CART) is a term introduced by Leo Breiman to refer to&nbsp;<em>decision tree algorithms</em>&nbsp;that can be used for classification or regression predictive modeling problems.</p>
<p>It’s similar to regression trees, except that it is used to predict a&nbsp;<em>qualitative response</em>&nbsp;rather than a quantitative response. For a classification tree, we assign each test instance to the&nbsp;<em>majority class (mode)</em>&nbsp;of the training instances in the region where it belongs. You can consider this action as a being like a data point voting itself into a region which results in selecting the majority.</p>
<p>In the classification setting, we replace the&nbsp;<em>sum of square error</em>&nbsp;by the&nbsp;<em>classification error rate</em>&nbsp;as a criterion for making the binary splits. The classification error rate&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>E</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E\)"}</annotation></semantics></math>&nbsp;is defined as the fraction of the training instances in that region that do not belong to the most common class. Where&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>p</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mi>k</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{p}_{jk}\)"}</annotation></semantics></math>&nbsp;represents the proportion (fraction) of training instances in the&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>j</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(j-\)"}</annotation></semantics></math>&nbsp;region that are from&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k-\)"}</annotation></semantics></math>th class:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>E</mi><mo>=</mo><mn>1</mn><mo>−<!-- − --></mo><munder><mo movablelimits="true" form="prefix">max</mo><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munder><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>p</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mi>k</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E = 1 - \max_{k} \hat{p}_{jk}\)"}</annotation></semantics></math></p>
<p>Basically, Certainty of Distribution (COD) shows how certain it is that a classifier sits inside a region.</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mtext>Let&nbsp;</mtext><mi>C</mi><mi>o</mi><mi>D</mi><mo>=</mo><munder><mo movablelimits="true" form="prefix">max</mo><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munder><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>p</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mi>k</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\text{Let }CoD = \max_{k} \hat{p}_{jk}"}</annotation></semantics></math></p>
<p>If&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi><mi>o</mi><mi>D</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(CoD\)"}</annotation></semantics></math>&nbsp;is close to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>,&nbsp;it means almost all of the training points inside a region are voting for a certain class label. So the classifier in this case is certain about the decision. On the other hand, when&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi><mi>o</mi><mi>D</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(CoD\)"}</annotation></semantics></math>&nbsp;is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0.5</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0.5\)"}</annotation></semantics></math>&nbsp;it means we can not trust the votes because there is a high classification error rate (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>E</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E\)"}</annotation></semantics></math>).</p>
<p>But one of the problems of classification error is that it’s&nbsp;<em>less sensitive</em>&nbsp;for tree-growing.</p>
<h3 id="gini-and-entropy">Gini and Entropy</h3>
<p>In practice people would prefer to use the&nbsp;<em>Gini index</em>&nbsp;and&nbsp;<em>Entropy</em>. The Gini index is the most commonly used measurement of inequality. For example in economics, the Gini index represents the income or wealth distribution of residents in a country. The Gini index is defined as&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>G</mi><mo>=</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>K</mi></mrow></munderover><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>p</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo>−<!-- − --></mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>p</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(G = \sum_{k=1}^{K} \hat{p}_{jk} (1 - \hat{p}_{jk})\)"}</annotation></semantics></math></p>
<p>Again&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>p</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mi>k</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{p}_{jk}\)"}</annotation></semantics></math>&nbsp;represents the proportion (fraction) of training instances in the<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>j</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(j-\)"}</annotation></semantics></math>th&nbsp;region that are from&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k-\)"}</annotation></semantics></math>th class.&nbsp;Its is a measure of total variance across the&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>K</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(K\)"}</annotation></semantics></math> classes. It takes a&nbsp;<em>small value</em>&nbsp;if&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>p</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mi>k</mi></mrow></msub><mo>≈<!-- ≈ --></mo><mn>0</mn><mo>,</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{p}_{jk} \approx 0,1\)"}</annotation></semantics></math>&nbsp;for all&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k\)"}</annotation></semantics></math>&nbsp;(why?).</p>
<p>Because it faces with a low inequality distribution in the votes. Gini index is therefore considered a measure of&nbsp;<em>node purity</em>.</p>
<p>Consider the following figure, let say the Gini index and Entropy lines are representing the probability of selecting a particular class for a data point. For example the fraction of training points in a region which are voting for class label&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;for a point. As you can sea in the figure, in the corners we have small values of Gini Index and mis-classification, but in the middle area which both classes are representing the same chance for classification, we have high error and it shows the most equality peak in the middle which results in high mis-classification error. If you check the Gini index, it is not linear but mis-classification is linear in form. It seems Entropy is smoother and better in this case.</p>
<p class="centerImage"><img src="../images/Classification%20trees%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Illustration of Gini index, Entropy vs. Classification Error.</h5>
<p>As an alternative to Gini index,&nbsp;<em>Entropy</em>, is defined as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>D</mi><mo>=</mo><mo>−<!-- − --></mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>K</mi></mrow></munderover><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>p</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mi>k</mi></mrow></msub><mi>l</mi><mi>o</mi><mi>g</mi><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>p</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mi>k</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"D = -\sum_{k=1}^{K} \hat{p}_{jk} log \hat{p}_{jk}"}</annotation></semantics></math></p>
<p>Again we have the same concept in this formula too.</p>
<p>Let us see another example on the Gini Index. Consider a two class problem with following splits:</p>
<p class="centerImage"><img src="../images/Classification%20trees%20image%203.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure: Gini index example</h5>
<p>As you can see, the value of Gini index is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;when facing with high inequalities such as the first case which&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi><mn>1</mn><mo>=</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C1=0\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi><mn>2</mn><mo>=</mo><mn>6</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C2=6\)"}</annotation></semantics></math>&nbsp;and its values gets bigger when we are dealing with equally distributed cases such&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi><mn>1</mn><mo>=</mo><mn>3</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C1 = 3\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi><mn>2</mn><mo>=</mo><mn>3</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C2 = 3\)"}</annotation></semantics></math>&nbsp;which results in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0.5</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0.5\)"}</annotation></semantics></math>&nbsp;for Gini index.</p>
<h2 id="your-task">Activity</h2>
<p>Can you think of some real-world usages of the&nbsp;<em>Gini Index</em> in economics?</p>
<hr style="width: 100%; height: auto; color: #ffffff; border: 1px inset #cccccc;">
<h4 id="references">References</h4>
<p>Breiman, L 1984, Classification and Regression Trees, New York: Routledge.</p>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>