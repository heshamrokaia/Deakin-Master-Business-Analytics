<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <title>CloudFirst CloudDeakin Template</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" type="text/css" href="/shared/HTML Templates/CloudFirst/styles/cloudfirst.css?v=20210913">
    <script src="/content/deakinscripts/h5pResponsive.js" type="text/javascript"></script>
    <link rel="stylesheet" type="text/css" href="/content/deakin-embedded-comments/embeddedComments.css?v=20201007">
</head><body data-template-type="cloudFirst"><h3></h3>
<h3>Possible Vulnerability Sources</h3>
<p style="text-align: justify;"><span style="list-style-type: unset;">A vulnerability is a weakness in a system that allows a curious or malicious attacker to gain unauthorized access. Identifying these vulnerabilities is essential for creating a more secure environment by implementing necessary safeguards to defend against potential loopholes. Failing to protect personally identifiable information (PII) or to comply with data protection laws can result in severe legal consequences. In the Federated Learning (FL) ecosystem, vulnerability sources can be categorized into five distinct groups.</span></p>
<ul>
<li style="text-align: justify;"><strong>Source 1 - </strong><span style="list-style-type: unset;"><strong>Communication Protocol:</strong> Federated Learning employs an iterative learning process that involves randomly selected clients, resulting in a substantial amount of network communication. To protect the anonymity of clients and the contents of their messages, a mixed network utilizing public key cryptography is implemented. However, it consists of multiple training rounds during which communications are not encrypted, leaving this non-secure communication as a significant vulnerability.</span></li>
<li style="text-align: justify;"><strong>Source 2 - Client-data Manipulations: </strong><span style="list-style-type: unset;">Federated learning in a large landscape involves numerous clients that may be susceptible to attacks aimed at exploiting model parameters and training data. Furthermore, access to the global model could be vulnerable to data reconstruction attacks, which involve methods to partially reconstruct a private dataset from publicly available aggregate information.</span>&nbsp;</li>
<li style="text-align: justify;"><strong>Source 3 - Compromised Central Server:</strong> <span style="list-style-type: unset;">In theory, the central server is expected to be robust and secure, as it plays a crucial role in distributing initial model parameters, aggregating local models, and disseminating global model updates to all clients. However, in practice, achieving 100% security for a server proves to be challenging. Whether cloud-based or physical, the central server must undergo regular and thorough checks to prevent the exploitation of any existing vulnerabilities.</span>&nbsp;</li>
<li style="text-align: justify;"><strong>Source 4 -&nbsp;Weak Aggregation Algorithm:</strong> <span style="list-style-type: unset;">The aggregation algorithm serves as the central authority in the process. When updating the local model, it must be capable of identifying any abnormalities in client updates and should include a configuration to discard updates from suspicious clients. If the aggregation algorithm is not properly configured, the global model may become vulnerable to malicious updates.</span>&nbsp;</li>
<li style="text-align: justify;"><strong>Source 5 - Federated Learning Implementation Environment:</strong> <span style="list-style-type: unset;">A team of architects, developers, and deployers involved in the implementation of federated learning can unintentionally create security risks. Often, confusion or a lack of understanding about what constitutes sensitive user data leads to data breaches. The implementation team may fail to take appropriate measures to identify and manage sensitive data, and they might misrepresent the facts when obtaining user consent regarding data usage.</span>&nbsp;</li>
</ul>
<div class="cf_instructions"></div>
<ul></ul>
<h3>Threat Model</h3>
<p><img src="image_20250623020600492.png" width="556" height="407"></p>
<p style="text-align: center;">Figure 10.7: A Threat Model For Federated Learning Architecture [2]</p>
<p style="text-align: justify;"><strong>Attack 1 - Poisoning:</strong> <span style="list-style-type: unset;">Poisoning occurs when tampered data weights are added to the global machine learning model. This risk is particularly high in federated learning, where various aspects of the process can be targeted. Poisoning can happen during the training phase, impacting either the training dataset or the local model, which in turn can affect the performance and accuracy of the global model. The likelihood of poisoning attacks from one or more clients’ training data is significant, as is the severity of the threat. </span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;"><img src="image_20250623020600606.png" width="766" height="188"></span></p>
<p style="text-align: center;"><span style="list-style-type: unset;">Figure 10.8: Poisoning Attack in federated learning </span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;">Data poisoning: In federated learning, data poisoning is defined as the generation of “dirty” samples intended to train the global model, with the aim of producing falsified model parameters that are then sent to the server. Data injection can be viewed as a subcategory of data poisoning, where a malicious client injects harmful data into the processing of the local model. Consequently, the attacker may gain control over multiple clients’ local models, ultimately manipulating the global model with the malicious data.</span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;"><img src="image_20250623021141695.png" width="530" height="202"></span></p>
<p style="text-align: center;"><span style="list-style-type: unset;">Figure 10.9: Data and Model Poisoning Attack in federated learning </span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;">Model poisoning: In model poisoning attacks, the malicious agent typically targets the global model directly. During this type of attack, the adversary can manipulate the updated model before it is sent to the central server for aggregation, which can easily compromise the global model. Research indicates that model poisoning attacks are generally more effective than data poisoning attacks. Moreover, the effectiveness of model poisoning tends to increase in large-scale federated learning environments with numerous clients. </span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;">Data Modification: Data tampering or modification attacks can involve changing or altering the training dataset. For example, feature collision occurs when two classes in the dataset are merged, tricking the machine learning model into consistently misclassifying the targeted class. Additionally, introducing a shade or pattern from another class to the targeted class can confuse the model. Another method involves randomly swapping labels within the training dataset. </span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;"><strong>Attack 2 - Backdoor Attacks:</strong> A backdoor attack (a Malware) injects a malicious task into an existing machine learning model while maintaining the model's accuracy for the legitimate task. Identifying backdoor attacks can be challenging and time-consuming, as the accuracy of the actual machine learning task may not be affected immediately. The severity of backdoor attacks is high because it often takes a significant amount of time to detect them. These attacks can have a considerable impact, as they can confuse machine learning models, leading them to confidently predict false positives.</span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;"><strong>Attack 3 - GANs: </strong>A Generative Adversarial Network (GAN) is a machine learning model that operates like a creative competition between two neural networks: the Generator and the Discriminator. The Generator's role is to create synthetic data—such as images, music, or text—that appears authentic, while the Discriminator's task is to distinguish between real data sourced from the training set and the artificial data generated by the Generator. </span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;">Generative Adversarial Network (GAN)-based attacks in federated learning have been extensively studied and analyzed by numerous researchers. Due to their ability to execute poisoning and inference attacks, GAN-based threats present significant risks. GANs can be utilized to extract training data through inference and then employ that information to poison the training dataset. Consequently, GAN-based threats are considered high-impact and are prioritized as critical vulnerabilities. </span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;"><strong>Attack 4 - System Disruption:</strong> System disruptions are frequently encountered when highly configured and secured applications must undergo downtime due to unplanned or scheduled activities on back-end servers. In federated learning, the severity of this threat is low because each client operates with a local-global model, allowing the training process to resume after an outage. However, despite the low severity, this remains a significant threat, as planned downtime could potentially be an attack aimed at stealing information from the FL environment. </span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;"><strong>Attack 5 - Malicious Server:</strong> In cross-device federated learning, the majority of the processing occurs at the central server. This server is responsible for selecting model parameters and deploying the global model. The presence of compromised or malicious servers can have a significant negative impact. Honest yet curious or malicious servers can readily access private client data or manipulate the global model, potentially exploiting shared computational resources to create harmful tasks within the overall machine learning model. </span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;"><strong>Attack 6 - Communication Bottlenecks:</strong> Training a machine learning model using data from multiple heterogeneous devices presents a challenge due to communication bandwidth limitations. In the federated learning approach, this issue is mitigated by transferring trained models rather than sending large amounts of raw data. The impact of communication bottlenecks in this context can be severe, as they can significantly disrupt the federated learning environment. </span></p>
<p style="text-align: justify;"><span style="list-style-type: unset;"><strong>Attack 7 - Free-riding Attack:</strong> A Free-Riding Attack in Federated Learning (FL) occurs when a participant, or client, attempts to benefit from the global model without making meaningful contributions. Essentially, these clients gain rewards without doing any work. Some clients take on a passive role, connecting to the network solely to take advantage of the benefits offered by the global machine learning model without contributing to the training process. In some cases, these passive clients may submit dummy updates instead of training the model with their local data. This type of attack has a more significant impact in smaller FL environments, as a lack of client participation can adversely affect the training of the global model. While the likelihood of this attack is low, its potential severity is considered medium. </span></p>
<p style="text-align: justify;"><strong><span style="list-style-type: unset;">To explore the defence strategices please focus on the following table:</span></strong></p>
<p style="text-align: justify;"><strong><span style="list-style-type: unset;"><img src="image_20250623112449954.png" width="872" height="341"></span></strong></p>
<hr>
<h3>References</h3>
<ol>
<li>Mothukuri, V., Parizi, R.M., Pouriyeh, S., Huang, Y., Dehghantanha, A. and Srivastava, G., 2021. A survey on security and privacy of federated learning. <em>Future Generation Computer Systems</em>, <em>115</em>, pp.619-640.</li>
<li>Liu, P., Xu, X. and Wang, W., 2022. Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives. <em>Cybersecurity</em>, <em>5</em>(1), p.4.</li>
</ol>
<hr>
<div><iframe src="/content/deakin-embedded-comments/embedded-comments-loader.html?v=20201006" style="position: absolute; left: -9999px; top: -9999px; width: 0px; height: 0px;"></iframe> <!--FORUMVARIABLESTART-->
<div style="display: none;" data-forumname="null"></div>
<!--FORUMVARIABLEEND--></div>
<div>
<script type="text/javascript" src="https://s.brightspace.com/lib/jquery/2.2.4/jquery.min.js"></script>
<script src="/shared/HTML Templates/CloudFirst/scripts/cloudfirst-v1.js?v=20210913"></script>
<script type="text/javascript" src="/content/deakinscripts/moment/moment.2.21.js"></script>
<script>
		// Nav Button
		$("body").append('<div style=" padding-top: 50px; padding-bottom: 50px; clear: both;"><hr style="width: 100%; height: auto; color: #ffffff; border: 1px inset #cccccc;" /><p style="padding-bottom:5px;"></p><a title="Previous" class="navrep-button" style="float: left;" href="#" target="_parent"> <i class="fa fa-chevron-left"></i> Previous </a> <a title="Next" class="navrep-button" style="float: right;"  href="#" target="_parent"> Next <i class="fa fa-chevron-right"></i> </a></div>');
		// Find Parent frame
		var backLink = $('body .d2l-iterator-button-prev', window.parent.parent.document).attr('href');
		var nextLink = $('body .d2l-iterator-button-next', window.parent.parent.document).attr('href');
		// console.log('backLink > ', backLink);
		// console.log('nextLink > ', nextLink);
		// Apply to each button
		$('.navrep-button:contains("Previous")').attr('href', backLink);
		$('.navrep-button:contains("Next")').attr('href', nextLink);
	</script>
<script type="text/javascript" src="/content/deakin-embedded-comments/embeddedComments.js?v=20201007"></script>
</div></body></html>