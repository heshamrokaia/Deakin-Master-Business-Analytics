<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

</head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Autoencoder%20image%201.jpg" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Autoencoder</h1>
</div>
<div>
<p>An Autoencoder is a neural network which can handle many hidden layers in its structure.</p>
<p>The aim of an Autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of&nbsp;<em>dimensionality reduction</em>.</p>
<p>This type of neural network is trained to attempt to copy its input to its output. Internally, it has a hidden layer&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Z</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Z\)"}</annotation></semantics></math>&nbsp;that describes a code used to represent the input. Recently, the Autoencoder concept has become more widely used for learning generative models of data.</p>
<p class="centerImage"><img src="../images/Autoencoder%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Simple structure of Autoencoder</h5>
<p>Consider the above figure as a simple illustration of Autoencoder. The aim is pretty simple. We would like to reconstruct the&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>=</mo><mo stretchy="false">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>…<!-- … --></mo><mo>,</mo><msub><mi>x</mi><mi>N</mi></msub><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\({x} = [x_1,\dots,x_N]^T\)"}</annotation></semantics></math>&nbsp;input features, in the output&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi>r</mi></mrow><mo>=</mo><mo stretchy="false">[</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mo>…<!-- … --></mo><mo>,</mo><msub><mi>r</mi><mi>N</mi></msub><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\({r} = [r_1,r_2,\dots,r_N]^T\)"}</annotation></semantics></math>&nbsp;with the help of a hidden layer (it could be many hidden layers, consider only one for now)&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi>Z</mi></mrow><mo>=</mo><mo stretchy="false">[</mo><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>Z</mi><mi>K</mi></msub><msup><mo stretchy="false">]</mo><mi>T</mi></msup><mo>=</mo><msub><mi>f</mi><mrow class="MJX-TeXAtom-ORD"><mi>θ<!-- θ --></mi></mrow></msub><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\({Z} = [Z_1,...,Z_K]^T = f_{\theta}({x})\)"}</annotation></semantics></math>.&nbsp;If we can successfully do this task, then we can extract the hidden layer&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Z</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Z\)"}</annotation></semantics></math>&nbsp;and take this as a more&nbsp;<em>compact and meaningful vector</em>&nbsp;which is able to reconstruct the exact data.</p>
<p>An Autoencoder learns to compress data from the input layer into a short code, and then uncompress that code into something that closely matches the original data. This forces the Autoencoder to engage in&nbsp;<em>dimensionality reduction</em>, for example by learning how to ignore noise.</p>
<p>As you may have guessed, the loss function of the Autoencoder should find the difference between the input and the output, since basically we are aiming for outputs that are the same as inputs. So a loss function&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">J</mi></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\mathcal{J}\)"}</annotation></semantics></math>&nbsp;computes a scalar&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">J</mi></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>r</mi></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\mathcal{J}({x},{r})\)"}</annotation></semantics></math>&nbsp;to measure how good a reconstruction&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>r</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(r\)"}</annotation></semantics></math>&nbsp;of a given input is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>x</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x\)"}</annotation></semantics></math>:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">J</mi></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>r</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>−<!-- − --></mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>N</mi></mrow></munderover><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>−<!-- − --></mo><msub><mi>r</mi><mi>n</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\mathcal{J}({x},{r}) = \sum_{n-1}^{N} (x_n - r_n)^2\)"}</annotation></semantics></math></p>
<p>Obviously we would like to minimize this error as the learning objective:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>θ<!-- θ --></mi><mo>,</mo><mi>ϕ<!-- ϕ --></mi></mrow></munder><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">J</mi></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>r</mi></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\min_{\theta,\phi} \mathcal{J}({x},{r})\)"}</annotation></semantics></math></p>
<p>Autoencoders are another way of&nbsp;<em>feature learning</em>. The solution is trivial, which means the solution or example are ridiculously simple and of little interest, unless there are constraints (such as sparsity) on the number of nodes in the hidden layers. A linear Autoencoder with&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>K</mi><mo>&lt;</mo><mi>N</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(% &lt;! K&lt;N %&gt;\)"}</annotation></semantics></math>&nbsp;acts as a PCA which we covered in weeks 3 and 4. It is non-linearity that makes it powerful.</p>
<p>If you have a look at the figure below, you can detect the layer&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>h</mi><mn>3</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h_3\)"}</annotation></semantics></math>.</p>
<p class="centerImage"><img src="../images/Autoencoder%20image%203.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Illustration of Deep Autoencoder.</h5>
<p>After the learning procedure, we can use this hidden layer of codes to reconstruct the original feature vector. However, this&nbsp;<em>hidden layer</em>&nbsp;which is an encoded version of inputs, is much smaller and more meaningful. So, rather than using the whole large feature vector (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>v</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(v\)"}</annotation></semantics></math>),&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>h</mi><mn>3</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h_3\)"}</annotation></semantics></math>&nbsp;or the encoded hidden layer could be used.</p>
<h2 id="your-task">Activity</h2>
<p>The following video tutorial further explains the concept of the deep Autoencoder. If you are interesteed, please go through it and share your understanding in the <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=1734011&amp;type=discuss&amp;rcode=DeakinUniversity-2003941" target="_top">discussion forum</a>.</p>
<div>
<p class="centerVideo"><iframe width="648" height="364" src="https://www.youtube.com/embed/H1AllrJ-_30?wmode=opaque" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></p>
<h5>This is an additional video, hosted on YouTube.</h5>
</div>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>