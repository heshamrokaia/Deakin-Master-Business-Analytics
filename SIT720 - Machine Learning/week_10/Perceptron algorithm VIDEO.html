<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<link rel="stylesheet" href="https://s.brightspace.com/lib/fonts/0.5.0/fonts.css"><script>function lti_launch( vars, target ) {
						var query = '';
						var new_tab = false;

						for(var key in vars) {
							if(query.length == 0) {
								query += '?' + key + '=' + encodeURIComponent(vars[key]);
							}
							else {
								query += '&' + key + '=' + encodeURIComponent(vars[key]);
							}
						}

						var url = '/d2l/customization/pearsonlti/6605/Launch' + query;(target == '_blank') ? window.open( url, '_blank' ) : location.replace( url );}</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/embeds.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					window.D2L.EmbedRenderer.renderEmbeds(document.body);
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/mathjax.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					if (document.querySelector('math') || /\$\$|\\\(|\\\[|\\begin{|\\ref{|\\eqref{/.test(document.body.innerHTML)) {
						document.querySelectorAll('mspace[linebreak="newline"]').forEach(elm => {
							elm.setAttribute('style', 'display: block; height: 0.5rem;');
						});

						window.D2L.MathJax.loadMathJax({
							outputScale: 1.5,
							renderLatex: true,
							enableMML3Support: false
						});
					}
				});</script><script src="https://s.brightspace.com/lib/bsi/2024.6.211/unbundled/prism.js?v=20.24.6.19120" type="module"></script><script>document.addEventListener('DOMContentLoaded', function() {
					document.querySelectorAll('.d2l-code').forEach(code => {
						window.D2L.Prism.formatCodeElement(code);
					});
				});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.documentElement.hasAttribute('lang')) return;
						document.documentElement.setAttribute('lang', 'en-GB'); 						
					});</script><script>document.addEventListener('DOMContentLoaded', function() {
						if (document.head.querySelector('title')) return;
						var handleAppendTitle = function(evt) {
							if (!evt || !evt.data) return;

							try {
								var data = JSON.parse(evt.data);
								if (data.handler !== 'd2l.iframe.requestPageTitle' || !data.pageTitle) return;

								window.removeEventListener('message', handleAppendTitle, false);

								var titleElm = document.createElement('title');
								titleElm.textContent = data.pageTitle;
								document.head.appendChild(titleElm);
							} catch (e) {}	
						};

						window.addEventListener('message', handleAppendTitle, false);
						window.parent.postMessage(JSON.stringify({ handler: 'd2l.iframe.requestPageTitle' }), '*');
					});</script><script>window.addEventListener('message', function(event) { 
					if( !event.data ) {
						return;
					}

					var params;
					try {
						params = JSON.parse( event.data );
					}
					catch {
						return;
					}
					if( !params.subject || params.subject !== 'lti.frameResize' ) {
						return;
					}

					const MAX_FRAME_HEIGHT = 10000
					if( !params.height || params.height < 1 || params.height > MAX_FRAME_HEIGHT ) {
						console.warn( 'Invalid height value received, aborting' );
						return;
					}
					var el = document.getElementsByTagName( 'iframe' );
					for ( var i=0; i < el.length; i++ ) {
						if( el[i].contentWindow === event.source ) {
							el[i].style.height = params.height + 'px';
							el[i].style.width = '100%';
							console.info( 'Setting iFrame height to ' + params.height );
							console.info( 'Setting iFrame width to 100%' );
						}
					}
				});</script></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;" data-new-gr-c-s-check-loaded="8.912.0" data-gr-ext-installed=""><div>
<h1>Perceptron algorithm</h1>
</div>
<div>
<p>Perceptron is a linear classifier (binary) and is a&nbsp;<em>single layer</em>&nbsp;neural network. A multi-layer perceptron is called a neural network.</p>
<p>The following figure is a simple illustration of a perceptron.</p>
<p class="centerImage"><img src="../images/Perceptron%20algorithm%20image%201.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><br>Figure. Illustration of perceptron structure</h5>
<p>As you can see we are dealing with&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>…<!-- … --></mo><mo>,</mo><msub><mi>x</mi><mi>M</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_1,\dots,x_M\)"}</annotation></semantics></math>&nbsp;features with&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>…<!-- … --></mo><mo>,</mo><msub><mi>w</mi><mi>M</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_1,\dots,w_M\)"}</annotation></semantics></math>&nbsp;corresponding weights. Like regression or many other machine learning problems, we do have a bias term too&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mn>0</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_0\)"}</annotation></semantics></math>.&nbsp;&nbsp;A&nbsp;<em>sum function</em>&nbsp;will calculate a value, which later will be presented as the output&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y}\)"}</annotation></semantics></math>.&nbsp;</p>
<p>Given input vector&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>…<!-- … --></mo><mo>,</mo><msub><mi>x</mi><mi>M</mi></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\({x} = (x_1,\dots,x_M)\)"}</annotation></semantics></math>&nbsp;of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>M</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(M\)"}</annotation></semantics></math>&nbsp;dimensions and weight vector&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo>=</mo><mo stretchy="false">(</mo><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mo>…<!-- … --></mo><mo>,</mo><msub><mi>w</mi><mi>M</mi></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\({w} = (w_0,\dots,w_M)\)"}</annotation></semantics></math>.&nbsp;The perceptron produces output of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>n</mi><mo stretchy="false">[</mo><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y} = sign[v({x},{w})]\)"}</annotation></semantics></math>&nbsp;where&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(v({x}, {w})\)"}</annotation></semantics></math>&nbsp;is the linear combiner of:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mtr><mtd></mtd></mtr><mtr><mtd><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></munderover><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>w</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd></mtd></mtr></mtable></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\begin{align} \\ v({x},{w}) = \sum_{i=1}^{M}w_ix_i+w_0\\ \\ \end{align}\)"}</annotation></semantics></math></p>
<p>Also there is a better matrix notation, let&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_0 = 1\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>…<!-- … --></mo><mo>,</mo><msub><mi>x</mi><mi>M</mi></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\({x} = (x_1,\dots,x_M)\)"}</annotation></semantics></math>&nbsp;then:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mtr><mtd></mtd></mtr><mtr><mtd><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>=</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow></mtd></mtr><mtr><mtd></mtd></mtr><mtr><mtd><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>n</mi><mo stretchy="false">[</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd></mtd></mtr></mtable></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\begin{align} \\ v({x},{w}) = {w}^T{x} = {x}^T{w}\\ \\ \hat{y}({x},{w}) = sign[{w}^T{x}]\\ \\ \end{align}\)"}</annotation></semantics></math></p>
<p>In summary, a perceptron is a simple neural network used for binary classification. It has only one layer with a single node.</p>
<p>Now lets visualize a&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>2</mn><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(2-\)"}</annotation></semantics></math>dimensional example. In the following figure we are explaining the problem that the preceptron tries to solve.</p>
<p class="centerImage"><img src="../images/Perceptron%20algorithm%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><br>Figure. Illustration of perceptron problem.</h5>
<p>Given weight&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>w</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w\)"}</annotation></semantics></math>,&nbsp;the perceptron linearly divides input space into two regions:</p>
<ul>
<li>All&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>x</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x\)"}</annotation></semantics></math>'s such that&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y}({x},{w}) = 1\)"}</annotation></semantics></math>&nbsp;or&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo>≥<!-- ≥ --></mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(v({x},{w}) \geq 0\)"}</annotation></semantics></math>(the above region of the hyperplane line)</li>
<li><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">All&nbsp;</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>x</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">'s such that</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><mo>−<!-- − --></mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y}({x},{w}) = -1\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;or&nbsp;</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo>&lt;</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(v({x},{w}) &lt; 0 %\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;(the below region of the hyperplane line)</span></li>
</ul>
<p>This corresponds to the&nbsp;<em>two sides of the hyperplane</em>&nbsp;defined by the equation:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mtr><mtd></mtd></mtr><mtr><mtd><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></munderover><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd></mtd></mtr></mtable></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\begin{align} \\ v({x},{w}) = \sum_{i=1}^{M}w_ix_i+w_0 = 0\\ \\ \end{align}\)"}</annotation></semantics></math></p>
<p>Note that&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo fence="false" stretchy="false">|</mo><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo fence="false" stretchy="false">|</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\vert v({x},{w})\vert\)"}</annotation></semantics></math>&nbsp;is proportional to the distance from&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>x</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x\)"}</annotation></semantics></math>&nbsp;to the hyperplane or how far a point is from the hyperplane line. If it is close to the boundary hyperplane (or in this case the line), we are probably less confident about the decision we are making. But if it is far enough from the line, we are more confident. We call the hyperplane&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">H</mi></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\mathcal{H}({w})\)"}</annotation></semantics></math>,&nbsp;so we can define the distance of a point from the hyperplane as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mtr><mtd></mtd></mtr><mtr><mtd><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">H</mi></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mtd><mtd><mi></mi><mo>=</mo><mfrac><mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></munderover><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>w</mi><mn>0</mn></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow><msqrt><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></munderover><msubsup><mi>w</mi><mi>i</mi><mn>2</mn></msubsup></msqrt></mfrac></mtd></mtr><mtr><mtd></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><mo>=</mo><mfrac><mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow><msqrt><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></munderover><msubsup><mi>w</mi><mi>i</mi><mn>2</mn></msubsup></msqrt></mfrac></mtd></mtr><mtr><mtd></mtd></mtr></mtable></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\begin{align} \\ dist({x},\mathcal{H}({w})) &amp; = \frac{|\sum_{i=1}^{M}w_ix_i+w_0|}{\sqrt{\sum_{i=1}^{M}w_i^2}} \\ \\ &amp; = \frac{|v({x},{w})|}{\sqrt{\sum_{i=1}^{M}w_i^2}}\\ \\ \end{align} %\)"}</annotation></semantics></math></p>
<p>Which is the perpendicular distance of the point to the line (see the figure below).</p>
<p>Which is the perpendicular distance of the point to the line (see the figure below).</p>
<p class="centerImage"><img src="../images/Perceptron%20algorithm%20image%203.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><br>Figure. Distance of a point&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>x</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x\)"}</annotation></semantics></math>&nbsp;to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">H</mi></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\mathcal{H}({w})\)"}</annotation></semantics></math></h5>
<p>Thus the sign of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(v({x},{w})\)"}</annotation></semantics></math>&nbsp;indicates on which side of hyperplane&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">H</mi></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\mathcal{H}({w})\)"}</annotation></semantics></math>&nbsp;is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>x</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x\)"}</annotation></semantics></math>.&nbsp;&nbsp;While the magnitude&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo fence="false" stretchy="false">|</mo><mi>v</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo><mo fence="false" stretchy="false">|</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\vert v({x},{w})\vert\)"}</annotation></semantics></math>&nbsp;indicates how far away&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>x</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x\)"}</annotation></semantics></math>&nbsp;is from&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">H</mi></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\mathcal{H}({x},{w})\)"}</annotation></semantics></math>.</p>
<p>Now, let us remind you the definition of linearly separable, if you have a look at the next figure (below), two sets&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>C</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C_1\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>C</mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C_2\)"}</annotation></semantics></math>&nbsp;are called linearly separable if there exists a hyperplane&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">H</mi></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\mathcal{H}({x},{w})\)"}</annotation></semantics></math>&nbsp;</p>
<p>that separates them.</p>
<p class="centerImage"><img src="../images/Perceptron%20algorithm%20image%204.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><br>Figure. Linearly separable.</h5>
<p>Thus, in a perceptron we would like to find the weight vector&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>w</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w\)"}</annotation></semantics></math>&nbsp;so that the perceptron correctly classify&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>2</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(2\)"}</annotation></semantics></math>&nbsp;classes, given sample training data.</p>
<p>Training data&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>D</mi><mo>=</mo><mo fence="false" stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo fence="false" stretchy="false">}</mo><mo>,</mo><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mo>…<!-- … --></mo><mo>,</mo><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(D = \{({x}_t,y_t)\},\ \ t=1,\dots,n\)"}</annotation></semantics></math>&nbsp;where<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mi>t</mi></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mn>1</mn></mrow></msub><mo>,</mo><mo>…<!-- … --></mo><mo>,</mo><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>M</mi></mrow></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\({x}_t = (x_{t1},\dots,x_{tM})\)"}</annotation></semantics></math>&nbsp;is the input vector at time&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>t</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(t\)"}</annotation></semantics></math>&nbsp;and<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mo>+</mo><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mo>−<!-- − --></mo><mo stretchy="false">)</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_t = (+/-) 1\)"}</annotation></semantics></math>&nbsp;is the desired output (see figure below).</p>
<p>It appears to be a simple classification problem. For now we will assume that the data is linearly separable.</p>
<p class="centerImage"><img src="../images/Perceptron%20algorithm%20image%205.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><br>Figure. An example of perceptron problem.</h5>
<p>Now, let’s see the perceptron algorithm for solving this problem. The steps of the algorithm are as follows:</p>
<ol>
<li>initialize&nbsp;&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo>=</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\({w} = 0\)"}</annotation></semantics></math></li>
<li>Retrieve next input&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mi>t</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_t\)"}</annotation></semantics></math>and desired output&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>t</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_t\)"}</annotation></semantics></math></li>
</ol>
<ul>
<li><span style="font-size: 15.2px;">Compute actual output&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mi>t</mi></msub><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>n</mi><mo stretchy="false">[</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mi>t</mi></msub><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y}_t = sign[{x}_t,{w}]\)"}</annotation></semantics></math>&nbsp;</span></li>
<li><span style="font-size: 15.2px;">Compute output error&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>e</mi><mo>=</mo><msub><mi>y</mi><mi>t</mi></msub><mo>−<!-- − --></mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mi>t</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(e = y_t - \hat{y}_t\)"}</annotation></semantics></math></span></li>
<li><span style="font-size: 15.2px;">Update weight, for all&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">←<!-- ← --></mo><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mi mathvariant="normal">Δ<!-- Δ --></mi><msub><mi>w</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_i \leftarrow w_i +\Delta w_i\)"}</annotation></semantics></math>, with&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi mathvariant="normal">Δ<!-- Δ --></mi><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mi>η<!-- η --></mi><msub><mi>e</mi><mi>t</mi></msub><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>i</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\Delta w_i = \eta e_t x_{ti}\)"}</annotation></semantics></math>. Note that&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn><mo>≤<!-- ≤ --></mo><mi>η<!-- η --></mi><mo>≤<!-- ≤ --></mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0 \leq \eta \leq 1\)"}</annotation></semantics></math>&nbsp;</span>is the&nbsp;<em>learning rate</em>.</li>
<li><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">Repeat from step 2&nbsp; until convergence</span></li>
</ul>
<p>It visits each data point in a loop and at each iteration it updates the&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>w</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w\)"}</annotation></semantics></math>.&nbsp; But what if&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>w</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w\)"}</annotation></semantics></math>&nbsp;converges to some values. Then no update if&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">^<!-- ^ --></mo></mover></mrow><mi>t</mi></msub><mo>=</mo><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y}_t = y_{t}\)"}</annotation></semantics></math>,&nbsp;e.g., the current weight&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>w</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w\)"}</annotation></semantics></math>&nbsp;already correctly classify current sample&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\({x}(t)\)"}</annotation></semantics></math>.&nbsp;Once again let us remind you that if training instances are drawn from two&nbsp;<em>linearly separate</em>&nbsp;sets&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>C</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C_1\)"}</annotation></semantics></math>&nbsp;and<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>C</mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C_2\)"}</annotation></semantics></math>,&nbsp;&nbsp;then the perceptron learning rule will&nbsp;<em>converge</em>&nbsp;after finite iterations. However, there is no guarantee for convergence if&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>C</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C_1\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>C</mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C_2\)"}</annotation></semantics></math>&nbsp;are not linearly separable!</p>
<p class="centerImage"><img src="../images/Perceptron%20algorithm%20image%206.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><br>Figure. Linearly separable.</h5>
<p align="center"><iframe src="https://deakin.au.panopto.com/Panopto/Pages/Embed.aspx?id=2d9d2901-8f1e-49a5-9285-afe70122a04b&amp;autoplay=false&amp;offerviewer=true&amp;showtitle=true&amp;showbrand=true&amp;captions=true&amp;interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen="" allow="autoplay" aria-label="Panopto Embedded Video Player"></iframe></p>
<table class="tableClear" style="width: 880px;">
<tbody>
<tr>
<td style="width: 59px;"></td>
<td style="width: 466.433px;"><a id="viewTranscript">View transcript</a></td>
<td style="width: 271.567px;"></td>
</tr>
</tbody>
</table>
<article class="js-transcript transcript" id="transcript-en">
<p class="transcript__para">SPEAKER 1: In this tutorial we're going to see an example of learning perceptron. Perceptron is a linear classifier and it's also used in supervised learning. Perceptron is usually used to classify the data into two parts, therefore it's also known as linear binary classifier. Let's see the example. Consider the current weights as -1, 2, 1. So the hyperplane with these weights-- which we could come up with that initially-- is something like this, which is 2x1 plus x2 minus 1, which is based on these weights. Now a new point comes in, and it lies in here. And right now we need to update this line.</p>
<p class="transcript__para">So the first thing to do is finding the weight of x and w, which we put this point into this hyperplane, which is 2 times 1 over 2, plus 1 times 1 minus 1, which is equal to 1. Again, for finding the y-hat x and w, we're using the sine function, and the sine of 1 is 1. So this is what we approximated for this point, and this is the real value of the output for this point. So the difference of our approximation with the real value is -2, which is -1 minus 1, which equals to -2. Now based on this, we're going to update the weights. So wi equals wi minus 2, eta xi.</p>
<p class="transcript__para">If we let the eta to be 1, this formula yields the yi minus 2xi. So we should find each weight-- w0, wy, and w2-- based on this formula. So for w0 it's -1 minus 2. X0 is 1, so it will result in -3. w1 is 2 minus 2x1, and x1 is, as you can see, 1 over 2, so it's 1. w2 equals 1 minus 2x2, and x2 is 1, so it would result in -1. So here is our new hyperplane based on the new training point. So this was an example of learning a perceptron. As can see, by adding a new training point we are updating the hyperplane, and we are finding a new hyperplane.</p>
<p class="transcript__para">So we should keep doing this for every single point in our training sample so we can finally come up with a hyperplane or line.</p>
</article>
<h2 id="your-task">Activity</h2>
<p><em>Neuroevolution of augmenting topologies</em>&nbsp;(NEAT) is one of the most interesting Neural-Network based methods. Check out the performance of a NEAT algorithm on one of the most popular games in the world!</p>
<p></p>
<div>
<p class="centerVideo"><iframe src="https://www.youtube.com/embed/qv6UVOQ0F44?wmode=opaque" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen" width="648" height="364" frameborder="0"></iframe></p>
<h5>This is an additional video, hosted on YouTube.</h5>
</div>
<span face="Lato, sans-serif" style="font-family: Lato, sans-serif;"><span style="font-size: 15.2px;"><br></span></span></div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>