<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

</head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Curse%20of%20Dimensionality%20image%201.jpg" alt="Water chain buoysMore images from Capri island" title="Water chain buoysMore images from Capri island" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Curse of Dimensionality</h1>
</div>
<div>
<p>The Curse of Dimensionality arises when applying machine learning algorithms to highly-dimensional data.</p>
<p>In machine learning we face unique problems when analysing and organising data in&nbsp;<em>high-dimensional spaces</em>. When the dimensionality increases, the&nbsp;<em>volume of the space increases</em>&nbsp;so fast that the available data become&nbsp;<em>sparse</em>. This is really problematic since there isn’t enough data locally.</p>
<p class="centerImage"><img src="../images/Curse%20of%20Dimensionality%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5 id="figure-1d-2d-3d-space-data">Figure. 1D, 2D, 3D space data</h5>
<p>Consider the figure above, first we observed some points in 1D data in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>4</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(4\)"}</annotation></semantics></math>&nbsp;regions (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>20</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(20\)"}</annotation></semantics></math>&nbsp;divided by&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>5</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(5\)"}</annotation></semantics></math>).&nbsp;Then these points are transferred into 2D space, into a&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>16</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(16\)"}</annotation></semantics></math>&nbsp;region space. In the next step the points are in a 3D space with&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>64</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(64\)"}</annotation></semantics></math>&nbsp;regions. What would happen when we get to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>100</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(100\)"}</annotation></semantics></math>&nbsp;</p>
<p>dimensions?</p>
<p>At its core, the&nbsp;<em>curse of dimensionality</em>, dictates that as the number of dimensions increases, the number of regions grows&nbsp;<em>exponentially</em>. As the number of regions grows and space increases each data point has more and more room. That makes our data sparse and somehow not useful anymore. For example, what if we would like to check on a neighbour of a data point? Clearly, there wouldn’t be&nbsp;<em>neighbours</em>&nbsp;nearby!</p>
<p>Think about describing any human being. If we use number of arms or legs it includes many other animals. So we need to add another dimension: an upright body posture will differentiate us from many animals. Must breath air will exclude a few. What if we want to differentiate one human from another human? Are these features enough? No! We have to add many more dimensions such as height, weight, skin colour, hair colour, hair type and many more. Now imagine we want to identify each individual on earth. How many descriptors would need to be added? Can we handle such complexity and the computational load of enough dimensions to define a human being?</p>
<p>This example shows why we need to add more dimensions to describe any object, how quickly the dimensionality increases and how complex the system grows.</p>
<p>Also some intuitions drawn from low dimensional spaces fail badly in high dimensions.</p>
<h4 id="example">Example</h4>
<p>In high dimensions, most of the volume of the unit sphere is very close to its surface.</p>
<p class="centerImage"><img src="../images/Curse%20of%20Dimensionality%20image%203.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5 id="figure-unit-sphere">Figure. Unit sphere</h5>
<p>Now, consider a sphere with radius&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>r</mi><mo>=</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(r=1\)"}</annotation></semantics></math>&nbsp;in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d-\)"}</annotation></semantics></math>dimensional space. Let’s compute&nbsp;<em>the fraction of the volume</em>&nbsp;that is between<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>r</mi><mo>=</mo><mn>1</mn><mo>−<!-- − --></mo><mi>ϵ<!-- ϵ --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(r=1-\epsilon\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>r</mi><mo>=</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(r=1\)"}</annotation></semantics></math>.&nbsp;&nbsp;</p>
<p>The volume of a sphere in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d-\)"}</annotation></semantics></math>dimension is given by&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>v</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>k</mi><mi>d</mi></msub><msup><mi>r</mi><mi>d</mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(v_d(r) = k_dr^d\)"}</annotation></semantics></math>.</p>
<p>So the desired fraction is:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mfrac><mrow><msub><mi>v</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−<!-- − --></mo><msub><mi>v</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>−<!-- − --></mo><mi>ϵ<!-- ϵ --></mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>v</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mn>1</mn><mo>−<!-- − --></mo><mo stretchy="false">(</mo><mn>1</mn><mo>−<!-- − --></mo><mi>ϵ<!-- ϵ --></mi><msup><mo stretchy="false">)</mo><mi>d</mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\frac{v_d(1) - v_d(1-\epsilon)}{v_d(1)} = 1-(1-\epsilon)^d"}</annotation></semantics></math></p>
<p>Now let’s extend our dimensions into higher numbers.</p>
<p>You can see from the following figure, the exponential growth of the fraction of volume based on&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>ϵ<!-- ϵ --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\epsilon\)"}</annotation></semantics></math>,&nbsp;which is a fairly small constant.</p>
<p class="centerImage"><img src="../images/Curse%20of%20Dimensionality%20image%204.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5 id="figure-simple-illustration-of-curse-of-dimensionality">Figure. Simple illustration of curse of dimensionality.</h5>
<p>This shows that the volume of the hypersphere tends towards zero and fraction of volume&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mfrac><mrow><msub><mi>v</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−<!-- − --></mo><msub><mi>v</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>−<!-- − --></mo><mi>ϵ<!-- ϵ --></mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>v</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\frac{v_d(1) - v_d(1-\epsilon)}{v_d(1)}\)"}</annotation></semantics></math>&nbsp;tends to grow to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>100</mn><mi mathvariant="normal">%<!-- % --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(100 \%\)"}</annotation></semantics></math>&nbsp;of the volume as the dimensionality tends to infinity, whereas the volume of the surrounding hypercube remains constant.</p>
<p>This surprising and rather counter-intuitive observation partially explains the problems associated with the curse of dimensionality in classification:</p>
<blockquote>
<p>In high dimensional spaces, most of the training data resides in the&nbsp;<strong>corners of the hypercube</strong>&nbsp;defining the feature space.</p>
</blockquote>
<p>Also it is obvious that the&nbsp;<em>curse of dimensionality</em>&nbsp;result in less distinctive distances in in high dimensions. So given a point in high dimensions, the relative distance between points far from it and close from it, becomes negligible. The following figure illustrates this point.</p>
<p class="centerImage"><img src="../images/Curse%20of%20Dimensionality%20image%205.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5 id="figure-euclidean-distance-was-used-to-generate-this-graph-similar-results-hold-for-other-distances-in-terms-of-distinctiveness">Figure. Euclidean distance was used to generate this graph, similar results hold for other distances in terms of distinctiveness.</h5>
<p>There is an analysis regarding this problem; it has been called the&nbsp;<em>concentration effect</em>.</p>
<h3 id="concentration-effect">Concentration effect</h3>
<p>Let us assume the ratio of the variance of the length of any point vector (denoted by&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo fence="false" stretchy="false">∥<!-- ∥ --></mo><mo>∥<!-- ∥ --></mo><msub><mi>X</mi><mi>d</mi></msub><mo>∥<!-- ∥ --></mo><mo fence="false" stretchy="false">∥<!-- ∥ --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\|\|X_d\|\|\)"}</annotation></semantics></math>&nbsp;with the length of the mean point vector (denoted by&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>E</mi><mo stretchy="false">[</mo><mo>∥<!-- ∥ --></mo><msub><mi>X</mi><mi>d</mi></msub><mo>∥<!-- ∥ --></mo><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E[\|X_d\|]\)"}</annotation></semantics></math>)&nbsp;converges to zero with increasing data dimensionality.</p>
<p>The proportional difference between the farthest-point distance&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(D_{max}\)"}</annotation></semantics></math>&nbsp;and the closest-point distance&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(D_{min}\)"}</annotation></semantics></math>&nbsp;(the relative contrast) vanishes:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>i</mi><mi>f</mi><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi>l</mi><mi>i</mi><msub><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo stretchy="false">→<!-- → --></mo><mi mathvariant="normal">∞<!-- ∞ --></mi></mrow></msub><mi>v</mi><mi>a</mi><mi>r</mi><mo stretchy="false">(</mo><mfrac><mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mi>X</mi><mi>d</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow><mrow><mi>E</mi><mo stretchy="false">[</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mi>X</mi><mi>d</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mo stretchy="false">]</mo></mrow></mfrac><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"if\ \ lim_{d\rightarrow \infty} var(\frac{||X_d||}{E[||X_d||]}) = 0"}</annotation></semantics></math></p>
<p>Then we can say:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mfrac><mrow><msub><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><mo>−<!-- − --></mo><msub><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><msub><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mfrac><mo stretchy="false">→<!-- → --></mo><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\frac{D_{max} - D_{min}}{D_{min}} \rightarrow 0"}</annotation></semantics></math></p>
<p>So it reduces the utility of the measure to discriminate between near and far neighbours. Relative contrast between near and far neighbours diminishes as the dimensionality increases. This is known as the&nbsp;<em>concentration effect</em>&nbsp;of the distance measure.</p>
<p>This problem can imply that:</p>
<ul>
<li>Clustering or KNN algorithms may be&nbsp;<em>meaningless</em>&nbsp;in high dimensions. However, there might still be patterns in high dimensions. We just need better distance metrics. So Research is needed!</li>
<li>Until we develop better distance metrics, we should aim to reduce the dimensionality where possible.</li>
</ul>
<h2 id="your-task">Activity</h2>
<p></p>
<div><iframe width="648" height="365" src="https://www.youtube.com/embed/JMmuVyDZ_XA?wmode=opaque" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe>
<p>This is an additional video, hosted on YouTube.</p>
</div>
<p>Discuss your understanding of the topic and why high dimensional data is problematic for machine learning.</p>
<p>Share your response in the <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=1734011&amp;type=discuss&amp;rcode=DeakinUniv-19277" target="_blank" rel="noopener">discussion forum</a>.</p>
<p></p>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>