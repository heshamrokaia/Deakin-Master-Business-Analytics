<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<link rel="stylesheet" href="https://s.brightspace.com/lib/fonts/0.5.0/fonts.css"></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Formulation%20of%20PCA%20and%20deriving%20principal%20components%20image%201.jpg" alt="Modern architecture Abstract shapes from a modern architecture building." title="Modern architecture Abstract shapes from a modern architecture building." style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Formulation of PCA and deriving principal components</h1>
</div>
<div>
<p>Let us say that we first project the data on&nbsp;<em>a new axis</em>, whose direction is specified by a&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>−<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d-\)"}</annotation></semantics></math>&nbsp;dimensional vector&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_1\)"}</annotation></semantics></math>.</p>
<p>Since we are only interested in direction of maximum variance, we assume&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_1\)"}</annotation></semantics></math>&nbsp;to be a&nbsp;<em>unit length vector</em>, i.e.&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo fence="false" stretchy="false">∥<!-- ∥ --></mo><msub><mi>u</mi><mn>1</mn></msub><mo>∥<!-- ∥ --></mo><mo>=</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\|u_1\| = 1\)"}</annotation></semantics></math>, or&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><msub><mi>u</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_1^Tu_1 = 1\)"}</annotation></semantics></math>.</p>
<p>Now each data point&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_i\)"}</annotation></semantics></math>&nbsp;can be projected on the vector<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_i\)"}</annotation></semantics></math>&nbsp;to create a new co-ordinate as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mn>1</mn></mrow></msub><mo>=</mo><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo stretchy="false">¯<!-- ¯ --></mo></mover></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_{i1} = u_1^T\bar{x}\)"}</annotation></semantics></math>.&nbsp;So the variance of the data projected on&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_i\)"}</annotation></semantics></math>&nbsp;is:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">(</mo></mrow><mfrac><mn>1</mn><mrow><mi>n</mi><mo>−<!-- − --></mo><mn>1</mn></mrow></mfrac><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">)</mo></mrow><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mo stretchy="false">(</mo><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>−<!-- − --></mo><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo stretchy="false">¯<!-- ¯ --></mo></mover></mrow><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\Big( \frac{1}{n-1}\Big) \sum_{i=1}^{n} (u_1^Tx_i - u_1^T\bar{x})^2"}</annotation></semantics></math></p>
<p>As we have shown before, the mean of the new data is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">¯<!-- ¯ --></mo></mover></mrow><mo>=</mo><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo stretchy="false">¯<!-- ¯ --></mo></mover></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\bar{y} = u_1^T\bar{x}\)"}</annotation></semantics></math>&nbsp;and the variance of the projected data is:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">(</mo></mrow><mfrac><mn>1</mn><mrow><mi>n</mi><mo>−<!-- − --></mo><mn>1</mn></mrow></mfrac><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">)</mo></mrow><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mo stretchy="false">(</mo><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>−<!-- − --></mo><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo stretchy="false">¯<!-- ¯ --></mo></mover></mrow><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\Big( \frac{1}{n-1}\Big) \sum_{i=1}^{n} (u_1^Tx_i - u_1^T\bar{x})^2"}</annotation></semantics></math></p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mo>=</mo><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">[</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">(</mo></mrow><mfrac><mn>1</mn><mrow><mi>n</mi><mo>−<!-- − --></mo><mn>1</mn></mrow></mfrac><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">)</mo></mrow><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−<!-- − --></mo><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo stretchy="false">¯<!-- ¯ --></mo></mover></mrow><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−<!-- − --></mo><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo stretchy="false">¯<!-- ¯ --></mo></mover></mrow><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">]</mo></mrow><msub><mi>u</mi><mn>1</mn></msub><mo>=</mo><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><mi>C</mi><msub><mi>u</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"= u_1^T \Big[ \Big( \frac{1}{n-1}\Big) \sum_{i=1}^{n} (x_i - \bar{x})(x_i - \bar{x})^T \Big]u_1 = u_1^TCu_1"}</annotation></semantics></math></p>
<p>Now as we know we would like to&nbsp;<em>find out the direction</em>&nbsp;so that the variance&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><mi>C</mi><msub><mi>u</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_1^TCu_1\)"}</annotation></semantics></math>&nbsp;is&nbsp;<em>maximised</em>. Recall that we also assume&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><msub><mi>u</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_1^Tu_1 = 1\)"}</annotation></semantics></math>.&nbsp;&nbsp;By putting it together we want to find:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mrow><mo>{</mo><mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mtr><mtd><munder><mo movablelimits="true" form="prefix">max</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow></munder><mtext>&nbsp;</mtext><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><mi>C</mi><msub><mi>u</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><mi>s</mi><mo>.</mo><mi>t</mi><mo>.</mo><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><msub><mi>u</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn></mtd></mtr></mtable><mo fence="true" stretchy="true"></mo></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\begin{cases} \max_{x} \ u_1^TCu_1\\ s.t.\ \ \ u_1^Tu_1 = 1 \end{cases}"}</annotation></semantics></math></p>
<p>For solving this problem we introduce a&nbsp;<em>Lagrange multiplier</em>&nbsp;and change the problem into an unconstrained maximisation problem:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><munder><mo movablelimits="true" form="prefix">max</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow></munder><mtext>&nbsp;</mtext><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><mi>C</mi><msub><mi>u</mi><mn>1</mn></msub><mo>+</mo><msub><mi>λ<!-- λ --></mi><mn>1</mn></msub><mo stretchy="false">(</mo><mn>1</mn><mo>−<!-- − --></mo><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><msub><mi>u</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\max_{x} \ u_1^TCu_1 + \lambda_1(1-u_1^Tu_1)"}</annotation></semantics></math></p>
<p>If you want to find maximums or minimums a good way to get started is to find out where the slope of the function (derivative) is equal to zero. Taking derivative w.r.t.&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_i\)"}</annotation></semantics></math>&nbsp;and setting it to zero we obtain:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>C</mi><msub><mi>u</mi><mn>1</mn></msub><mo>=</mo><msub><mi>λ<!-- λ --></mi><mn>1</mn></msub><msub><mi>u</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"Cu_1 = \lambda_1u_1"}</annotation></semantics></math></p>
<p>This is an eigenvalue problem, where&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_1\)"}</annotation></semantics></math>&nbsp;is the largest eigenvalue of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_i\)"}</annotation></semantics></math>&nbsp;is the corresponding eigenvector.&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_i\)"}</annotation></semantics></math>&nbsp;is known as&nbsp;<em>the first principal component</em>.</p>
<p>Now what about&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>u</mi><mi>d</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_2,...,u_d\)"}</annotation></semantics></math>?</p>
<ul>
<li>Next set of axes&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>u</mi><mi>d</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_2,...,u_d\)"}</annotation></semantics></math>&nbsp;can be found incrementally by finding a direction that&nbsp;<em>maximizes the variance</em>&nbsp;and is&nbsp;<em>orthogonal to all the principal axes</em>&nbsp;found so far.</li>
<li>The directions have to be orthogonal since we want them to be uncorrelated (Why uncorrelated?).</li>
<li>Therefore, the principal axes can be collectively written using the Eigenvector matrix&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>U</mi><mo>=</mo><mo stretchy="false">[</mo><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>u</mi><mi>d</mi></msub><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(U = [u_1,u_2,...,u_d]\)"}</annotation></semantics></math>&nbsp;n the order of&nbsp;<em>decreasing eigenvalues</em>&nbsp;of the covariance matrix&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>.&nbsp;</li>
</ul>
<span style="font-family: Lato, sans-serif; font-size: 0.95rem;"><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">The question which arises here is what if we project the data using all&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\)"}</annotation></semantics></math>&nbsp;principal components? Well in this case we just doing de-correlation but no dimensionality reduction. However, if we&nbsp;<em>project data on only top&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k\)"}</annotation></semantics></math>&nbsp;principal components&nbsp;such that&nbsp;&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi><mo>≤<!-- ≤ --></mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k \leq d\)"}</annotation></semantics></math>&nbsp;</em></span></span>we achieve&nbsp;<em>dimensionality reduction</em>&nbsp;and each new dimension is also&nbsp;<em>uncorrelated</em>&nbsp;of other dimensions.
<h3 id="pca-via-eigen-value-decomposition">PCA via Eigen Value Decomposition</h3>
<p>Now we’ll see how to perform PCA with eigenvalue decomposition. It is fairly easy:</p>
<ol>
<li>Compute data covariance matrix&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C\)"}</annotation></semantics></math>&nbsp;</li>
<li>Perform Eigen value decomposition&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo stretchy="false">(</mo><mi>E</mi><mi>V</mi><mi>D</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\((EVD)\)"}</annotation></semantics></math>&nbsp;as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi><mo>=</mo><mi>U</mi><mi>D</mi><msup><mi>U</mi><mi>T</mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C = UDU^T\)"}</annotation></semantics></math></li>
<li>Reduced dimension data is given by:</li>
</ol>
<h5><img src="../images/Formulation%20of%20PCA%20and%20deriving%20principal%20components%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></h5>
<h5></h5>
<p>So as you can see,&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Y\)"}</annotation></semantics></math>&nbsp;which is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi><mo>×<!-- × --></mo><mi>k</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\times k\)"}</annotation></semantics></math>&nbsp;matrix, is the reduced dimension data from&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\)"}</annotation></semantics></math>&nbsp;dimensions to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k\)"}</annotation></semantics></math>&nbsp;dimension.&nbsp;And we will achieve this by multiplying&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>X</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(X\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>U</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(U\)"}</annotation></semantics></math>:&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>X</mi><mrow class="MJX-TeXAtom-ORD"><mi>n</mi><mo>×<!-- × --></mo><mi>d</mi></mrow></msup><mtext>&nbsp;</mtext><msup><mi>U</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>×<!-- × --></mo><mi>k</mi></mrow></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(X^{n\times d} \ U^{d\times k}\)"}</annotation></semantics></math>&nbsp;which will result in top&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k\)"}</annotation></semantics></math>&nbsp;Eigen vectors in the decreasing order of eigenvalues.</p>
<h3 id="pca-minimum-error-formulation">PCA: Minimum Error Formulation</h3>
<p>We would like to analyse PCA from another perspective. This is an alternative formulation of PCA based on&nbsp;<em>projection error minimization</em>. Suppose we project our data on&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k\)"}</annotation></semantics></math>&nbsp;dimensions from&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\)"}</annotation></semantics></math>&nbsp;dimensions.&nbsp;Obviously losses incurred due to losing some features in data (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi><mo>&lt;</mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k &lt; d\)"}</annotation></semantics></math>).&nbsp;But the error we have while using PCA’s best&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k\)"}</annotation></semantics></math>&nbsp;</p>
<p>dimensions in terms of least square error, is the minimum possible error we can have.</p>
<ul>
<li>Let us consider a set of new axes&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>u</mi><mi>d</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_1,...,u_d\)"}</annotation></semantics></math>&nbsp;in such a way that they are&nbsp;<em>mutually orthogonal</em>, i.e.&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msubsup><mi>u</mi><mi>i</mi><mi>T</mi></msubsup><msub><mi>u</mi><mi>j</mi></msub><mo>=</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_i^Tu_j = 1\)"}</annotation></semantics></math>&nbsp;if&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>i</mi><mo>=</mo><mi>j</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(i=j\)"}</annotation></semantics></math>&nbsp;otherwise&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>.&nbsp;Now if we project a point such as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_i\)"}</annotation></semantics></math>&nbsp;on&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>u</mi><mi>d</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_1,...,u_d\)"}</annotation></semantics></math>&nbsp;to get new coordinates as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msubsup><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><msub><mi>u</mi><mi>j</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_{ij} = x_{i}^{T}u_j\)"}</annotation></semantics></math>.&nbsp;</li>
<li>So for all&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\)"}</annotation></semantics></math>&nbsp;dimensions we can write this as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi></mrow></munderover><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>u</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_i =\sum_{j=1}^{d} y_{ij} u_{j}\)"}</annotation></semantics></math>&nbsp;or&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munderover><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>u</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub><mo>+</mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi></mrow></munderover><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>u</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_i = \sum_{j=1}^{k} y_{ij} u_{j} + \sum_{j=k+1}^{d} y_{ij} u_{j}\)"}</annotation></semantics></math>&nbsp;as two separated terms.</li>
<li><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">If we would like to&nbsp;</span><em style="font-family: Lato, sans-serif; font-size: 0.95rem;">minimize the mean square error</em><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;due to projection in new&nbsp;</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;dimension, we have:&nbsp;</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>u</mi><mi>k</mi></msub></mrow></munder><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−<!-- − --></mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munderover><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>u</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msup><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\min_{u_1,...,u_k} \frac{1}{n} \sum_{i=1}^{n} ||x_i - \sum_{j=1}^{k} y_{ij}u_{j}||^2\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;which we find that once again&nbsp;</span><em style="font-family: Lato, sans-serif; font-size: 0.95rem;"><em>top&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>k</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(k\)"}</annotation></semantics></math>&nbsp;</em></em><em style="font-family: Lato, sans-serif; font-size: 0.95rem;">eigenvectors of covariance</em><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;are the optimal solutions.</span></li>
</ul>
<h1 id="your-task"><span style="color: rgb(96, 56, 255);">Activity</span></h1>
<p>The youtube resources in the&nbsp;<em>see also</em>&nbsp;section below will help you explore this topic further if you would like to know more.</p>
<p>Share any questions or responses in the <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=1734011&amp;type=discuss&amp;rcode=DeakinUniv-19277" target="_blank" rel="noopener">Student Discussion</a></p>
<hr style="width: 100%; height: auto; color: #ffffff; border: 1px inset #cccccc;">
<div>
<h3>SEE ALSO</h3>
</div>
<ul role="complementary" style="list-style-type: none;">
<li><a href="https://www.youtube.com/watch?v=2bV_YQTIJrg&amp;list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM&amp;index=3" target="_blank" rel="noopener noreferrer">PCA: DIRECTION OF GREATEST VARIANCE</a>Principal Component Analysis (PCA) reduces the dimensionality of the data by selecting directions along which our data has the largest variance. Picking the direction of greatest variance preserves the distances in the original space (far-away points in the original space are less likely to end up very close to each other)</li>
<li><a href="https://www.youtube.com/watch?v=fKivxsVlycs&amp;feature=youtu.be" target="_blank" rel="noopener noreferrer">PRINCIPAL COMPONENTS = EIGENVECTORS</a>We can find the direction of the greatest variance in our data from the covariance matrix. It is the vector that does not rotate when we multiply it by the covariance matrix. Such vectors are called eigenvectors, and have corresponding eigenvalues. Eigenvectors that have the largest eigenvalues will be the principal components (new dimensions of our data).</li>
<li><a href="https://www.youtube.com/watch?v=2fCBE7DWgd0&amp;feature=youtu.be" target="_blank" rel="noopener noreferrer">FINDING EIGENVALUES AND EIGENVECTORS</a>To find the eigenvectors, we first solve the determinant equation for the eigenvalues. We then solve for each eigenvector by plugging the corresponding eigenvalue into the linear system. Remember that eigenvectors must have unit length.</li>
</ul>
<span face="Lato, sans-serif" style="font-family: Lato, sans-serif;"><span style="font-size: 15.2px;"><br></span></span></div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>