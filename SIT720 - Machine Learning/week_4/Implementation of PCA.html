<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<link rel="stylesheet" href="https://s.brightspace.com/lib/fonts/0.5.0/fonts.css"></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Implementation%20of%20PCA%20image%201.jpg" alt="Golden Grain Fields With Circular Patterns Washington, United States Of America" title="Golden Grain Fields With Circular Patterns Washington, United States Of America" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Implementation of PCA</h1>
</div>
<div>
<p>There are several alternative ways of implementing PCA.</p>
<p id="pca-for-data-where-n--d">PCA for data where&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi><mo>&lt;</mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n &lt; d\)"}</annotation></semantics></math></p>
<p>There are cases when the number of data points (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\)"}</annotation></semantics></math>)&nbsp;&nbsp;is less that number of dimensions&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\)"}</annotation></semantics></math>&nbsp;i.e.&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi><mo>&lt;</mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n &lt; d\)"}</annotation></semantics></math></p>
<p>Consider the following scenario:</p>
<ul>
<li>say we have&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>100</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(100\)"}</annotation></semantics></math>&nbsp;images in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>64</mn><mo>×<!-- × --></mo><mn>64</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(64\times64\)"}</annotation></semantics></math>&nbsp;dimensions,&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi><mo>=</mo><mn>100</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n = 100\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>=</mo><mn>64</mn><mo>×<!-- × --></mo><mn>64</mn><mo>=</mo><mn>4096</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d=64\times64 = 4096\)"}</annotation></semantics></math>.</li>
<li>In this case, the number of nonzero eigenvalues of data covariance matrix is less than or equal to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\)"}</annotation></semantics></math>.</li>
<li>If we use Eigen Value Decomposition (EVD) on the covariance matrix of size&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>×<!-- × --></mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d \times d\)"}</annotation></semantics></math>,&nbsp;we need to perform computations of the order of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>O</mi><mo stretchy="false">(</mo><msup><mi>d</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(O(d^3)\)"}</annotation></semantics></math>. This may be too&nbsp;<em>expensive!</em></li>
<li><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">In such cases, SVD can reduce the computations to&nbsp;</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(O(n^3)\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;or less.</span></li>
</ul>
<h3 id="using-svd-for-pca">Using SVD for PCA</h3>
<p>We can use SVD to perform PCA. As you have seen before in previous section, given any&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi><mo>×<!-- × --></mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n \times d\)"}</annotation></semantics></math>&nbsp;matrix&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Y\)"}</annotation></semantics></math>&nbsp;its&nbsp;<em>Singular Value Decomposition (SVD)</em>is given as:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Y</mi><mo>=</mo><mi>U</mi><mi>S</mi><msup><mi>V</mi><mi>T</mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Y = USV^T\)"}</annotation></semantics></math></p>
<p>Where:</p>
<ul>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>U</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(U\)"}</annotation></semantics></math>&nbsp;is a&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi><mo>×<!-- × --></mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\times d\)"}</annotation></semantics></math>&nbsp;orthogonal matrix (same as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>U</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(U\)"}</annotation></semantics></math>&nbsp;in previous section)</li>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>S</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(S\)"}</annotation></semantics></math>&nbsp;is a&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>×<!-- × --></mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\times d\)"}</annotation></semantics></math>&nbsp;diagonal matrix with elements&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>S</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>σ<!-- σ --></mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(S(i,i) = \sigma_i\)"}</annotation></semantics></math>&nbsp;</li>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>V</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(V\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;is a&nbsp;&nbsp;</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>×<!-- × --></mo><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\times d\)"}</annotation></semantics></math><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">&nbsp;</span><span style="font-family: Lato, sans-serif; font-size: 0.95rem;">orthogonal matrix</span></li>
</ul>
<p>Now if&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Y\)"}</annotation></semantics></math>&nbsp;is mean-centred version of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>X</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(X\)"}</annotation></semantics></math>&nbsp;then the covariance of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Y\)"}</annotation></semantics></math>&nbsp;is:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo stretchy="false">(</mo><mi>n</mi><mo>−<!-- − --></mo><mn>1</mn><mo stretchy="false">)</mo><mi>C</mi><mo>=</mo><mi>Y</mi><msup><mi>Y</mi><mi>T</mi></msup><mo>=</mo><mi>U</mi><mi>S</mi><mo stretchy="false">(</mo><msup><mi>V</mi><mi>T</mi></msup><mi>V</mi><mo stretchy="false">)</mo><mi>S</mi><msup><mi>U</mi><mi>T</mi></msup><mo>=</mo><mi>U</mi><mi>S</mi><mi>I</mi><mi>S</mi><msup><mi>U</mi><mi>T</mi></msup><mo>=</mo><mi>U</mi><msup><mi>S</mi><mn>2</mn></msup><msup><mi>U</mi><mi>T</mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\((n-1)C = YY^T = US(V^TV)SU^T = USISU^T = US^2U^T\)"}</annotation></semantics></math></p>
<p>Remember that&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi>V</mi><mi>T</mi></msup><mi>V</mi><mo>=</mo><mi>I</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(V^TV = I\)"}</annotation></semantics></math>, therefore:</p>
<p>&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi><mo>=</mo><mi>U</mi><mo stretchy="false">(</mo><mfrac><msup><mi>s</mi><mn>2</mn></msup><mrow><mi>n</mi><mo>−<!-- − --></mo><mn>1</mn></mrow></mfrac><mo stretchy="false">)</mo><msup><mi>U</mi><mi>T</mi></msup><mo>=</mo><mi>U</mi><mi>D</mi><msup><mi>U</mi><mi>T</mi></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C = U(\frac{s^2}{n-1})U^T = UDU^T\)"}</annotation></semantics></math></p>
<p>What we just did is that we created a connection among SVD and EVD. Also&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>U</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(U\)"}</annotation></semantics></math>&nbsp;of SVD is same as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>U</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(U\)"}</annotation></semantics></math>&nbsp;of EVD. Therefore, the singular vectors of SVD are the same as Eigenvectors of EVD and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>D</mi><mo>=</mo><mfrac><msup><mi>s</mi><mn>2</mn></msup><mrow><mi>n</mi><mo>−<!-- − --></mo><mn>1</mn></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(D = \frac{s^2}{n-1}\)"}</annotation></semantics></math>&nbsp;We have the relation&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mi>d</mi></msub><mo>=</mo><mfrac><msubsup><mi>σ<!-- σ --></mi><mi>d</mi><mn>2</mn></msubsup><mrow><mi>n</mi><mo>−<!-- − --></mo><mn>1</mn></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_d = \frac{\sigma_d^2}{n-1}\)"}</annotation></semantics></math>.&nbsp;If you do not want to use EVD you can just use SVD and get the matrix&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>S</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(S\)"}</annotation></semantics></math>&nbsp;or get the singular values and then compute the eigenvalues (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>λ<!-- λ --></mi><mi>d</mi></msub><mo>=</mo><mfrac><msubsup><mi>σ<!-- σ --></mi><mi>d</mi><mn>2</mn></msubsup><mrow><mi>n</mi><mo>−<!-- − --></mo><mn>1</mn></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\lambda_d = \frac{\sigma_d^2}{n-1}\)"}</annotation></semantics></math>).&nbsp;This gives the things we need to perform PCA. Remember performing PCA is nothing but multiplying&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>U</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(U\)"}</annotation></semantics></math>&nbsp;</p>
<p>matrix by the data matrix.</p>
<p>Consider the following figure as a real-world example.</p>
<p class="centerImage"><img src="../images/Implementation%20of%20PCA%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5 id="figure-illustration-of-projected-data-right-and-original-data-left">Figure. Illustration of projected data (right) and original data (left)</h5>
<p>The first major axis is the direction of largest variance direction. The second major axis is the direction of the second largest variance. If we calculate the values of these two axis in Figure 1 and call them&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_1\)"}</annotation></semantics></math>&nbsp;and<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>u</mi><mn>2</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(u_2\)"}</annotation></semantics></math>&nbsp;we can find the projected&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>X</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(X\)"}</annotation></semantics></math>&nbsp;&nbsp;data by multiplying&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>X</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(X\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>U</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(U\)"}</annotation></semantics></math>&nbsp;(right image of Figure). As you can see in the figure, the projected data lost its correlated form and looks uncorrelated. Remember in this example we did not perform any dimensionality reduction. Both projected data and the original data had&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>2</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(2\)"}</annotation></semantics></math>&nbsp;dimensions.&nbsp;So in this particular example, we used PCA to de-correlate the dimensions. In this example the covariance matrix of the original is:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>C</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing="0.7em 0.7em" columnspacing="1em"><mtr><mtd><mn>0.6592</mn></mtd><mtd><mn>0.2538</mn></mtd></mtr><mtr><mtd><mn>0.2538</mn></mtd><mtd><mn>0.9864</mn></mtd></mtr></mtable><mo>]</mo></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"C = \begin{bmatrix} 0.6592 &amp; 0.2538 \\[0.3em] 0.2538 &amp; 0.9864 \\[0.3em] \end{bmatrix}"}</annotation></semantics></math></p>
<p>Which as you can see captures some relations and correlations among the dimensions. On the other hand, the covariance matrix of the projected data is:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>C</mi><mo>=</mo><mrow><mo>[</mo><mtable rowspacing="0.7em 0.7em" columnspacing="1em"><mtr><mtd><mn>1.1247</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0.5208</mn></mtd></mtr></mtable><mo>]</mo></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"C = \begin{bmatrix} 1.1247 &amp; 0 \\[0.3em] 0 &amp; 0.5208 \\[0.3em] \end{bmatrix}"}</annotation></semantics></math></p>
<p>Which illustrates two important points:</p>
<ol>
<li>There are no correlations among the projected data&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo>;</mo><mspace width="1em"></mspace><mi>i</mi><mo>≠<!-- ≠ --></mo><mi>j</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C(i,j) = 0; \quad i\neq j\)"}</annotation></semantics></math>.</li>
<li>The first dimension or feature has a higher value which means it is more important than the second one&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>C</mi><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo><mo>&gt;</mo><mi>C</mi><mo stretchy="false">(</mo><mn>2</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(C(1,1) &gt; C(2,2)\)"}</annotation></semantics></math>.</li>
</ol>
<span style="font-family: Lato, Arial, sans-serif; font-size: 0.95rem;"><span style="font-family: Lato, Arial, sans-serif; font-size: 0.95rem;">Now if we decide to drop one of the dimensions and use dimensionality reduction by PCA, we should choose the eigenvector corresponding to the eigenvalue&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>=</mo><mn>1.1247</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(= 1.1247\)"}</annotation></semantics></math>&nbsp;</span></span>and then project all data on that axis. Mean square error based on this approximation would be the sum of the remaining eigenvalues.
<h1 id="your-task"><span style="color: rgb(96, 56, 255);">Activity</span></h1>
<p>Can you describe this step in different words? <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=1734011&amp;type=discuss&amp;rcode=DeakinUniv-19277" target="_blank" rel="noopener">Student Discussion</a>&nbsp;what you think this means to your fellow students.</p>
<br><span face="Lato, sans-serif" style="font-family: Lato, sans-serif;"><span style="font-size: 15.2px;"><br></span></span></div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>