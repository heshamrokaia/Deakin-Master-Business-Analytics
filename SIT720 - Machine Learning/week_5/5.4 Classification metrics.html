<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

</head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Classification%20metrics%20image%201.jpg" alt="Network color communication background. Illustration of an abstract social network, flat design" title="Network color communication background. Illustration of an abstract social network, flat design" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Classification metrics</h1>
</div>
<div>
<p>The metrics that you choose to evaluate your machine learning model are very important. The choice of evaluation metrics influences how performance is measured and compared.</p>
<p>The most common type of machine learning applications are&nbsp;<em>classification problems</em>. There are myriad metrics that can be used to evaluate predictions for these types of problems.</p>
<h3 id="confusion-matrix">Confusion Matrix</h3>
<p>A&nbsp;<em>confusion matrix</em>&nbsp;is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and divided down by each class. Confusion matrices are a way to understand the types of errors made by a model. Confusion matrices are also called&nbsp;<em>contingency tables</em>.</p>
<p>One of the reasons for using a confusion matrix is that, accuracy is not a reliable metric for the&nbsp;<em>real performance</em>&nbsp;of a classifier. If the data set is unbalanced (i.e. when the numbers of observations in different classes vary greatly), it will yield misleading results. For example, if there were 90 apples and only 10 oranges in the data set, a particular classifier might classify all the observations as apples. But is this wise?</p>
<p>The following figure illustrates a sample confusion matrix for a classification problem with&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>11</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(11\)"}</annotation></semantics></math>&nbsp;classes. The&nbsp;<em>diagonal values</em>&nbsp;represents the elements where the predicted classes were equal to the expected classes, and the&nbsp;<em>off-diagonal values</em>&nbsp;represent the elements where the classifier got the prediction wrong. The higher the proportion of values on the diagonal of the matrix in relation to values off of the diagonal, the better the classifier is (why?).</p>
<p class="centerImage"><img src="../images/Classification%20metrics%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><br>Figure. Confusion matrix for a classification problem.</h5>
<p>Now that we understand the fundamentals around confusion matrices, let’s explore more detailed concepts. Consider the following figure as a confusion matrix for only two classes.</p>
<p class="centerImage"><img src="../images/Classification%20metrics%20image%203.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><br>Figure. Confusion matrix for two classes.</h5>
<p>You could represent the&nbsp;<em>positive</em>&nbsp;class as&nbsp;<em>class 1</em>&nbsp;and the&nbsp;<em>negative</em>&nbsp;class as&nbsp;<em>class 0</em>. For the acronyms used in the table (i.e. TP), the second letter (e.g. letter&nbsp;<em>P</em>&nbsp;in&nbsp;<em>TP</em>) says what we predicted and the first letter (e.g. letter&nbsp;<em>T</em>&nbsp;in&nbsp;<em>TP</em>) says whether it was&nbsp;<em>true</em>&nbsp;or&nbsp;<em>false</em>. In this case we define the accuracy as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>a</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi><mtext>&nbsp;</mtext><mo>+</mo><mtext>&nbsp;</mtext><mi>T</mi><mi>N</mi></mrow><mrow><mi>T</mi><mi>P</mi><mtext>&nbsp;</mtext><mo>+</mo><mtext>&nbsp;</mtext><mi>F</mi><mi>P</mi><mtext>&nbsp;</mtext><mo>+</mo><mtext>&nbsp;</mtext><mi>F</mi><mi>N</mi><mtext>&nbsp;</mtext><mo>+</mo><mtext>&nbsp;</mtext><mi>T</mi><mi>N</mi></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"accuracy = \frac{TP \ + \ TN}{TP \ + \ FP \ + \ FN \ + \ TN}"}</annotation></semantics></math></p>
<p>But as we have said before, accuracy may not be a useful metric for imbalanced class problems. Also There may be differential costs of making errors for different classes. For example, an incorrect medical diagnosis may be more costly than a false positive! So we need high confidence predictions only. Therefore, we can define other evaluation metrics based on a confusion matrix:</p>
<ul>
<li><strong>True Positive Rate (TPR) or Recall or Sensitivity:</strong><br>is the fraction of&nbsp;<em>true positive&nbsp; (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>T</mi><mi>P</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(TP\)"}</annotation></semantics></math>)&nbsp;samples that have been predicted positive over the total amount of positive&nbsp;samples (<span style="color: #b45f06;"><strong>TP+FN</strong></span>).</em></li>
</ul>
<span style="font-size: 15.2px;"><i><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mtext>&nbsp;</mtext><mo>+</mo><mtext>&nbsp;</mtext><mi>F</mi><mi>N</mi></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"recall= \frac{TP}{TP \ + \ FN}"}</annotation></semantics></math></i></span></div>
<ul>
<li><span style="font-size: 15.2px;"><i><strong>False positive rate (FPR):</strong>&nbsp;&nbsp;</i></span><i>is the fraction of&nbsp;<em>false predicted positive&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>F</mi><mi>P</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(FP\)"}</annotation></semantics></math>&nbsp;samples over the total amount of&nbsp;negative samples&nbsp; (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mi>T</mi><mi>N</mi><mtext>&nbsp;</mtext><mo>+</mo><mtext>&nbsp;</mtext><mi>F</mi><mi>P</mi></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\({TN \ + \ FP}\)"}</annotation></semantics></math>).</em></i></li>
</ul>
<p><span style="font-size: 15.2px;"><i><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>N</mi><mtext>&nbsp;</mtext><mo>+</mo><mtext>&nbsp;</mtext><mi>F</mi><mi>P</mi></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"FPR= \frac{FP}{TN \ + \ FP}"}</annotation></semantics></math></i></span></p>
<p>Reporting all these metrics in a machine learning classifier, can shed some lights on the overall performance of the classifier. The next metric we are going to cover for classifiers, is ROC curve.</p>
<h3 id="roc-curve">ROC Curve</h3>
<p><em>Receiver Operating Characteristics</em>&nbsp;(ROC) curve has long been used in signal detection theory to depict the trade-off between the true positive rate and false positive rate over noisy channels. Recent years have seen an increase in the use of ROC graphs in the machine learning community. ROC curve is especially useful for domains with imbalanced class distribution and unequal classification error costs.</p>
<p>The ROC curve is created by plotting the&nbsp;<em>true positive rate</em>&nbsp;(TPR) against the&nbsp;<em>false positive rate</em>&nbsp;(FPR) at various threshold settings. This has to be done to depict relative trade-offs between benefits (true positives) and costs (false positives).</p>
<p>As you can see in the figure below, different methods can work better in different parts of ROC space. Lets say there are two algorithms like&nbsp;<em>Alg 1</em>and&nbsp;<em>Alg 2</em>&nbsp;in the figure. The&nbsp;<em>Alg 2</em>&nbsp;is good in the sense that it can give you high true positive rate while keeping the false positive rate low. whereas in&nbsp;<em>Alg 1</em>, if it is allowed to incur more false positive rate, then&nbsp;<em>Alg 1</em>&nbsp;can give us better higher true positive rate too.</p>
<p class="centerImage"><img src="../images/Classification%20metrics%20image%204.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><br>Figure. Sample ROC curve</h5>
<p>Obviously it depends on the specification of the problem. how much can we afford false positive rate. if we can afford higher false positive rate, we can have higher true positive rate too.</p>
<p>A model that predicts at chance (random guessing) will have an ROC curve that looks like the diagonal dashed line in the figure above. That is not a discriminating model. The further the curve is from the diagonal line, the better the model is at discriminating between positives and negatives in general.</p>
<p>Consider the following figure as another example. Lets say we are designing a classifier for a medical diagnosis. In this case we probably do not mind&nbsp;<em>false positives</em>&nbsp;since missing&nbsp;&nbsp;<em>positive occurrence</em>&nbsp;in detection of diseases are extremely costly.</p>
<p class="centerImage"><img src="../images/Classification%20metrics%20image%205.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><br>Figure. Illustration of different scenarios in ROC curve.</h5>
<p>But, there can be situations where we do mind the&nbsp;<em>false positive rate</em>. A good example of that could be in conviction for a crime. You do not want to waste someones life with a&nbsp;<em>false positive</em>&nbsp;decision!</p>
<p>There are useful statistics that can be calculated via ROC curve, like the&nbsp;<em>Area Under the Curve</em>&nbsp;(AUC) and the&nbsp;<em>Youden Index</em>. These tell you how well the model predicts and the optimal cut point for any given model (under specific circumstances). AUC is used to summarize the ROC curve using a single number. The higher the value of AUC, better performing is the classifier! A random classifier has an AUC of 0.5.</p>
<h3 id="f-1-measure">F-1 Measure</h3>
<p>Another useful metric could be the combination of&nbsp;<em>Precision</em>&nbsp;and&nbsp;<em>Recall</em>.&nbsp;&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>F</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(F_1\)"}</annotation></semantics></math>-measure is a metric that combines both Precision and Recall in a single number.&nbsp;&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>F</mi><mn>1</mn></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(F_1\)"}</annotation></semantics></math>-measure is defined as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>F</mi><mn>1</mn></msub><mo>=</mo><mn>2</mn><mo>×<!-- × --></mo><mfrac><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>×<!-- × --></mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(F_1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}\)"}</annotation></semantics></math><strong></strong></p>
<h2 id="your-task">Activity</h2>
<p>How comprehensive is a confusion matrix for evaluating a model? Can you think of an example where a confusion matrix is appropriate?</p>
<p>How about a good use of an ROC Curve?</p>
<p>Can you offer some cases in which a confusion matrix or ROC Curve is not enough for evaluating a model?</p>
<div><span style="font-size: 15.2px;"><i></i></span></div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>