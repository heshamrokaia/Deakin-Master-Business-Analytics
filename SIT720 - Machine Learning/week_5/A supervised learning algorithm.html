<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
</head>
<!-- ------------------------------------ -->
<body class="cloudFirst">
<p><img src="../images/A%20supervised%20learning%20algorithm%20image%201.jpg" alt="science formula and math equation abstract background. concept of machine learning and artificial intelligence" title="science formula and math equation abstract background. concept of machine learning and artificial intelligence" style="max-width: 100%;" data-d2l-editor-default-img-style="true" /></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">&copy; Getty Images</a></address>
<div>
<h1>A supervised learning algorithm</h1>
</div>
<div>
<p>Let&rsquo;s take a closer look at a supervised learning example. This is an important concept so it&rsquo;s stated in several different ways. If you understand this concept you can skip ahead.</p>
<p>Consider a supervised learning algorithm with&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\)"}</annotation></semantics></math>&nbsp;training data&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo fence="false" stretchy="false">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo fence="false" stretchy="false">}</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\{x_i,y_i\}\)"}</annotation></semantics></math>&nbsp;where&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mspace width="1em" /><mi>I</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\quad I=1,2,3,...,n\)"}</annotation></semantics></math>.&nbsp;The aim is to find a function that&rsquo;s as close as possible to the unknown function, to determine the existing relationship between&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_i\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_i\)"}</annotation></semantics></math>.</p>
<p>In other words you have two sets of data: the input and the output. The output set is obtained by applying the function to the input set. This means for each element in the input set there is a corresponding element in the output set. You are trying to figure out the relationship between the pairs of numbers. The relationship between the two is the function.</p>
<h3 id="hypothesis-space">Hypothesis space</h3>
<p>We will name a hypothesis function,&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>,&nbsp;as an element of a range of possible functions&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>H</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(H\)"}</annotation></semantics></math>,&nbsp;</p>
<p>usually called the&nbsp;<em>hypothesis space</em>. We&rsquo;ll select a hypothesis function that we think is similar to the&nbsp;<em>true function</em>&nbsp;behind the data.</p>
<p>Some examples of hypothesis space are:</p>
<ul>
<li>space of all linear functions in&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi><mo>&#x2212;<!-- − --></mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d-\)"}</annotation></semantics></math>dimensions space of all polynomial functions up to degree&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>p</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(p\)"}</annotation></semantics></math>.</li>
</ul>
<h3 id="finding-a-function">Finding a function</h3>
<p>Let us come back to our main problem. We would like to find the function&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>&nbsp;which can map the input to the corresponding output&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo>:</mo><mi>X</mi><mo stretchy="false">&#x2192;<!-- → --></mo><mi>Y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h: X\rightarrow Y\)"}</annotation></semantics></math>&nbsp;accurately, to take the values from set&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>X</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(X\)"}</annotation></semantics></math>&nbsp;to set&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Y\)"}</annotation></semantics></math>.</p>
<p><strong>In supervised learning, given the training data,&nbsp;<em>the learning algorithm seeks a function</em>&nbsp;on&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo>:</mo><mi>X</mi><mo stretchy="false">&#x2192;<!-- → --></mo><mi>Y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h: X\rightarrow Y\)"}</annotation></semantics></math>&nbsp;where&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>X</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(X\)"}</annotation></semantics></math>&nbsp;is the input space and&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Y\)"}</annotation></semantics></math></strong><strong>&nbsp;is the output space.</strong></p>
<p>The question which arises here is:</p>
<p>how can we measure the quality of function&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>?&nbsp;How can we understand how accurately&nbsp;&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>&nbsp;can map&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>X</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(X\)"}</annotation></semantics></math>&nbsp;to the target&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Y\)"}</annotation></semantics></math>?</p>
<p>To answer this question, we need to introduce a new function called the&nbsp;<em>loss function</em>.</p>
<h3 id="loss-function">Loss function</h3>
<p>The loss function is really a measure of accuracy. How accurately does your&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>&nbsp;function describe the relationship between&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>X</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(X\)"}</annotation></semantics></math>&nbsp;to the target&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>Y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(Y\)"}</annotation></semantics></math>?</p>
<p>A function&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>&nbsp;is applied to a training instance&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_i\)"}</annotation></semantics></math>&nbsp;and it gives the output&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h(x_i)\)"}</annotation></semantics></math>.</p>
<p>Let&rsquo;s denote the function as&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mi>h</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y}_i = h(x_i)\)"}</annotation></semantics></math></p>
<p>However, since we are dealing with a supervised problem we know that the true output is&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_i\)"}</annotation></semantics></math>.</p>
<p>n order to measure how well function&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>&nbsp;fits the training data, we need to find the difference between&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_i\)"}</annotation></semantics></math>&nbsp;and&nbsp;&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y}_i\)"}</annotation></semantics></math>.</p>
<p>To measure the difference we define a different equation, a loss function&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(L(y_i,\hat{y}_i)\)"}</annotation></semantics></math>.</p>
<h4 id="examples">Examples</h4>
<p>You should be familiar with some of these examples of loss functions:</p>
<ul>
<li>Square loss:&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>&#x2212;<!-- − --></mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(L(y_i,\hat{y}_i) = (y_i-\hat{y}_i)^2\)"}</annotation></semantics></math>&nbsp;
<p>(useful for regression)</p>
</li>
<li>
<p></p>
Absolute loss:&nbsp;</li>
</ul>
<span style="font-size: 15.2px;"><math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(L(y_i,\hat{y}_i) =\)"}</annotation></semantics></math><math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>i</mi></msub><mo>&#x2212;<!-- − --></mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_i - \hat{y}_i\)"}</annotation></semantics></math>&nbsp;(useful for regression)</span>
<ul>
<li><span style="font-size: 15.2px;"><math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn><mo>&#x2212;<!-- − --></mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0-1\)"}</annotation></semantics></math>&nbsp;loss</span></li>
</ul>
<span style="font-size: 15.2px;">&nbsp;</span><math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(L(y_i,\hat{y}_i) = 1(y_i,\hat{y}_i)\)"}</annotation></semantics></math>&nbsp;which is equal to&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;if&nbsp;&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_i = \hat{y}_i\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>&nbsp;otherwise (useful for classification)
<ul>
<li>Other loss functions for classification problem:&nbsp;<br />e.g. Logistic loss, Hinge loss</li>
</ul>
<p>The loss function is used to compute the error between the actual result of&nbsp;&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_i\)"}</annotation></semantics></math>&nbsp;and what we calculated as&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;<!-- ^ --></mo></mover></mrow><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\hat{y}_i\)"}</annotation></semantics></math>.</p>
<h3 id="empirical-risk">Empirical risk</h3>
<p>Similar to the loss function, we can define a factor called&nbsp;<em>empirical risk</em>. Among all functions in hypothesis space, that is,&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo>&#x2208;<!-- ∈ --></mo><mi>H</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h \in H\)"}</annotation></semantics></math>,&nbsp;&nbsp;we select the function&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>,&nbsp;which minimises the empirical risk. You can calculate the empirical risk by averaging the results of the loss function. The lower the empirical risk based on the training data, the closer the function represents the true relationship between the pair of values&nbsp;&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>x</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(x_i\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_i\)"}</annotation></semantics></math>.&nbsp;</p>
<p>But how can achieve this? In theory, the answer is simple! We just need to minimize the risk of loss. In other words, we select a function&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>&nbsp;that achieves minimum risk:</p>
<p><math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>h</mi><mo>&#x2208;<!-- ∈ --></mo><mi>H</mi></mrow></munder><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>&#x2211;<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mi>h</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\min_{h \in H} \frac{1}{n} \sum_{i=1}^{n} L(y_i,h(x_i))"}</annotation></semantics></math></p>
<p>This simple equation is the sum of all losses for all&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\)"}</annotation></semantics></math>&nbsp;training points which can be calculated by&nbsp;&nbsp;<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><munderover><mo>&#x2211;<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mi>h</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\sum_{i=1}^{n} L(y_i,h(x_i))\)"}</annotation></semantics></math>.&nbsp;&nbsp;Then we divide this value by&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\)"}</annotation></semantics></math>&nbsp;to find this average loss or the risk.</p>
<p>This equation states that by finding a function like<math title=""  xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h\)"}</annotation></semantics></math>&nbsp;which minimizes the risk as mentioned in the above formula, one can find the solution of the learning problem. Hence a supervised learning algorithm is often trained or learnt through an&nbsp;<em>optimization algorithm</em>.</p>
<h2 id="your-task">Activity</h2>
<p>Have you ever thought about the true application of 0 &minus; 1 loss? Write and share a simple explanation of the 0 - 1 loss function and its use?</p>
<p>Please share your thoughts and examples in the <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=1734011&amp;type=discuss&amp;rcode=DeakinUniversity-2000313" target="_top">discussion forum</a>.</p>
<p>&nbsp;</p>
<span face="Lato, sans-serif" style="font-family: Lato, sans-serif;"><span style="font-size: 15.2px;"><br /></span></span></div>
<hr />
<div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</body>
<!-- ------------------------------------ -->
<script defer type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
<script  type="text/javascript">
function localProc(){
  console.log("ready!");
}
</script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-mathjax.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
</html>