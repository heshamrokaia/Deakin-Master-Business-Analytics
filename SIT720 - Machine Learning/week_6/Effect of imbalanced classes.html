<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<link rel="stylesheet" href="https://s.brightspace.com/lib/fonts/0.5.0/fonts.css"></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Model%20complexity%20image%201.jpg" alt="Abstract chaotic background" title="Abstract chaotic background" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Model complexity</h1>
</div>
<div>
<p>Over-fitting happens when we find an overly complex model based on the data. Under-fitting is the result of an extremely simple model.</p>
<p>We have already encountered over-fitting and under-fitting in previous lessons. The figure below illustrates some of these concepts. Over-fitting will happen when your model starts to capture some irrelevant noise points in the data while building the model, rather than the whole pattern (right image on the figure). Under-fitting is the result of an extremely simple model (left image on the figure).</p>
<p class="centerImage"><img src="../images/Model%20complexity%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Over-fitting and Under-fitting.</h5>
<p>Under-fitting occurs if the complexity of the model is lower than necessary.</p>
<ul>
<li><strong>Scenario-1:</strong>&nbsp;We may be using a linear model, when the data requires a nonlinear model.</li>
<li><strong>Scenario-2:</strong>&nbsp;We may be using the right hypothesis (linear or nonlinear) but the number of variables might be falling short of what is required. For example, to predict the income of a person,&nbsp;<em>age</em>&nbsp;alone may not be sufficient.</li>
</ul>
<p>We can&nbsp;<em>detect</em>&nbsp;under-fitting by checking if the model fitting error on the training data is high.</p>
<h4 id="example-1">Example 1:</h4>
<p>To predict a person’s income, knowing&nbsp;<em>age</em>&nbsp;alone is not sufficient. Assuming our dataset has information about age, sex, education; we could&nbsp;<em>add</em>&nbsp;them as explaining variables. Our model becomes more interesting and&nbsp;<em>more complex</em>.</p>
<p>The new model explains the data better but is still not good enough. We need to&nbsp;<em>add</em>&nbsp;even more variables (i.e. location, profession of parents, social background, number of children, weight, preferred colour, best meal, last holiday destination).</p>
<p>Our model will be even better but will probably be&nbsp;<em>over-fitting</em>&nbsp;now. It will probably produce poor predictions on unseen data. It has learnt too many specifics of the training data and will probably have learnt the unhelpful&nbsp;<em>background noise</em>.</p>
<h4 id="example-2-overfitting">Example 2 (<em>Overfitting</em>):</h4>
<p>Let’s say you attend a symphony and want to record the clearest sound possible. You buy a super-sensitive microphone and audio system to pick up all the sounds in the auditorium. Now, you have started to over-fit. You are detecting unhelpful, undesirable noise such as:</p>
<ul>
<li>you hear your neighbours shuffling in their seats</li>
<li>the musicians turning their pages</li>
<li>even the swishing of the conductor’s coat jacket.</li>
</ul>
<p>So fitting a perfect model is only listening to the Symphony (<em>signal</em>) and not to the&nbsp;<em>background noise</em>.</p>
<h3 id="bias-variance-decomposition">Bias Variance Decomposition</h3>
<p>Let us assume our data&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\((x,y)\) "}</annotation></semantics></math>&nbsp;has the true relation&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>ϵ<!-- ϵ --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y=f(x) + \epsilon\)"}</annotation></semantics></math>, where&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>ϵ<!-- ϵ --></mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\epsilon\)"}</annotation></semantics></math>&nbsp;is a measurement of noise in&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y\)"}</annotation></semantics></math>&nbsp;with mean zero and variance&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msubsup><mi>σ<!-- σ --></mi><mrow class="MJX-TeXAtom-ORD"><mi>ϵ<!-- ϵ --></mi></mrow><mn>2</mn></msubsup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\sigma_{\epsilon}^2\)"}</annotation></semantics></math>.</p>
<p>Also assume that we are fitting an hypothesis function (or model)&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>h</mi><mi>D</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h_D(x)\)"}</annotation></semantics></math>&nbsp;using dataset&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>D</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(D\)"}</annotation></semantics></math>.&nbsp;&nbsp;The expected loss (or risk) has&nbsp;<em>three components</em>.&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>R</mi><mi>i</mi><mi>s</mi><mi>k</mi><mo>=</mo><mo fence="false" stretchy="false">{</mo><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">[</mo><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−<!-- − --></mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mo fence="false" stretchy="false">}</mo><mn>2</mn></msup><mo>+</mo><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">[</mo><mo fence="false" stretchy="false">{</mo><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−<!-- − --></mo><msub><mi>E</mi><mi>D</mi></msub><mo stretchy="false">[</mo><msub><mi>h</mi><mi>D</mi></msub><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mo fence="false" stretchy="false">}</mo><mn>2</mn></msup><mo stretchy="false">]</mo><mo>+</mo><msubsup><mi>σ<!-- σ --></mi><mrow class="MJX-TeXAtom-ORD"><mi>ϵ<!-- ϵ --></mi></mrow><mn>2</mn></msubsup><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ Risk = \{E_{D}[h_{D}(x) - f(x)]\}^2 + E_{D}[\{h_{D}(x) - E_D[h_D({x})]\}^2] + \sigma_{\epsilon}^2 \\"}</annotation></semantics></math></p>
<p>where:</p>
<ul>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>R</mi><mi>i</mi><mi>s</mi><mi>k</mi><mo>=</mo><mo fence="false" stretchy="false">{</mo><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">[</mo><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−<!-- − --></mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mo fence="false" stretchy="false">}</mo><mn>2</mn></msup><mo>+</mo><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">[</mo><mo fence="false" stretchy="false">{</mo><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−<!-- − --></mo><msub><mi>E</mi><mi>D</mi></msub><mo stretchy="false">[</mo><msub><mi>h</mi><mi>D</mi></msub><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mo fence="false" stretchy="false">}</mo><mn>2</mn></msup><mo stretchy="false">]</mo><mo>+</mo><msubsup><mi>σ<!-- σ --></mi><mrow class="MJX-TeXAtom-ORD"><mi>ϵ<!-- ϵ --></mi></mrow><mn>2</mn></msubsup><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\\ Risk = \{E_{D}[h_{D}(x) - f(x)]\}^2 + E_{D}[\{h_{D}(x) - E_D[h_D({x})]\}^2] + \sigma_{\epsilon}^2 \\\)"}</annotation></semantics></math>&nbsp;is the&nbsp;<em>(bias)</em><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi></mi><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(^2\)"}</annotation></semantics></math></li>
<li><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">[</mo><mo fence="false" stretchy="false">{</mo><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−<!-- − --></mo><msub><mi>E</mi><mi>D</mi></msub><mo stretchy="false">[</mo><msub><mi>h</mi><mi>D</mi></msub><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mo fence="false" stretchy="false">}</mo><mn>2</mn></msup><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E_{D}[\{h_{D}(x) - E_D[h_D({x})]\}^2]\)"}</annotation></semantics></math>&nbsp;is the&nbsp;<em>variance&nbsp;</em></li>
<li><em style="font-family: Lato, sans-serif; font-size: 0.95rem;"><em><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msubsup><mi>σ<!-- σ --></mi><mrow class="MJX-TeXAtom-ORD"><mi>ϵ<!-- ϵ --></mi></mrow><mn>2</mn></msubsup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\sigma_{\epsilon}^2\)"}</annotation></semantics></math>&nbsp;is the noise (Irreducible error).</em></em></li>
</ul>
<p>Let’s see more details about these separate parts:</p>
<ul>
<li>&nbsp;<em>(bias)<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi></mi><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(^2\)"}</annotation></semantics></math>:</em>&nbsp;&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo fence="false" stretchy="false">{</mo><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">[</mo><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−<!-- − --></mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mo fence="false" stretchy="false">}</mo><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\{E_{D}[h_{D}(x) - f(x)]\}^2\)"}</annotation></semantics></math>&nbsp;This term shows how accurate your hypothesis function is (in other words how accurate your designed model is:&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>h</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h_d(x)\)"}</annotation></semantics></math>. The&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>E</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E\)"}</annotation></semantics></math>&nbsp;(expectation) means you need to average out this error to find out the expectation of error regarding this hypothesis (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h_{D}(x)\)"}</annotation></semantics></math>)&nbsp;and the true function output (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(f(x)\)"}</annotation></semantics></math>)&nbsp;As long as you are building an accurate model with a low error rate,&nbsp;<em>(bias)<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi></mi><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(^2\)"}</annotation></semantics></math>&nbsp;is a small value and possibly close to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>.</em></li>
<li><em>variance:&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>E</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">[</mo><mo fence="false" stretchy="false">{</mo><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>D</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−<!-- − --></mo><msub><mi>E</mi><mi>D</mi></msub><mo stretchy="false">[</mo><msub><mi>h</mi><mi>D</mi></msub><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi>x</mi></mrow><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mo fence="false" stretchy="false">}</mo><mn>2</mn></msup><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E_{D}[\{h_{D}(x) - E_D[h_D({x})]\}^2]\)"}</annotation></semantics></math>&nbsp;</em>You will notice that this term does not have&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(f(x)\)"}</annotation></semantics></math>&nbsp;inside it. That means it solely relies on your model which is&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>h</mi><mi>D</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h_D(x)\)"}</annotation></semantics></math>&nbsp;To put it simply, this model measures the tolerance of your calculated model while changing just the data set&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>D</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(D\)"}</annotation></semantics></math>.&nbsp;&nbsp;If it varies too much that’s a problem! The&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>E</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(E\)"}</annotation></semantics></math>&nbsp;or the expectation of this term measures the complexity of your model. The higher the variance the more complex the model.<br>
<p></p>
</li>
</ul>
<p>Hence, you can see that increasing the variance of a model means lowering bias as the model becomes more complex. On the other hand you can see that the low complexity for a model will result in high bias and low variance. So higher bias results in lower variance and high variance results in lower bias.</p>
<p>This illustrates another trade-off problem in machine learning. Basically, we can only minimise&nbsp;<em>bias</em>&nbsp;and&nbsp;<em>variance</em>&nbsp;since you should not be worried about noise&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msubsup><mi>σ<!-- σ --></mi><mrow class="MJX-TeXAtom-ORD"><mi>ϵ<!-- ϵ --></mi></mrow><mn>2</mn></msubsup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\sigma_{\epsilon}^2\)"}</annotation></semantics></math>&nbsp;for now. Noise is related to your observations from the function.</p>
<h3 id="variance-bias-trade-off">Variance bias trade off</h3>
<p>To better illustrate the variance-bias trade-off examine the following figure. As you can see, the best model is a model with low variance and low bias. It means the model is not too complex but is properly accurate. The worst model would have high bias, which means it’s not accurate based on the training data, and high variance which means it’s far too complex.</p>
<p class="centerImage"><img src="../images/Model%20complexity%20image%203.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Bias Variance Trade-off</h5>
<p>As another example lets consider these two situations:</p>
<ul>
<li>Models with too few parameters are inaccurate because of a large bias (not enough flexibility):&nbsp;<em>under-fitting</em>.</li>
</ul>
<p class="centerImage"><img src="../images/Model%20complexity%20image%204.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5><strong>Figure.</strong>&nbsp;Under-fitting with a large bias.</h5>
<ul>
<li>Models with too many parameters are inaccurate because of a large variance (too much sensitivity to the sample):&nbsp;<em>over-fitting</em>.</li>
</ul>
<p class="centerImage"><img src="../images/Model%20complexity%20image%205.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;Over-fitting with large variance</h5>
<h3 id="summary">Summary</h3>
<p>Based on the above information on the bias-variance trade-off, we know that:</p>
<ul>
<li>Low bias implies high variance, and high bias implies low variance</li>
<li>We need to find the&nbsp;<em>sweet spot</em>&nbsp;where&nbsp;<em><em>Risk = bias<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mi></mi><mn>2</mn></msup></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(^2\)"}</annotation></semantics></math>&nbsp;</em></em><em>+ variance + noise</em>&nbsp;is the minimum.</li>
<li>The&nbsp;<em>minimum error</em>&nbsp;is at the right&nbsp;<em>model complexity</em>.</li>
</ul>
<br style="font-size: 0.95rem;">
<p>Another interesting question arises here is when using linear models, can we still over-fit?&nbsp;<em>It depends!</em></p>
<p>Depends on our&nbsp;<em>model complexity</em>. In linear models, the model complexity&nbsp;<em>grows with the number of features</em>. Using all data dimensions as features may fit the model on&nbsp;<em>background noise</em>&nbsp;as well as true patterns (signal).</p>
<p>In the next section we are going to introduce&nbsp;<em>Regularisation</em>&nbsp;as a technique used to control the model complexity.</p>
<h2 id="your-task">Activity</h2>
<p>Check out this video from Stanford University on diagnosing bias and variance:</p>
<p></p>
<div style="font-size: 0.95rem;">
<p class="centerVideo"><iframe width="648" height="365" src="https://www.youtube.com/embed/ewogYw5oCAI?wmode=opaque" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></p>
<h5>This is an additional video, hosted on YouTube.</h5>
</div>
<p></p>
<p>What additional or different information does this video provide? <a href="/d2l/common/dialogs/quickLink/quickLink.d2l?ou=1734011&amp;type=discuss&amp;rcode=DeakinUniversity-2001546" target="_top">Share</a>&nbsp;your thoughts.</p>
</div>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>