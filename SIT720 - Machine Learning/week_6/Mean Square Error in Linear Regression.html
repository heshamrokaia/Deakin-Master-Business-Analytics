<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<link rel="stylesheet" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/open-source-css-and-js/prism/prism.css" type="text/css"><link rel="stylesheet" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/open-source-css-and-js/prism/prism.overrides.css" type="text/css"></head><body style="color: rgb(32, 33, 34); font-family: verdana, sans-serif; font-size: 10px;"><p><img src="../images/Training%20a%20logistic%20regression%20model%20image%201.jpg" alt="Individuals Unite To Become Team, Family, Network" title="Individuals Unite To Become Team, Family, Network" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<address><a href="https://www.gettyimages.com.au" target="_blank" rel="noopener noreferrer">© Getty Images</a></address>
<div>
<h1>Training a logistic regression model</h1>
</div>
<p><strong>Training a logistic regression model means using training data to&nbsp;</strong><em><strong>estimate the regression coefficient vector&nbsp;</strong><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>.</em></p>
<p><em>In training a logistic regression model, we can use&nbsp;maximum likelihood estimation (MLE)&nbsp;to estimate&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>.&nbsp;</em>The likelihood function of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>&nbsp;using data&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\((x_i,y_i)\)"}</annotation></semantics></math>&nbsp;is given as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mi>l</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac></mstyle><annotation encoding="latex">{"version":"1.1","math":"l(\textbf{w})=\frac{1}{1+exp(-y_i\textbf{x}_i^T\textbf{w})}"}</annotation></semantics></math></p>
<p>In the above, we assuming a Bernoulli distribution on&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_i\)"}</annotation></semantics></math>&nbsp;because of the binary forms of the outputs.</p>
<p>But how did we come up with this likelihood function? Lets see the mathematical derivations:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mi>σ<!-- σ --></mi><mo stretchy="false">(</mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ P(y=1\vert\textbf{x}) = \sigma(\textbf{x}^T\textbf{w}) = \frac{1}{1+exp(-\textbf{x}^T\textbf{w})} \\"}</annotation></semantics></math></p>
<p>Also when&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi><mo>=</mo><mo>−<!-- − --></mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y=-1\)"}</annotation></semantics></math>&nbsp;we have:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mo>−<!-- − --></mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−<!-- − --></mo><mi>σ<!-- σ --></mi><mo stretchy="false">(</mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ P(y=-1\vert\textbf{x}) = 1 - \sigma(\textbf{x}^T\textbf{w}) = \frac{1}{1+exp(\textbf{x}^T\textbf{w})} \\"}</annotation></semantics></math></p>
<p>So as you can see&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(y\vert\textbf{x})\)"}</annotation></semantics></math>&nbsp;is:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><mi>y</mi><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \frac{1}{1+exp(-y\textbf{x}^T\textbf{w})} \\"}</annotation></semantics></math></p>
<p>When&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi><mo>=</mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y=1\)"}</annotation></semantics></math>&nbsp;the above equation becomes:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ P(y=1\vert\textbf{x}) = \frac{1}{1+exp(-\textbf{x}^T\textbf{w})} \\"}</annotation></semantics></math></p>
<p>When<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi><mo>=</mo><mo>−<!-- − --></mo><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y=-1\)"}</annotation></semantics></math>&nbsp;the above equation becomes:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mo>−<!-- − --></mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ P(y=-1\vert\textbf{x}) = \frac{1}{1+exp(\textbf{x}^T\textbf{w})} \\"}</annotation></semantics></math></p>
<p>Therefore the likelihood function of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>&nbsp;using data&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\((\textbf{x}_i,y_i)\)"}</annotation></semantics></math>&nbsp;is given as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>l</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><mi>y</mi><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ l(w) = \frac{1}{1+exp(-y\textbf{x}^T\textbf{w})} \\"}</annotation></semantics></math></p>
<p>Assuming training data with&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\)"}</annotation></semantics></math>&nbsp;independent instances&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo fence="false" stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mn>1</mn></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">y</mtext></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><mo stretchy="false">(</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>n</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">y</mtext></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mo fence="false" stretchy="false">}</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\{(\textbf{x}_1,\textbf{y}_1),...,(\textbf{x}_n,\textbf{y}_n)\}\)"}</annotation></semantics></math>,&nbsp;we have joint likelihood as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>l</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mn>1</mn></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mn>1</mn><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mo>×<!-- × --></mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>×<!-- × --></mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>n</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>n</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ l(\textbf{w}) = \frac{1}{1+exp(-y_1\textbf{x}_1^T\textbf{w})} \times ... \times \frac{1}{1+exp(-y_n\textbf{x}_n^T\textbf{w})} \\"}</annotation></semantics></math></p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mo>=</mo><msubsup><mi mathvariant="normal">Π<!-- Π --></mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></msubsup><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ = \Pi_{i=1}^{n} \frac{1}{1+exp(-y_i\textbf{x}_i^T\textbf{w})} \\"}</annotation></semantics></math></p>
<p>So the Joint likelihood function while having&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(n\)"}</annotation></semantics></math>&nbsp;<em>independent samples</em>&nbsp;using training data is the multiplication of the likelihood of each point.&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>l</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><msubsup><mi mathvariant="normal">Π<!-- Π --></mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></msubsup><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ l(\textbf{w}) = \Pi_{i=1}^{n} \frac{1}{1+exp(-y_i\textbf{x}_i^T\textbf{w})} \\"}</annotation></semantics></math></p>
<p>Maximum likelihood estimation method maximises&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>l</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l(\textbf{w})\)"}</annotation></semantics></math>&nbsp;with respect to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>.</p>
<h3 id="logistic-loss-function">Logistic Loss Function</h3>
<p>Maximising likelihood is&nbsp;<em>equivalent</em>&nbsp;to maximising the&nbsp;<em>log of the likelihood function</em>&nbsp;because both provide the same solution for&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>.&nbsp;Remember by taking the log of the function you are still able to find the maximum or minimum of the function since the logarithmic functions are monotone increasing functions. Thus, you can write the Log of the likelihood function by taking the log of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>l</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(l(\textbf{w})\)"}</annotation></semantics></math>&nbsp;as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><mi>L</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&nbsp;</mtext><mi>l</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><mo>−<!-- − --></mo><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ L(\textbf{w}) = log\ l(\textbf{w}) = - \sum_{i=1}^{n} log(1+exp(-y_i\textbf{x}_i^T\textbf{w})) \\"}</annotation></semantics></math></p>
<p>Maximising&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&nbsp;</mtext><mi>l</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(log\ l(\textbf{w})\)"}</annotation></semantics></math>&nbsp;is equivalent to minimising&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo>−<!-- − --></mo><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&nbsp;</mtext><mi>l</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(-log\ l(\textbf{w})\)"}</annotation></semantics></math>,&nbsp;which brings us to the following minimisation problem:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow></munder><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \min_{w} \sum_{i=1}^{n} log(1+exp(-y_i\textbf{x}_i^T\textbf{w})) \\"}</annotation></semantics></math></p>
<p>But&nbsp;<em>how so we minimise the above function with respect to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>?&nbsp;</em>Remember that we would like to minimise the following function:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>w</mi></mrow></munder><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ \min_{w} \sum_{i=1}^{n} log(1+exp(-y_i\textbf{x}_i^T\textbf{w})) \\"}</annotation></semantics></math></p>
<p>The above function is also called the&nbsp;<em>Logistic Loss</em>&nbsp;function. The usual approach is to take the derivative and equate it to zero to solve for<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>.</p>
<p>However, in this case, the solution&nbsp;<em>does not</em>&nbsp;have a closed form. Therefore, we need to solve the problem&nbsp;<em>iteratively</em>.</p>
<p>It is important to remember that sometimes, we can derive a closed form formula for the minimiser (e.g. linear regression) meaning we can compute the minimiser in one step. If that’s not possible, we take multiple steps iteratively to reach to the minimum (e.g. logistic regression and Kmeans). So in this case, we need to perform&nbsp;<em>Coordinate-wise Gradient Descent Optimisation</em>.</p>
<p>The question still remains,&nbsp;<em>how do we find the minimum?</em></p>
<h4 id="computing-the-minimum">Computing the minimum</h4>
<p>Before answering this question, lets recap the difference of two types of functions&nbsp;<em>Convex</em>&nbsp;and&nbsp;<em>Non-convex</em>. Consider the following figure as illustration of two types of functions.</p>
<p class="centerImage"><img src="../images/Training%20a%20logistic%20regression%20model%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Convex and non-convex functions.</h5>
<p>The basic difference between the two categories is that:</p>
<ul>
<li>Convex optimisations can deal with only one optimal solution, which is globally optimal. The other possibility is that you prove that there is no feasible solution to the problem (right image on the figure above)</li>
<li>In non-convex optimisations, you may have multiple locally optimal points. It can take a lot of time to identify whether the problem has no solution or if the solution is global (left image on figure). Hence, the time efficiency of the convex optimisation problem is much better.</li>
</ul>
<p>So:</p>
<ul>
<li>Sometimes, we can derive a closed form formula for the minimiser (e.g. linear regression) meaning we can compute the minimiser in one step.</li>
<li>If there is no closed form formula, we must take multiple steps iteratively to reach to the minimum. (eg. logistic regression and Kmeans)</li>
</ul>
<p class="centerImage"><img src="../images/Training%20a%20logistic%20regression%20model%20image%203.jpg" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<p></p>
<p>Strategies for finding your way forward. © Getty Images (2018)</p>
<p>Imagine you’re blindfolded, but you can see out of the bottom of the blindfold to the ground at your feet. I drop you off somewhere and tell you that you’re in a&nbsp;<em>convex shaped valley</em>&nbsp;and escape is at the bottom/minimum.&nbsp;<em>How do you get out?</em></p>
<p>The simplest way is to look for steepest slope down! Basically you start walking and you look for slopes going down, preferably the steepest slopes. In maths, we call the&nbsp;<em>slopes</em>&nbsp; <strong>derivatives!</strong></p>
<p>The slope of a secant line (line connecting two points on a graph) approaches the derivative when the interval between the points shrinks down to zero. That is the basic idea for optimising these scenarios.</p>
<h3 id="iterative-optimising">Iterative optimising</h3>
<p>Lets get back to the iterative optimising. As we said before, optimisation theory has many methods for iterative optimisation. Two popular methods to compute gradient (derivatives) of the objective function are:</p>
<ul>
<li>
<p><strong>Gradient Descent</strong>&nbsp;(uses first derivative):</p>
</li>
<ul>
<li><span style="font-size: 15.2px;">To minimise&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(L(w)\)"}</annotation></semantics></math>,&nbsp;we use the iterative update:</span><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">←<!-- ← --></mo><msub><mi>w</mi><mi>t</mi></msub><mo>−<!-- − --></mo><msub><mi>η<!-- η --></mi><mi>t</mi></msub><mfrac><mi mathvariant="normal">∂<!-- ∂ --></mi><mrow><mi mathvariant="normal">∂<!-- ∂ --></mi><mi>w</mi></mrow></mfrac><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_{t+1} \leftarrow w_t - \eta_t \frac{\partial}{\partial w} L(w)\)"}</annotation></semantics></math></li>
</ul>
</ul>
<p><span style="font-size: 15.2px;"><span style="font-size: 15.2px;">Where&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>H</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(H\)"}</annotation></semantics></math>&nbsp;&nbsp;is the Hessian matrix with&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>H</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(H_{ij}\)"}</annotation></semantics></math>&nbsp;being&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mfrac><msup><mi mathvariant="normal">∂<!-- ∂ --></mi><mn>2</mn></msup><mrow><mi mathvariant="normal">∂<!-- ∂ --></mi><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∂<!-- ∂ --></mi><msub><mi>w</mi><mi>j</mi></msub></mrow></mfrac><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\frac{\partial^2}{\partial w_i \partial w_j} L(w)\)"}</annotation></semantics></math>.&nbsp;</span></span>Newton’s method is an iterative method for finding the roots of a differentiable function.</p>
<p>Remember, Gradient Descent maximises a function using knowledge of its derivative. While Newton’s method, a root finding algorithm, maximises a function using knowledge of its second derivative. This can be&nbsp;<em>faster</em>&nbsp;when the second derivative is known and easy to compute. However, the analytic expression for the second derivative is often complicated or intractable, requiring a lot of computation.</p>
<p class="centerImage"><img src="../images/Training%20a%20logistic%20regression%20model%20image%204.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Find the minimum iteratively.</h5>
<h4 id="coordinate-wise-gradient-descent-optimisation">Coordinate-wise Gradient Descent Optimisation</h4>
<p>Now, lets get back to Coordinate-wise Gradient Descent Optimisation. In order to complete the original task. First randomly initialise&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>.&nbsp;Fix all the variables except for one. That is, for each&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>j</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(j\)"}</annotation></semantics></math>, optimise&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mi>j</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_j\)"}</annotation></semantics></math>&nbsp; fixing&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>−<!-- − --></mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi></mrow></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_1,...,w_{j-1},w_{j+1},..., w_{d}\)"}</annotation></semantics></math>.&nbsp;</p>
<p>Then, minimise the objective function with respect to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mi>j</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_j\)"}</annotation></semantics></math>&nbsp;using Gradient descent as:</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mstyle><mspace linebreak="newline"></mspace><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub><mo stretchy="false">←<!-- ← --></mo><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub><mo>+</mo><mi>η<!-- η --></mi><mfrac><mi mathvariant="normal">∂<!-- ∂ --></mi><mrow><mi mathvariant="normal">∂<!-- ∂ --></mi><msub><mi>w</mi><mi>j</mi></msub></mrow></mfrac><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">(</mo></mrow><munderover><mo>∑<!-- ∑ --></mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−<!-- − --></mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi><mi>T</mi></msubsup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mo maxsize="1.623em" minsize="1.623em">)</mo></mrow><mspace linebreak="newline"></mspace></mstyle><annotation encoding="latex">{"version":"1.1","math":"\\ w_{j} \leftarrow w_{j} + \eta \frac{\partial}{\partial w_j}\ \ \Big(\sum_{i=1}^{n} log(1+exp(-y_i\textbf{x}_i^T\textbf{w}))\Big) \\"}</annotation></semantics></math></p>
<p>Similarly optimise for other<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>w</mi><mi>j</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(w_j\)"}</annotation></semantics></math>'s (<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>j</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(j\)"}</annotation></semantics></math>&nbsp;is from&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>&nbsp;to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>d</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(d\)"}</annotation></semantics></math>).&nbsp;Continue until the objective function stops changing. The solution is unique (due to the convexity of the objective function), irrespective of the initialisation of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{w}\)"}</annotation></semantics></math>.</p>
<p>Note that there is always a chance of getting stuck in local minimums rather than the global minimum. When you’re dealing with convex functions, that’s not a problem but with non-convex functions, it could be a serious problem.</p>
<h2 id="your-task">Activity</h2>
<p>Why do you think many believe you should run the Gradient descent with many different random initialisations?</p>
<hr>
<div><iframe class="quickNavStyle" scrolling="no" src="../00-assets/navbar/navbar.html" title="NavBar" allowfullscreen="allowfullscreen" frameborder="0"></iframe></div>
<!-- <div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
</div> -->
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
<script src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/jquery/jquery_3_5_1/jquery-3.5.1.min.js"></script>
<script src="../00-assets/navbar/navbar-parent.js"></script>
<script src="../00-assets/js/sit307-720.js"></script>
</p></body></html>