<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager --><!-- Google Tag Manager --><script>window.dataLayer = window.dataLayer || [];window.dataLayer.push({'cmsType' : 'D2L'});</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-N3CB');</script><!-- End Google Tag Manager -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CloudDeakin Dual Delivery Template</title>
<link rel="stylesheet" type="text/css" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-fl.css">
  <link rel="stylesheet" type="text/css" href="../00-assets/navbar/navbar-parent.css">
<link rel="stylesheet" type="text/css" href="../00-assets/css/sit307-720.css">

<link rel="stylesheet" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/open-source-css-and-js/prism/prism.css" type="text/css"><link rel="stylesheet" href="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/open-source-css-and-js/prism/prism.overrides.css" type="text/css"></head><body class="cloudFirst"><div>
<h1><span style="color: rgb(96, 56, 255);">Linear classification</span></h1>
</div>
<div>
<p>Logistic regression is the appropriate regression analysis to conduct when the output values of the feature vectors are&nbsp;<em>binary</em>.</p>
<p>Like all regression analyses, the logistic regression is a predictive analysis. Before talking about logistic regression, lets first review linear classification.</p>
<h3 id="reviewing-linear-classification">Reviewing linear classification</h3>
<p>Consider a set of training data of the form:&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo fence="false" stretchy="false">{</mo><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo fence="false" stretchy="false">}</mo><mspace width="1em"></mspace><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><mi>n</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\{\textbf{x}_i,y_i\} \quad i=1,...,n\)"}</annotation></semantics></math>.&nbsp;For each data point&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}_i\)"}</annotation></semantics></math>,&nbsp;there is a output&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mi>y</mi><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y_i\)"}</annotation></semantics></math>,&nbsp;which can take discrete values eg:&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>1</mn><mo>,</mo><mo>−<!-- − --></mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mo>+</mo><mn>1</mn><mo>,</mo><mi>o</mi><mi>r</mi><mo fence="false" stretchy="false">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>K</mi><mo fence="false" stretchy="false">}</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0/1, -1/+1, or \{1,2,…,K\}\)"}</annotation></semantics></math>.</p>
<p>When there are only&nbsp;<em>two possible values</em>&nbsp;for output, we call the problem a&nbsp;<em>binary classification</em>&nbsp;problem. If there are more values it’s a&nbsp;<em>multi-class classification</em>&nbsp;problem.</p>
<h4 id="example-1-binary">Example 1: binary</h4>
<p>Given an image, the task maybe to classify if it is an image of&nbsp;<em>a fruit</em>&nbsp;or&nbsp;<em>not fruit</em>.</p>
<h4 id="example-2-multi-class">Example 2: multi-class</h4>
<p>Given an image of a fruit, the task may be to classify whether it is an orange, an apple or a banana.</p>
<h3 id="but-what-do-we-mean-by-linear-classification">But what do we mean by linear classification?</h3>
<p>By linear classification, we mean that the&nbsp;<em>separation boundary</em>&nbsp;between any two classes is&nbsp;<em>linear</em>. This is just&nbsp;<em>our hypothesis</em>. It may not be true!</p>
<p>First, we start with a linear assumption, later we will derive more complicated scenarios.</p>
<p class="centerImage"><img src="../images/Linear%20classification%20image%202.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. The green line indicates the linear boundary for classification of two classes.</h5>
<p>If you think in terms of regression in which&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y\)"}</annotation></semantics></math>'s&nbsp;are continuous different numbers, classification is&nbsp;<em>intrinsically non-linear</em>.</p>
<p>Why?</p>
<p>Because two instances&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>i</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}_i\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msub><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>j</mi></msub></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}_j\)"}</annotation></semantics></math>&nbsp;and their outputs may be different but may belong to the same class. Take the red bubbles in the above figure as an example. You can see they are quite different points but they have the same class label because they are under the line. They are in the same class but they are not linear with respect to each other.</p>
<p>So how can we handle this non-linear ambiguity in forms of regression?</p>
<p>Lets assume it is just a regression case. Then we handle the&nbsp;<em>non-linearity</em>as&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>=</mo><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h(\textbf{x}) = \textbf{x}^T\textbf{w}\)"}</annotation></semantics></math>.&nbsp;So basically we project&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}\)"}</annotation></semantics></math>&nbsp;to the new line&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h(\textbf{x})\)"}</annotation></semantics></math>.</p>
<h3 id="logistic-regression">Logistic regression</h3>
<p>But how can we make a classification decision?</p>
<p>It seems that we need another function such as a decision function&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>σ<!-- σ --></mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\sigma(h(\textbf{x}))\)"}</annotation></semantics></math>&nbsp;which uses a&nbsp;<em>fixed non-linear</em>&nbsp;link function and projects the value of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>h</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(h(\textbf{x})\)"}</annotation></semantics></math>&nbsp;into&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\([0,1]\)"}</annotation></semantics></math>&nbsp;interval.</p>
<p>So if the final score is close to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>,&nbsp;we would label that instance label&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>.&nbsp;If it was closer to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>,&nbsp;we call it label&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;and if it is close to the middle of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;and&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>&nbsp;it is much closer to the decision boundary and it is not so clear which side it should be classified to.</p>
<p>This was the really simple illustration of Logistic Regression approach. To conclude:&nbsp;</p>
<p><math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">→<!-- → --></mo><mtext>&nbsp;</mtext><mi>h</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo stretchy="false">→<!-- → --></mo><mi>σ<!-- σ --></mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x} \rightarrow \ h(\textbf{x})\rightarrow \sigma(h(\textbf{x}))\)"}</annotation></semantics></math></p>
<p>So two approaches are generally available:</p>
<ul>
<li><strong>Ignore non-linearity:</strong><br>Using&nbsp;<em>least squares</em>&nbsp;for classification: treat binary outputs like the outputs in the regression problem. This may not be the best method but it is easy!</li>
<li><strong>Using link function:</strong><br>Another approach is to use the conditional probability of the class as the output in the regression problem, i.e. fitting regression on&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(y=1\vert\textbf{x})\)"}</annotation></semantics></math>&nbsp;instead of&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y\)"}</annotation></semantics></math>.&nbsp;&nbsp;In other words using a link function to transform the output to the classification scenario. In here, in a simple case, if&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo fence="false" stretchy="false">|</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo><mo>&gt;</mo><mn>0.5</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) &gt; 0.5\)"}</annotation></semantics></math>,&nbsp;&nbsp;we may want to choose class label&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>1</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(1\)"}</annotation></semantics></math>&nbsp;for the data point&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}\)"}</annotation></semantics></math>&nbsp;unless, it seems more logical to select label&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mn>0</mn></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(0\)"}</annotation></semantics></math>&nbsp;for this data point.</li>
</ul>
<p class="centerImage"><img src="../images/Linear%20classification%20image%203.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. Logistic link function&nbsp;</h5>
<p>Least squares regression can perform very badly when some points in the training data have excessively large or small values for the dependent variable compared to the rest of the training data (see figure below). The reason for this is that since the least squares method is concerned with minimising the sum of the squared error, any training point that has a dependent value that differs a lot from the rest of the data will have a disproportionately large effect on the resulting constants that are being solved for.</p>
<p class="centerImage"><img src="../images/Linear%20classification%20image%204.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure. The flaws in Least Square.</h5>
<p>When using Logistic link function to relate&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mi>T</mi></msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">w</mtext></mrow></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(\textbf{x}^T \textbf{w}\)"}</annotation></semantics></math>&nbsp;to&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>y</mi></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(y\)"}</annotation></semantics></math>,&nbsp;the linear model is called&nbsp;<em>Logistic Regression</em>. Logistic regression is a popular model for classification. (Misnomer? Yes! but it is actually a classification method) Other link functions used are&nbsp;<em>Probit link function</em>,&nbsp;<em>tanh function</em>&nbsp;(see figure below).</p>
<p class="centerImage"><img src="../images/Linear%20classification%20image%205.png" alt="" title="" style="max-width: 100%;" data-d2l-editor-default-img-style="true"></p>
<h5>Figure.&nbsp;<math title="" xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mstyle><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">x</mtext></mrow><mo stretchy="false">)</mo></mstyle><annotation encoding="latex">{"version":"1.1","math":"\(tanh(\textbf{x})\)"}</annotation></semantics></math>&nbsp;function</h5>
<p>In the next section we are going to talk about the formulation of Logistic Regression.</p>
<p></p>
</div>
<hr>
<div style="padding-top: 20px;"><a href="#" class="navrep-button" target="_parent" title="Previous" style="padding: .5rem .5rem; font-size: 12pt; float: left;"> &lt; Previous</a> <a href="#" class="navrep-button" target="_parent" title="Next" style="padding: .5rem .5rem; font-size: 12pt; float: right;">Next &gt;</a></div>
<p style="padding-bottom: 50px;"></p>
<p>
<script defer="defer" type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-intmedia-donotedit/templates/sebe-master.js"></script>
</p>
<p>
<script>
function localProc(){
  console.log("ready!");
}
</script>
</p>
<p>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-mathjax.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl-nav.js"></script>
<script type="text/javascript" src="/content/enforced/384049-SEBE_LST_01/z-sebe-lst-learning-design/fl-dual-delivery/js/sebe-fl.js"></script>
</p></body></html>